[Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 0, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Dr. Seemab latif\nLecture 7\n12 Nov 2024\nA R T I F I C I A L  I N T E L L I G E N C E\nArtificialIntelligence\n十\nMachineLearning\nDeepLearning\n0'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 1, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley.]\nProbability'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 2, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Today\n\uf0a7Probability\n\uf0a7Random Variables\n\uf0a7Joint and Marginal Distributions\n\uf0a7Conditional Distribution\n\uf0a7Product Rule, Chain Rule, Bayes’ Rule\n\uf0a7Inference\n\uf0a7Independence\n\uf0a7You’ll need all this stuff A LOT for the \nnext few weeks, so make sure you go \nover it now!\nT'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 3, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Inference in Ghostbusters\n\uf0a7A ghost is in the grid \nsomewhere\n\uf0a7Sensor readings tell how \nclose a square is to the \nghost\n\uf0a7On the ghost: red\n\uf0a71 or 2 away: orange\n\uf0a73 or 4 away: yellow\n\uf0a75+ away: green\nP(red | 3)\nP(orange | 3)\nP(yellow | 3)\nP(green | 3)\n0.05\n0.15\n0.5\n0.3\n\uf0a7Sensors are noisy, but we know P(Color | Distance)\n[Demo: Ghostbuster – no probability (L12D1) ]'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 4, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Video of Demo Ghostbuster – No probability'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 5, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Uncertainty\n\uf0a7General situation:\n\uf0a7Observed variables (evidence): Agent knows certain \nthings about the state of the world (e.g., sensor \nreadings or symptoms)\n\uf0a7Unobserved variables: Agent needs to reason about \nother aspects (e.g. where an object is or what disease is \npresent)\n\uf0a7Model: Agent knows something about how the known \nvariables relate to the unknown variables\n\uf0a7Probabilistic reasoning gives us a framework for \nmanaging our beliefs and knowledge\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.110.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.17<0.01\n<0.01\n0.03\n<0.01\n0.05\n0.05\n<0.01\n0.05\n0.81<0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 6, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Random Variables\n\uf0a7A random variable is some aspect of the world about \nwhich we (may) have uncertainty\n\uf0a7R = Is it raining?\n\uf0a7T = Is it hot or cold?\n\uf0a7D = How long will it take to drive to work?\n\uf0a7L = Where is the ghost?\n\uf0a7We denote random variables with capital letters\n\uf0a7Random variables have domains\n\uf0a7R in {true, false}   (often write as {+r, -r})\n\uf0a7T in {hot, cold}\n\uf0a7D in [0, \uf0a5)\n\uf0a7L in possible locations, maybe {(0,0), (0,1), …}\nT'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 7, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Probability Distributions\n\uf0a7Associate a probability with each value\n\uf0a7Temperature:\nT\nP\nhot\n0.5\ncold\n0.5\nW\nP\nsun\n0.6\nrain\n0.1\nfog\n0.3\nmeteor\n0.0\n\uf0a7Weather: \nPM\nP\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 8, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Shorthand notation:\nOK if all domain entries are unique\nProbability Distributions\n\uf0a7Unobserved random variables have distributions\n\uf0a7A distribution is a TABLE of probabilities of values\n\uf0a7A probability (lower case value) is a single number\n\uf0a7Must have:                                                 and\nT\nP\nhot\n0.5\ncold\n0.5\nW\nP\nsun\n0.6\nrain\n0.1\nfog\n0.3\nmeteor\n0.0\nPM\nP\n1P(W :\n=rain)\n0.1\nIP(X\nVc\n0\n=P(X\nP\n1P(\nP(W\nrazn\nraznP(hot\nP(T\nhot\n川P(cold)\nP(T\n(1o'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 9, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Joint Distributions\n\uf0a7A joint distribution over a set of random variables:\nspecifies a real number for each assignment (or outcome): \n\uf0a7Must obey:\n\uf0a7Size of distribution if n variables with domain sizes d?\n\uf0a7For all but the smallest distributions, impractical to write out!\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\n7\n2\nn\n了\n？P(X1\n=c1, X2\n川\nCnD\ncn.\nC\nC\n1\n2\n?\n了\n了P(\nα1,C2,.\nCn）\n0P(1,c2,.\nCn)\n1\n=\nx1,c2,...MnP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 10, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Probabilistic Models\n\uf0a7\nA probabilistic model is a joint distribution \nover a set of random variables\n\uf0a7\nProbabilistic models:\n\uf0a7(Random) variables with domains \n\uf0a7Assignments are called outcomes\n\uf0a7Joint distributions: say whether assignments \n(outcomes) are likely\n\uf0a7Normalized: sum to 1.0\n\uf0a7Ideally: only certain variables directly interact\n\uf0a7\nConstraint satisfaction problems:\n\uf0a7Variables with domains\n\uf0a7Constraints: state whether assignments are \npossible\n\uf0a7Ideally: only certain variables directly interact\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nT\nW\nP\nhot\nsun\nT\nhot\nrain\nF\ncold\nsun\nF\ncold\nrain\nT\nDistribution over T,W\nConstraint over T,W\n00\n？'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 11, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Events\n\uf0a7An event is a set E of outcomes\n\uf0a7From a joint distribution, we can \ncalculate the probability of any event\n\uf0a7Probability that it’s hot AND sunny?\n\uf0a7Probability that it’s hot?\n\uf0a7Probability that it’s hot OR sunny?\n\uf0a7Typically, the events we care about \nare partial assignments, like P(T=hot)\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nP(E)\nP(c1\nCn\nc1...Cn\nn)EEP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 12, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Quiz: Events\n\uf0a7P(+x, +y) ?\n\uf0a7P(+x) ?\n\uf0a7P(-y OR +x) ?\nX\nY\nP\n+x\n+y\n0.2\n+x\n-y\n0.3\n-x\n+y\n0.4\n-x\n-y\n0.1\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 13, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Marginal Distributions\n\uf0a7\nMarginal distributions are sub-tables which eliminate variables \n\uf0a7\nMarginalization (summing out): Combine collapsed rows by adding\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nT\nP\nhot\n0.5\ncold\n0.5\nW\nP\nsun\n0.6\nrain\n0.4\nP\nM\n+\n了P(t)\nP(t,s)\nSPP(s)\nP(t, s)\nfM\nP\n1P(X1 =c1) =\nP(X1 = x1, X2 = ∞2)\nC2P(x,y)\nP(x)\nAy\nP()=\nMP(x.y)\nD\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 14, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Quiz: Marginal Distributions\nX\nY\nP\n+x\n+y\n0.2\n+x\n-y\n0.3\n-x\n+y\n0.4\n-x\n-y\n0.1\nX\nP\n+x\n-x\nY\nP\n+y\n-y\nDP(c)\nP(\n(x,y)\nhD(y)\nP(\nP(?\nx,y)\nIDP(x,y)\nP(x)\nAy\nP()=\nMP(x.y)\nD\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 15, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Probabilities\n\uf0a7A simple relation between joint and conditional probabilities\n\uf0a7In fact, this is taken as the definition of a conditional probability\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nP(b)\nP(a)\nP(a,b)\nP(a,b)\nP(a|b)\nP\n6P\nM\n+\n了P(W\n[T\n=???\nS\nC)P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.5×P(W\n= s,T\n= c) + P(W\n= r,T :\n=C)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 16, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Quiz: Conditional Probabilities\nX\nY\nP\n+x\n+y\n0.2\n+x\n-y\n0.3\n-x\n+y\n0.4\n-x\n-y\n0.1\n\uf0a7P(+x | +y) ?\n\uf0a7P(-x | +y) ?\n\uf0a7P(-y | +x) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 17, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Distributions\n\uf0a7Conditional distributions are probability distributions over \nsome variables given fixed values of others\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nW\nP\nsun\n0.8\nrain\n0.2\nW\nP\nsun\n0.4\nrain\n0.6\nConditional Distributions\nJoint Distribution\nM\nhot\n>P\nW\ncold\n7\n1P\nM\n+\n了一'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 18, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Normalization Trick\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nW\nP\nsun\n0.4\nrain\n0.6\nP(W :\n==\nCM)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了CP(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 19, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='SELECT the joint \nprobabilities \nmatching the \nevidence\nNormalization Trick\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nW\nP\nsun\n0.4\nrain\n0.6\nT\nW\nP\ncold\nsun\n0.2\ncold\nrain\n0.3\nNORMALIZE the \nselection\n(make it sum to one)\nP(W :\nT=\n=???\n三r\n c)M)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了CP(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nCM\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 20, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Normalization Trick\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nW\nP\nsun\n0.4\nrain\n0.6\nT\nW\nP\ncold\nsun\n0.2\ncold\nrain\n0.3\nSELECT the joint \nprobabilities \nmatching the \nevidence\nNORMALIZE the \nselection\n(make it sum to one)\n\uf0a7Why does this work? Sum of selection is P(evidence)!  (P(T=c), here)\nP\nM\n+\n了M\nD\nCM\nD\nCP(\n(1,2)\nP(\n(C1,C2)\nP(c1|∞2)\n=\nP(c2)\nZc1\nP(x1,2)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 21, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Quiz: Normalization Trick\nX\nY\nP\n+x\n+y\n0.2\n+x\n-y\n0.3\n-x\n+y\n0.4\n-x\n-y\n0.1\nSELECT the joint \nprobabilities \nmatching the \nevidence\nNORMALIZE the \nselection\n(make it sum to one)\n\uf0a7P(X | Y=-y) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 22, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='\uf0a7(Dictionary) To bring or restore to a normal condition\n\uf0a7Procedure:\n\uf0a7Step 1: Compute Z = sum over all entries\n\uf0a7Step 2: Divide every entry by Z\n\uf0a7Example 1\nTo Normalize\nAll entries sum to ONE\nW\nP\nsun\n0.2\nrain\n0.3\nZ = 0.5\nW\nP\nsun\n0.4\nrain\n0.6\n\uf0a7Example 2\nT\nW\nP\nhot\nsun\n20\nhot\nrain\n5\ncold\nsun\n10\ncold\nrain\n15\nNormalize\nZ = 50\nNormalize\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 23, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Probabilistic Inference\n\uf0a7Probabilistic inference: compute a desired probability \nfrom other known probabilities (e.g. conditional from \njoint)\n\uf0a7We generally compute conditional probabilities \n\uf0a7P(airport on time | no reported accidents) = 0.90\n\uf0a7These represent the agent’s beliefs given the evidence\n\uf0a7Probabilities change with new evidence:\n\uf0a7P(airport on time | no accidents, 5 a.m.) = 0.95\n\uf0a7P(airport on time | no accidents, 5 a.m., raining) = 0.80\n\uf0a7Observing new evidence causes beliefs to be updated'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 24, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Inference by Enumeration\n\uf0a7\nGeneral case:\n\uf0a7Evidence variables: \n\uf0a7Query* variable:\n\uf0a7Hidden variables:\nAll variables\n* Works fine with \nmultiple query \nvariables, too\n\uf0a7\nWe want:\n\uf0a7\nStep 1: Select the \nentries consistent \nwith the evidence\n\uf0a7\nStep 2: Sum out H to get joint \nof Query and evidence\n\uf0a7\nStep 3: Normalize\n1\nN\nn\n7\n7E1\nEk\n:e1\nekH\nH\n7P\ne\n1\ne\nk\n√\n&D\ne\n1\ne\nk\n&\n7P(Q,1\nh1\nhr,\ne1\nek)\nh...hrX\nP(x)\nE-\n0.05\n-1\n0.25\n0.07\n1\n0.2\n5\n0.01\n2\n0.15Z\nP(Q,e\ne1\nek\nb1\nP(Qle1\nP(Q,6\nek\n-Z\ne1\nek'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 25, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='28\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\nhot\nfog\nsummer\n0.01\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\ncold\nfog\nsummer\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\nsun\n0.15\nwinter\no\nrain\n0.20\nwinter\no\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 26, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='29\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerate optionswith\nhot\nfog\nsummer\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\ncold\nfog\nsummer\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\no\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 27, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='30\nInference by Enumeration\nSeason\nTemp\nWeather\nhot\nsummer\nsun\n0.35\nP(SI sun)?\nsumimei\nhot\nrain\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\nwinter\nhot\nsun\n0.10\nMintei\nhot\nrain\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\nwinter\ncold\nfog\n0.18\nwinter\ncald\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 28, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='31\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nsummer\ncold\nrain\n0.05\nvariable(s)\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 29, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='32\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nhot\nsummer\nsun\n0.35\nP(SI sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\ncold\nsummer\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 30, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='33\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(SI sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 31, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='34\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(SIsun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nsummer\ncold\nrain\n0.05\nvariable(s)\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nP(S|sun)=\nwinter\nhot\nsun\n0.10\n{summer:0.45/(0.45+0.25),\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter: 0.25/(0.45+0.25))\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 32, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Inference by Enumeration\n\uf0a7P(W)?\n\uf0a7P(W | winter)?\n\uf0a7P(W | winter, hot)?\nS\nT\nW\nP\nsummer\nhot\nsun\n0.30\nsummer\nhot\nrain\n0.05\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.05\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 33, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='\uf0a7Obvious problems:\n\uf0a7Worst-case time complexity O(dn) \n\uf0a7Space complexity O(dn) to store the joint distribution\nInference by Enumeration'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 34, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='The Product Rule\n\uf0a7Sometimes have conditional distributions but want the joint\nP(x,y)\nP(xly):\nP(\n(y)P(?\n(y)P(\nP(\ny)\nx,y\nI8\nX\n='), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 35, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='The Product Rule\n\uf0a7Example:\nR\nP\nsun\n0.8\nrain\n0.2\nD\nW\nP\nwet\nsun\n0.1\ndry\nsun\n0.9\nwet\nrain\n0.7\ndry\nrain\n0.3\nD\nW\nP\nwet\nsun\n0.08\ndry\nsun\n0.72\nwet\nrain\n0.14\ndry\nrain\n0.06\nP\nM\n?\n1D\nM\n了M\nP\n1P(?\n(y)P(\nP(\ny)\nx,y\nI'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 36, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='The Chain Rule\n\uf0a7More generally, can always write any joint distribution as an \nincremental product of conditional distributions\n\uf0a7Why is this always true?\nP(x1,2,3) = P(x1)P(x2|x1)P(x3|1,2)P(x1,x2,...cn) =\n11\nP(xic...i-1)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 37, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes Rule'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 38, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes’ Rule\n\uf0a7Two ways to factor a joint distribution over two variables:\n\uf0a7Dividing, we get:\n\uf0a7Why is this at all helpful?\n\uf0a7Lets us build one conditional from its reverse\n\uf0a7Often one conditional is tricky but the other one is simple\n\uf0a7Foundation of many systems we’ll see later (e.g. ASR, MT)\n\uf0a7In the running for most important AI equation!\nThat’s my rule!\nP(c,y)\nP(c|y)P(y)P(y|∞)\nP(cly) :\n(P(c)\n=\nP\n(y)y'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 39, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Inference with Bayes’ Rule\n\uf0a7Example: Diagnostic probability from causal probability:\n\uf0a7Example:\n\uf0a7M: meningitis, S: stiff neck\n\uf0a7Note: posterior probability of meningitis still very small\n\uf0a7Note: you should still get stiff necks checked out!  Why?\nP(s I m) = 0.8\nExample\nP(m) = 0.0001\ngivens\nP(s) = 0.01\nP(s|m)P(m)\n0.8x0.0001\nP(m|s)=\nP(s)\n0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 40, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Quiz: Bayes’ Rule\n\uf0a7Given:\n\uf0a7What is P(W | dry) ? \nR\nP\nsun\n0.8\nrain\n0.2\nD\nW\nP\nwet\nsun\n0.1\ndry\nsun\n0.9\nwet\nrain\n0.7\ndry\nrain\n0.3\nP\nM\n?\n1M\nP\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 41, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Ghostbusters, Revisited\n\uf0a7Let’s say we have two distributions:\n\uf0a7Prior distribution over ghost location: P(G)\n\uf0a7Let’s say this is uniform\n\uf0a7Sensor reading model: P(R | G)\n\uf0a7Given: we know what our sensors do\n\uf0a7R = reading color measured at (1,1)\n\uf0a7E.g. P(R = yellow | G=(1,1)) = 0.1\n\uf0a7We can calculate the posterior \ndistribution P(G|r) over ghost locations \ngiven a reading using Bayes’ rule:\n[Demo: Ghostbuster – with probability (L12D2) ]\n0.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.170.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11(46)d\n(6)d(6|)d x'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 42, 'total_pages': 43, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181353+05'00'", 'modDate': "D:20241211181353+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Video of Demo Ghostbusters with Probability')]
[Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 0, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes’ Nets\n[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley.  All CS188 materials are available at http://ai.berkeley.edu.]\nX4'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 1, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Probabilistic Models\n\uf0a7Models describe how (a portion of) the world works\n\uf0a7Models are always simplifications\n\uf0a7May not account for every variable\n\uf0a7May not account for all interactions between variables\n\uf0a7“All models are wrong; but some are useful.”\n– George E. P. Box\n\uf0a7What do we do with probabilistic models?\n\uf0a7We (or our agents) need to reason about unknown \nvariables, given evidence\n\uf0a7Example: explanation (diagnostic reasoning)\n\uf0a7Example: prediction (causal reasoning)\n\uf0a7Example: value of information\n3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 2, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Independence'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 3, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='\uf0a7Two variables are independent if:\n\uf0a7This says that their joint distribution factors into a product two \nsimpler distributions\n\uf0a7Another form:\n\uf0a7We write: \n\uf0a7Independence is a simplifying modeling assumption\n\uf0a7Empirical joint distributions: at best “close” to independent\n\uf0a7What could we assume for {Weather, Traffic, Cavity, Toothache}?\nIndependence\ny : P(c,y)\nP(c)P(y)\nVc, y\n=h‘cA\nP(c)\nP(c)\ny)\n='), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 4, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Independence?\nT\nW\nP\nhot\nsun\n0.4\nhot\nrain\n0.1\ncold\nsun\n0.2\ncold\nrain\n0.3\nT\nW\nP\nhot\nsun\n0.3\nhot\nrain\n0.2\ncold\nsun\n0.3\ncold\nrain\n0.2\nT\nP\nhot\n0.5\ncold\n0.5\nW\nP\nsun\n0.6\nrain\n0.4\nM\nD\n1\n1DM\nDP\nM\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 5, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Independence\n\uf0a7N fair, independent coin flips:\nH\n0.5\nT\n0.5\nH\n0.5\nT\n0.5\nH\n0.5\nT\n0.5\nP\n1PP(X1.\nX2,\nP\nXn.on0.411'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 6, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Conditional Independence\n\uf0a7\nP(Toothache, Cavity, Catch)\n\uf0a7\nIf I have a cavity, the probability that the probe catches in it \ndoesn't depend on whether I have a toothache:\n\uf0a7P(+catch | +toothache, +cavity) = P(+catch | +cavity)\n\uf0a7\nThe same independence holds if I don’t have a cavity:\n\uf0a7P(+catch | +toothache, -cavity) = P(+catch| -cavity)\n\uf0a7\nCatch is conditionally independent of Toothache given Cavity:\n\uf0a7P(Catch | Toothache, Cavity) = P(Catch | Cavity)\n\uf0a7\nEquivalent statements:\n\uf0a7P(Toothache | Catch , Cavity) = P(Toothache | Cavity)\n\uf0a7P(Toothache, Catch | Cavity) = P(Toothache | Cavity) P(Catch | Cavity)\n\uf0a7One can be derived from the other easily"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 7, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Independence\n\uf0a7Unconditional (absolute) independence very rare (why?)\n\uf0a7Conditional independence is our most basic and robust form \nof knowledge about uncertain environments.\n\uf0a7X is conditionally independent of Y given Z\nif and only if:\nor, equivalently, if and only if\nVx, y,2 : P(x, y2)\nP(c|2)P(y2)\n川X\nYVx, y,z : P(xl\nP(c\n2, y)\nZ'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 8, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Independence\n\uf0a7What about this domain:\n\uf0a7Traffic\n\uf0a7Umbrella\n\uf0a7Raining'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 9, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Independence\n\uf0a7What about this domain:\n\uf0a7Fire\n\uf0a7Smoke\n\uf0a7Alarm'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 10, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional Independence and the Chain Rule\n\uf0a7Chain rule: \n\uf0a7Trivial decomposition:\n\uf0a7With assumption of conditional independence:\n\uf0a7Bayes’nets / graphical models help us express conditional independence assumptions\nP(Rain)P(Traffic|Rain)P(Umbrella|Rain,\nTrafficP(Traffic,\nRain, UmbrellaP(Rain)P(Traffic|Rain)P(Umbrella\nRainP(X1, X2,... Xn) = P(X1)P(X2|X1)P(X3|X1, X2) ..'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 11, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Inference in Ghostbusters\n\uf0a7A ghost is in the grid \nsomewhere\n\uf0a7Sensor readings tell how \nclose a square is to the \nghost\n\uf0a7On the ghost: red\n\uf0a71 or 2 away: orange\n\uf0a73 or 4 away: yellow\n\uf0a75+ away: green'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 12, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Video of Demo Ghostbusters with Probability'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 13, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='15\nGhostbusters model\nVariables and ranges:\n0.11\n0.11\n0.11\nG (ghost location) in {(1,1),..,(3,3)}\n0.11\n0.11\n0.11\nCx,y\n(color measured at square x,y) in\n{red,orange,yellow,green}\n0.11\n0.11\n0.11\nGhostbuster physics:\nUniform prior distribution over ghost location: P(G)\nSensor model: P(Cxy I G) (depends only on distance to G)\n E.g. P(C1,1 = yellow | G = (1,1)) = 0.1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 14, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='16\nGhostbusters model, contd.\nP(G, C1,1, .. C3,3) has 9 x 49 = 2,359,296 entries!!!\n0.11\n00\n0.11\nGhostbuster independence:\n0.11\n0.11\n0.11\nAre\ne C1,1 and\nC1,2 independent?\n0.11\n0.11\n0.11\nE.g., does P(C1,1 = yellow) = P(C1,1 = yellow I C1,2 = orange)\nGhostbuster physics again:\nP(Cxy I G) depends only on distance to G\n■ So P(C1,1 = yellow I G = (2,3)) = P(C1,1 = yellow I G = (2,3), C1,2 = orange)\n■ I.e., C1,1 is conditionally independent of C1,2 given G'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 15, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='17\nGhostbusters model, contd.\nApply the chain rule to decompose the joint probability model:\nP(G, C1,1, .. C3,3) = P(G) P(C1,1 I G) P(C1,2 I G, C1,1) P(C1,3 | G, C1,1, C1,2)... P(C3,3 I G, C1,1, ., C3,2)\nNow simplify using conditional independence:\nP(G, C1,1, . C3,3) = P(G) P(C1,1 I G) P(C1,2 I G) P(C1,3 I G) .. P(C3,3 I G)\nI.e., conditional independence properties of ghostbuster physics simplify the probability\nmodel from exponential to quadratic in the number of squares'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 16, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes’Nets: Big Picture\nDISTRIBUTIONS\nIN NL EASY STEPS!'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 17, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes’ Nets: Big Picture\n\uf0a7Two problems with using full joint distribution tables \nas our probabilistic models:\n\uf0a7Unless there are only a few variables, the joint is WAY too \nbig to represent explicitly\n\uf0a7Hard to learn (estimate) anything empirically about more \nthan a few variables at a time\n\uf0a7Bayes’ nets: a technique for describing complex joint \ndistributions (models) using simple, local \ndistributions (conditional probabilities)\n\uf0a7More properly called graphical models\n\uf0a7We describe how variables locally interact\n\uf0a7Local interactions chain together to give global, indirect \ninteractions\n\uf0a7For about 10 min, we’ll be vague about how these \ninteractions are specified\nX40.411'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 18, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Graphical Model Notation\n\uf0a7Nodes: variables (with domains)\n\uf0a7Can be assigned (observed) or unassigned \n(unobserved)\n\uf0a7Arcs: interactions\n\uf0a7Similar to CSP constraints\n\uf0a7Indicate “direct influence” between variables\n\uf0a7Formally: encode conditional independence \n(more later)\n\uf0a7For now: imagine that arrows mean \ndirect causation (in general, they don’t!)\nWeatherCavity\ner\nToothache\nCatch'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 19, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Coin Flips\n\uf0a7N independent coin flips\n\uf0a7No interactions between variables: absolute independence\nX1\nX2\nXn'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 20, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Example: Traffic\nVariables:\nT: There is traffic\nU: I'm holding my umbrella\nR: It rains\nR\nT\nU"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 21, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Smoke alarm\nVariables:\nF\nF: There is fire\nS: There is smoke\nA: Alarm sounds\nS\nA'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 22, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Example: Ghostbusters\nVariables:\n0.11\n00\n0.11\nG: The ghost's location\nC1,1,..\nC3,3 :\n0.11\n0.11\n0.11\nG\nThe observation at each location\n0.11\n0.11\n0.11\nWant to estimate:\nP( G | C1,1, .. C3,3 )\nC1,1\nC1,2\nC3,3\nThis is called a Naive Bayes model:\nOne discrete query variable (often called the class or category variable)\nAll other variables are (potentially) evidence variables\nEvidence variables are all conditionally independent given the query variable"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 23, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Example Bayes' Net: Car Insurance\nAge\nSocioEcon\nGoodStudent\nExtraCar\nRiskAversion\nMakeModel\nVehicleYear\nYearsLicensed\nMileage\nDrivingSkill\nAntiTheft\nSafetyFeatures\nCarValue\nGaraged\nAirbag\nDrivingRecord\nRuggedness\nDrivingBehavior\nCushioning\nTheft\nAccident\nOwnCarDamage\nOwnCarCost\nOtherCost\nMedicalCost\nLiabilityCost\nPropertyCost"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 24, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Example Bayes' Net: Car Won't Start\nalternator\nfanbelt\nbattery age\nbroken\nbroken\nbattery\nno charging\ndead\nbattery\nbattery\nfuel line\nstarter\nno oil\nflat\nno gas\nmeter\nblocked\nbroken\ncarwon'\nlights\noil light\ngasgauge\ndipstick\nstart"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 25, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes’ Net Semantics\nBuild Your\nOwn\nBnyes Net'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 26, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Bayes Net Syntax\nA set of nodes, one per variable X;\nP(G)\n(1,1)\n(1,2)\nG\n(1,3)\nA directed, acyclic graph\n0.11\n0.11\n0.11\nA conditional distribution for each node\ngiven its parent variables in the graph\nC3,3)\nCPT (conditional probability table); each row is a\nP(C1,1 I G)\ndistribution for child given values of its parents\ng\n(1,1)\n0.01\n0.1\n0.3\n0.59\n(1,2)\n0.1\n0.3\n0.5\n0.1\n(1,3)\n0.3\n0.5\n0.19\n0.01\nBayes net = Topology (graph) + Local Conditional Probabilities'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 27, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Alarm Network\n\uf0a7Variables\n\uf0a7B: Burglary\n\uf0a7E: Earthquake\n\uf0a7A: Alarm goes off\n\uf0a7M: Mary calls\n\uf0a7J: John calls'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 28, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Alarm Network\nP(E)\nP(B)\ntrue\nfalse\ntrue\nfalse\nBurglary\nEarthquake\n0.002\n0.998\n0.001\n0.999\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\nAlarm\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nNumber of free parameters\nfalse\nfalse\n0.001\n0.999\nin each CPT:\nJohn\nMary\ncalls\ncalls\nParent range sizes d1,...,dk\nA\nP(J|A)\nA\nP(M|A)\ntrue\nfalse\ntrue\nfalse\nChild range size d\ntrue\n0.9\n0.1\ntrue\n0.7\n0.3\nEach table row must sum to 1\nfalse\n0.05\n0.95\nfalse\n0.01\n0.99\n(d-1) II; d;'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 29, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Bayes net global semantics\nBayes nets encode joint distributions as product of\nconditional distributions on each variable:\n((x)suadd I x)d  = ('xx)d\nExploits sparse structure: number of parents is\nusuallysmall"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 30, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='33\nSize of a Bayes Net\nHow big is a joint distribution over N\nBoth give you the power to calculate\nvariables, each with d values?\nP(X1, X2, .., XN)\nQN\nBayes Nets: huge space savings with sparsity!\nHow big is an N-node net if nodes\nAlso easier to elicit local CPTs\nhave at most k parents?\nAlso faster to answer queries (coming)\nO(N * dk)\nidwod'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 31, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='34\nExample\nP(E)\nP(b,-e, a, -j, -m) =\nP(B)\ntrue\nfalse\ntrue\nfalse\nBurglary\nEarthquake\nP(b) P(-e) P(a|b,-e) P(-jla) P(-m|a)\n0.002\n0.998\n0.001\n0.999\n=.001x.998x.94x.1x.3=.000028\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\nAlarm\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nfalse\nfalse\n0.001\n0.999\nJohn\nMary\ncalls\ncalls\nA\nP(J|A)\nA\nP(M|A)\ntrue\nfalse\ntrue\nfalse\ntrue\n0.9\n0.1\ntrue\n0.7\n0.3\nfalse\n0.05\n0.95\nfalse\n0.01\n0.99\n32'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 32, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='35\nConditional independence in BNs\nCompare the Bayes net global semantics\nP(X1...,Xn) = II; P(X; I Parents(X;))\nwith the chain rule identity\n(T-!X".x I \'X)d I = (\'x"..Tx)d\nAssume (without loss of generality) that X1...,X, sorted in topological order according to\nthe graph (i.e., parents before children), so Parents(X;) = X1,.,Xi-1\nSo the Bayes net asserts conditional independences P(X, I X1,..,Xi-1) = P(X, I Parents(X;))'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 33, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Conditional independence semantics\nEvery variable is conditionally independent of its non-descendants given its parents\nConditional independence semantics <=> global semantics\nn\nUm\nX\nZj\nY\n34'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 34, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Burglary\nP(B)\nP(E)\ntrue\nfalse\nBurglary\ntrue\nfalse\n0.001\n0.999\n0.002\n0.998\nEarthquake\nBurglary\nEarthquake\nAlarm\nAlarm\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nfalse\nfalse\n0.001\n0.999\n35'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 35, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='38\nExample: Burglary\nP(A)\nAlarm\ntrue\nfalse\nAlarm\nBurglary\nEarthquake\nA\nB\nP(E|A,B)\nA\nP(B|A)\ntrue\nfalse\nBurglary\nEarthquake\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\n36'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 36, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Traffic\nR\nT\n+r\n1/4\n-r\n3/4\n+r\n+t\n3/4\n-t\n1/4\n-r\n+t\n1/2\n-t\n1/2\nRRD\nt\n-r\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 37, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Traffic\n\uf0a7Causal direction\nR\nT\n+r\n1/4\n-r\n3/4\n+r\n+t\n3/4\n-t\n1/4\n-r\n+t\n1/2\n-t\n1/2\n+r\n+t\n3/16\n+r\n-t\n1/16\n-r\n+t\n6/16\n-r\n-t\n6/16\nRR1\n1\nR'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 38, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Example: Reverse Traffic\n\uf0a7Reverse causality?\nT\nR\n+t\n9/16\n-t\n7/16\n+t\n+r\n1/3\n-r\n2/3\n-t\n+r\n1/7\n-r\n6/7\n+r\n+t\n3/16\n+r\n-t\n1/16\n-r\n+t\n6/16\n-r\n-t\n6/16\n1\n1\nRR\nT\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 39, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Causality?\n\uf0a7When Bayes’ nets reflect the true causal patterns:\n\uf0a7Often simpler (nodes have fewer parents)\n\uf0a7Often easier to think about\n\uf0a7Often easier to elicit from experts\n\uf0a7BNs need not actually be causal\n\uf0a7Sometimes no causal net exists over the domain \n(especially if variables are missing)\n\uf0a7E.g. consider the variables Traffic and Drips\n\uf0a7End up with arrows that reflect correlation, not causation\n\uf0a7What do the arrows really mean?\n\uf0a7Topology may happen to encode causal structure\n\uf0a7Topology really encodes conditional independence\nBP(cc1.,...i-1)\n= P(c;parents(Xi))'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'file_path': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 40, 'total_pages': 41, 'format': 'PDF 1.5', 'title': 'CS 294-5: Statistical Natural Language Processing', 'author': 'Preferred Customer', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2016', 'producer': 'Microsoft® PowerPoint® 2016', 'creationDate': "D:20241211181446+05'00'", 'modDate': "D:20241211181446+05'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='43\nSummary\nIndependence and conditional independence are\nimportant forms of probabilistic knowledge\nBayes net encode joint distributions efficiently by\ntaking advantage of conditional independence\nGlobal joint probability = product of local conditionals\nAllows for flexible tradeoff between model accuracy\nand memory/compute efficiency\nA\nA\nB\nB\nA\nB\nC\nB\nE\nE\nE\nStrict Independence\nNaive Bayes\nSparse Bayes Net\nJoint Distribution')]
[Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 0, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='CS 188\nIntroduction to Artiﬁcial Intelligence\nFall 2018\nNote 6\nThese lecture notes are heavily based on notes originally written by Josh Hug and Jacky Liang.\nProbabilistic Inference\nIn artiﬁcial intelligence, we often want to model the relationships between various nondeterministic events.\nIf the weather predicts a 40% chance of rain, should I carry my umbrella? How many scoops of ice cream\nshould I get if the more scoops I get, the more likely I am to drop it all? If there was an accident 15 minutes\nago on the freeway on my route to Oracle Arena to watch the Warriors’ game, should I leave now or in 30\nminutes? All of these questions (and innumerable more) can be answered with probabilistic inference.\nWe’re assuming that you’ve learned the foundations of probability in CS70, so these notes will not review\nbasic concepts of probability like PDFs, conditional probabilities, independence, and conditional indepen-\ndence.'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 0, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='dence.\nIn previous sections of this class, we modeled the world as existing in a speciﬁc state that is always known.\nFor the next several weeks, we will instead use a new model where each possible state for the world has\nits own probability. For example, we might build a weather model, where the state consists of the season,\ntemperature and weather. Our model might say that P(winter, 35◦, cloudy) = 0.023. This number represents\nthe probability of the speciﬁc outcome that it is winter, 35◦, and cloudy.\nMore precisely, our model is a joint distribution, i.e. a table of probabilities which captures the likelihood\nof each possible outcome, also known as an assignment. As an example, consider the table below:\nSeason\nTemperature\nWeather\nProbability\nsummer\nhot\nsun\n0.30\nsummer\nhot\nrain\n0.05\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.05\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 0, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='cold\nrain\n0.05\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.05\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nThis model allows us to answer questions that might be of interest to us, for example:\n• What is the probability that it is sunny? P(W = sun)\n• What is the probability distribution for the weather, given that we know it is winter? P(W | S = winter)\n• What is the probability that it is winter, given that we know it is rainy and cold? P(S = winter | T =\ncold,W = rain)\n• What is the probability distribution for the weather and season give that we know that it is cold?\nP(S,W | T = cold)\nCS 188, Fall 2018, Note 6\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 1, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Given a joint PDF, we can trivially perform compute any desired probablity distribution P(Q1 ...Qk | e1 ...ek)\nusing a simple and intuitive procedure known as inference by enumeration, for which we deﬁne three types\nof variables we will be dealing with:\n1. Query variables Qi, which are unknown and appear on the left side of the probability distribution we\nare trying to compute.\n2. Evidence variables ei, which are observed variables whose values are known and appear on the right\nside of the probability distribution we are trying to compute.\n3. Hidden variables, which are values present in the overall joint distribution but not in the distribution\nwe are currently trying to compute.\nIn this procedure, we collect all the rows consistent with the observed evidence variables, sum out all the\nhidden variables, and ﬁnally normalize the table so that it is a probability distribution (i.e. values sum to 1).'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 1, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='For example, if we wanted to compute P(W | S = winter), we’d select the four rows where S is winter, then\nsum out over T and normalize. This yields the following probability table:\nW\nS\nUnnormalized Sum\nProbability\nsun\nwinter\n0.10+0.15 = 0.25\n0.25/(0.25+0.25) = 0.5\nrain\nwinter\n0.05+0.20 = 0.25\n0.25/(0.25+0.25) = 0.5\nHence P(W = sun | S = winter) = 0.5 and P(W = rain | S = winter) = 0.5, and we learn that in winter\nthere’s a 50% chance of sun and a 50% chance of rain (classic California weather).\nSo long as we have the joint PDF table, inference by enumeration (IBE) can be used to compute any desired\nprobablity distribution, even for multiple query variables Q1...Qk.\nBayes Nets (Representation)\nWhile inference by enumeration can compute probabilities for any query we might desire, representing an\nentire joint distribution in the memory of a computer is impractical for real problems - if each of n variables'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 1, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='we wish to represent can take on d possible values (it has a domain of size d), then our joint distribution\ntable will have dn entries, exponential in the number of variables and quite impractical to store!\nBayes nets avoid this issue by taking advantage of the idea of conditional probability. Rather than storing\ninformation in a giant table, probabilities are instead distributed across a large number of smaller local\nprobability tables along with a directed acyclic graph (DAG) which captures the relationships between\nvariables. The local probability tables and the DAG together encode enough information to compute any\nprobability distribution that we could have otherwise computed given the entire joint distribution.\nSpeciﬁcally, each node in the graph represents a single random variable and each directed edge represents\none of the conditional probability distributions we choose to store (i.e. an edge from node A to node B'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 1, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='indicates that we store the probability table for P(B|A)). Each node is conditionally independent of all its\nancestor nodes in the graph, given all of its parents. Thus, if we have a node representing variable X, we\nstore P(X|A1,A2,...,AN), where A1,...,AN are the parents of X.\nAs an example of a Bayes Net, consider a model where we have ﬁve binary random variables described\nbelow:\nCS 188, Fall 2018, Note 6\n2'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 2, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='• B: Burglary occurs.\n• A: Alarm goes off.\n• E: Earthquake occurs.\n• J: John calls.\n• M: Mary calls.\nAssume the alarm can go off if either a burglary or an earthquake occurs, and that Mary and John will call\nif they hear the alarm. We can represent these dependencies with the graph shown below.\nAs a reality check, it’s important to internalize that Bayes Nets are only a type of model. Models attempt\nto capture the way the world works, but because they are always a simpliﬁcation they are always wrong.\nHowever, with good modeling choices they can still be good enough approximations that they are useful for\nsolving real problems in the real world. In general, they will not account for every variable or even every\ninteraction between variables.\nReturning to our discussion, we formally deﬁne a Bayes Net as consisting of:\n• A directed acyclic graph of nodes, one per variable X.\n• A conditional distribution for each node P(X|A1 ...An), where Ai is the ith parent of X, stored as a'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 2, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='conditional probability table or CPT. Each CPT has n+2 columns: one for the values of each of the\nn parent variables A1 ...An, one for the values of X, and one for the conditional probability of X.\nIn the alarm model above, we would store probability tables P(B),P(E),P(A | B,E),P(J | A) and P(M | A).\nGiven all of the CPTs for a graph, we can calculate the probability of a given assignment using the chain\nrule: P(X1,X2,...,Xn) = ’n\ni=1 P(Xi|parents(Xi)).\nFor the alarm model above, we might calculate the probability of one event as follows: P(−b,−e,+a,+j,−m) =\nP(−b)·P(−e)·P(+a|−b,−e)·P(+ j|+a)·P(−m|+a).\nCS 188, Fall 2018, Note 6\n3\nB\nE\nA\nJ\nM'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 3, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='This works because of the conditional independence relationships given by the graph. Speciﬁcally, we rely\non the fact that P(xi|x1,...,xi−1) = P(xi|parents(Xi)). Or in other words, that the probability of a speciﬁc\nvalue of Xi depends only on the values assigned to Xi’s parents.\nBayes Nets (Inference)\nInference is the process of calculating the joint PDF for some set of query variables based on some set\nof observed variables. We can solve this problem naively by forming the joint PDF and using inference by\nenumeration as described above. This requires the creation of and iteration over an exponentially large table.\nAn alternate approach is to eliminate variables one by one. To eliminate a variable X, we:\n1. Join (multiply together) all factors involving X.\n2. Sum out X.\nA factor is deﬁned simply as an unnormalized probability. At all points during variable elimination, each\nfactor will be proportional to the probability it corresponds to but the underlying distribution for each factor'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 3, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='won’t necessarily sum to 1 as a probability distribution should.\nLet’s make these ideas more concrete with an example. Suppose we have a model as shown below, where\nT, C, S, and E can take on binary values, as shown below. Here, T represents the chance that an adventurer\ntakes a treasure, C represents the chance that a cage falls on the adventurer given that he takes the treasure,\nS represents the chance that snakes are released if an adventurer takes the treasure, and E represents the\nchance that the adventurer escapes given information about the status of the cage and snakes.\nIn this case, we have the factors P(T), P(C|T), P(S|T), and P(E|C,S). Suppose we want to calculate\nP(T| + e). The inference by enumeration approach would be to form the 16 row joint PDF P(T,C,S,E),\nselect only the rows corresponding to +e, then summing out C and S and ﬁnally normalizing.\nThe alternate approach is to eliminate C, then S, one variable at a time. We’d proceed as follows:'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 3, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='The alternate approach is to eliminate C, then S, one variable at a time. We’d proceed as follows:\n• Join (multiply) all the factors involving C, forming P(C,+e|T,S) = P(C|T)·P(+e|C,S).\n• Sum out C from this new factor, leaving us with a new factor P(+e|T,S).\n• Join all factors involving S, forming P(+e,S|T) = P(S|T)·P(+e|T,S).\nCS 188, Fall 2018, Note 6\n4\nT\nC\nS\nE'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 4, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='• Sum out S, yielding P(+e|T).\nOnce we have P(+e|T), we can easily compute P(T|+e).\nWhile this process is more involved from a conceptual point of view, the maximum size of any factor\ngenerated is only 8 rows instead of 16 as it would be if we formed the entire joint PDF.\nAn alternate way of looking at the problem is to observe that the calculation of P(+e,T) can either be done,\nas it is in inference by enumeration, as follows:\nÂ\ns Â\nc\nP(T)P(s|T)P(c|T)P(+e|c,s)\nVariable elimination is equivalent to calculating P(+e,T) as follows:\nP(T)Â\ns\nP(s|T)Â\nc\nP(c|T)P(+e|c,s)\nBayes Nets (Sampling)\nAn alternate approach for probabilistic reasoning is to implicitly calculate the probabilities for our query by\nsimply counting samples.\nFor example, suppose we wanted to calculate P(T| + e). If we had a magic machine that could generate\nsamples from our distribution, we could collect all samples for which the adventurer escapes the maze, and'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 4, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='then compute the fraction of those escapes for which the adventurer also took the treasure. Put differently,\nif we could run simulations of say, a few million adventurers, we’d easily be able to compute any inference\nwe’d want just by looking at the samples.\nGiven a Bayes Net model, we can easily write a simulator. For example, consider the CPTs given below for\nthe simpliﬁed model with only two variables T and C.\nCS 188, Fall 2018, Note 6\n5\nT\nP(T)\n+t\n0.99\nT\n-t\n0.01\nT\nC\nP(C|T)\n+t\n+c\n0.95\n+t\n-C\n0.05\n-t\n+C\n0.0\n-t\n-C\n1.0'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 5, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='A simple simulator in Python would be as follows:\nimport random\ndef get_t():\nif random.random() < 0.99:\nreturn True\nreturn False\ndef get_c(t):\nif t and random.random() < 0.95:\nreturn True\nreturn False\ndef get_sample():\nt = get_t()\nc = get_c(t)\nreturn [t, c]\nWe call this simple approach prior sampling. The downside of this approach is that it may require the\ngeneration of a very large number of samples in order to perform analysis of unlikely scenarios. If we\nwanted to compute P(C|−t), we’d have to throw away 99% of our samples.\nOne way to mitigate this this problem, we can modify our procedure to early reject any sample inconsistent\nwith our evidence. For example, for the query P(C|−t), we’d avoid generating a value for C unless t is true.\nThis still means we have to throw away most of our samples, but at least the bad samples we generate take\nless time to create. We call this approach rejection sampling.'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 5, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='less time to create. We call this approach rejection sampling.\nThese two approaches work for the same reason, which is that any valid sample occurs with the same\nprobability as speciﬁed in the joint PDF. In other words, the probability of every sample is based on the\nproduct of every CPT, or as I personally call it, the "every CPT participates principle".\nA more exotic approach is likelihood weighting, which ensures that we never generate a bad sample. In\nthis approach, we manually set all variables equal to the evidence in our query. For example, if we wanted\nto compute P(C| −t), we’d simply declare that t is false. The problem here is that this may yield samples\nthat are inconsistent with the correct distribution. As an example, consider the more complex four variable\nmodel for T, C, S, and E given earlier in these notes. If we wanted to compute P(T,S,+c,+e), and simply\npicked values for T and S without taking into account the fact that c = false, and e = true, then there’s no'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 5, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='guarantee that our samples actually obey the joint PDF given by the Bayes Net. For example, if the cage\nonly ever falls if the treasure is taken, then we’d want to ensure that T is always true instead of using the\nP(T) distribution given in the Bayes Net.\nPut differently, if we simply force some variables equal to the evidence, then our samples occur with proba-\nbility given only equal to the products of the CPTs of the non-evidence variables. This means the joint PDF\nhas no guarantee of being correct (though may be for some cases like our two variable Bayes Net). Instead,\nif we have sampled variables Z1 through Zp and ﬁxed evidence variables E1 through Em a sample is given\nby the probability P(Z1...Zp,E1...Em) = ’p\ni P(Zi)|Parents(Zi). What is missing is that the probability of a\nsample does not include all the probabilities of P(Ei|Parents(Ei)), i.e. not every CPT participates.\nLikelihood weighting solves this issue by using a weight for each sample, which is the probability of the'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 5, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='evidence variables given the sampled variables. That is, instead of counting all samples equally, we can\nCS 188, Fall 2018, Note 6\n6'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 6, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='deﬁne a weight w j for sample j that reﬂects how likely the observed values for the evidence variables are,\ngiven the sampled values. In this way, we ensure that every CPT participates. To do this, we iterate through\neach variable in the Bayes net, as we do for normal sampling), sampling a value if the variable is not an\nevidence variable, or changing the weight for the sample if the variable is evidence.\nFor example, suppose we want to calculate P(T| + c,+e). For the jth sample, we’d perform the following\nalgorithm:\n• Set wj to 1.0, and c = true and e = true.\n• For T: This is not an evidence variable, so we sample tj from P(T).\n• For C: This is an evidence variable, so we multiply the weight of the sample by P(+c|tj), i.e. w j =\nw j ·P(+c|tj).\n• For S: sample sj from P(S | tj).\n• For E: multiply the weight of the sample by P(+e|+c,s j), i.e. w j = w j ·P(+e|+c,sj).\nThen when we perform the usual counting process, we weight sample j by w j instead of 1, where 0 <='), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 6, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Then when we perform the usual counting process, we weight sample j by w j instead of 1, where 0 <=\nw j <= 1. This approach works because in the ﬁnal calculations for the probabilities, the weights effectively\nserve to replace the missing CPTs. In effect, we ensure that the weighted probability of each sample is given\nby P(z1...zp,e1...em) = [’p\ni P(zi | Parents(zi))]·[’m\ni P(ei) | Parents(ei))].\nFor all three of our sampling methods (prior sampling, rejection sampling, and likelihod weighting), we\ncan get increasing amounts of accuracy by generating additional samples. However, of the three, likelihood\nweighting is the most computationally efﬁcient, for reasons beyond the scope of this course.\nGibbs Sampling is a fourth approach for sampling. In this approach, we ﬁrst set all variables to some totally\nrandom value (not taking into account any CPTs). We then repeatedly pick one variable at a time, clear its'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 6, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='value, and resample it given the values currently assigned to all other variables.\nFor the T,C,S,E example above, we might assign t = true, c = true, s = false, and e = true. We then pick\none of our four variables to resample, say S, and clear it. We then pick a new variable from the distribution\nP(S| + t,+c,+e). This requires us knowing this conditional distribution. It turns out that we can easily\ncompute the distribution of any single variable given all other variables. More speciﬁcally, P(S|T,C,E) can\nbe calculated only using the CPTs that connect S with its neighbors. Thus, in a typical Bayes Net, where\nmost variables have only a small number of neighbors, we can precompute the conditional distributions for\neach variable given all of its neighbors in linear time.\nWe will not prove this, but if we repeat this process enough times, our later samples will eventually converge\nto the correct distribution even though we may start from a low-probability assignment of values. If you’re'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 6, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='curious, there are some caveats beyond the scope of the course that you can read about under the Failure\nModes section of the Wikipedia article for Gibbs Sampling.\nBayes Nets (D-Separation)\nOne useful question to ask about a set of random variables is whether or not one variable is independent from\nanother, or if one random variable is conditionally independent of another given a third random variable.\nBayes’ Nets representation of joint probability distributions gives us a way to quickly answer such questions\nby inspecting the topological structure of the graph.\nCS 188, Fall 2018, Note 6\n7'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 7, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='We already mentioned that a node is conditionally independent of all its ancestor nodes in the graph\ngiven all of its parents.\nWe will present all three canonical cases of connected three-node two-edge Bayes’ Nets, or triples, and the\nconditional independence relationships they express.\nCausal Chains\nFigure 1: Causal Chain with no observations.\nFigure 2: Causal Chain with Y observed.\nFigure 1 is a conﬁguration of three nodes known as a causal chain. It expresses the following representation\nof the joint distribution over X, Y, and Z:\nP(x,y,z) = P(z|y)P(y|x)P(x)\nIt’s important to note that X and Z are not guaranteed to be independent, as shown by the following coun-\nterexample:\nP(y|x) =\n(\n1\nif x = y\n0\nelse\nP(z|y) =\n(\n1\nif z = y\n0\nelse\nIn this case, P(z|x) = 1 if x = z and 0 otherwise, so X and Z are not independent.\nHowever, we can make the statement that X ?? Z | Y, as in Figure 2. Recall that this conditional indepdence\nmeans:\nP(X|Z,Y) = P(X|Y)\nWe can prove this statement as follows:'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 7, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='means:\nP(X|Z,Y) = P(X|Y)\nWe can prove this statement as follows:\nP(X|Z,y)\n=\nP(X,Z,y)\nP(Z,y)\n= P(Z|y)P(y|X)P(X)\nÂx P(X,y,Z)\n= P(Z|y)P(y|X)P(X)\nP(Z|y)Âx P(y|x)P(x)\n=\nP(y|X)P(X)\nÂx P(y|x)P(x) = P(y|X)P(X)\nP(y)\n= P(X|y)\nAn analogous proof can be used to show the same thing for the case where X has multiple parents. To\nsummarize, in the causal chain chain conﬁguration, X ?? Z | Y.\nCommon Cause\nAnother possible conﬁguration for a triple is the common cause. It expresses the following representation:\nP(x,y,z) = P(x|y)P(z|y)P(y)\nJust like with causal chain, we can show that X is not guaranteed to be independent of Z with the following\ncounterexample distribution:\nCS 188, Fall 2018, Note 6\n8\nX\nY\nZX\nY\nZ'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 8, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Figure 3: Common Cause with no observations.\nFigure 4: Common Cause with Y observed.\nP(x|y) =\n(\n1\nif x = y\n0\nelse\nP(z|y) =\n(\n1\nif z = y\n0\nelse\nThen P(x|z) = 1 if x = z and 0 otherwise, so X and Z are not independent.\nBut it is true that X ?? Z | Y. That is, X and Z are independent if Y is observed as in Figure 4. We can show\nthis as follows:\nP(X|Z,y) = P(X,Z,y)\nP(Z,y)\n= P(X|y)P(Z|y)P(y)\nP(Z|y)P(y)\n= P(X|y)\nCommon E↵ect\nThe ﬁnal possible conﬁguration for a triple is the common effect, as shown in the ﬁgures below.\nFigure 5: Common Effect with no observations.\nFigure 6: Common Effect with Y observed.\nIt expresses the representation:\nP(x,y,z) = P(y|x,z)P(x)P(z)\nIn the conﬁguration shown in Figure 5, X and Z are independent: X ?? Z. However, they are not necessarily\nindependent when conditioned on Y (Figure 6). As an example, suppose all three are binary variables. X\nCS 188, Fall 2018, Note 6\n9\nY\nX\nZY\nX\nZX\nZ\nYX\nZ\nY'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 9, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='and Z are true and false with equal probability:\nP(X = true)\n=\nP(X = false) = 0.5\nP(Z = true)\n=\nP(Z = false) = 0.5\nand Y is determined by whether X and Z have the same value:\nP(Y|X,Z) =\n8\n>\n<\n>\n:\n1\nif X = Z and Y = true\n1\nif X 6= Z and Y = false\n0\nelse\nThen X and Z are independent if Y is unobserved. But if Y is observed, then knowing X will tell us the value\nof Z, and vice-versa. So X and Z are not conditionally independent given Y.\nCommon Effect can be viewed as “opposite” to Causal Chains and Common Cause – X and Z are guaranteed\nto be independent if Y is not conditioned on. But when conditioned on Y, X and Z may be dependent\ndepending on the speciﬁc probability values for P(Y | X,Z)).\nThis same logic applies when conditioning on descendents of Y in the graph. If one of Y’s descendent nodes\nis observed, as in Figure 7, X and Z are not guaranteed to be independent.\nFigure 7: Common Effect with child observations.\nGeneral Case, and D-separation'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 9, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='Figure 7: Common Effect with child observations.\nGeneral Case, and D-separation\nWe can use the previous three cases as building blocks to help us answer conditional independence questions\non an arbitrary Bayes’ Net with more than three nodes and two edges. We formulate the problem as follows:\nGiven a Bayes Net G, two nodes X and Y, and a (possibly empty) set of nodes {Z1,...Zk} that represent\nobserved variables, must the following statement be true: X ?? Y|{Z1,... Zk}?\nD-separation (directed separation) is a property of the structure of the Bayes Net graph that implies this\nconditional independence relationship, and generalizes the cases we’ve seen above. If a set of variables\nZ1,···Zk d-separates X and Y, then X ?? Y | {Z1,···Zk} in all possible distributions that can be encoded by\nthe Bayes net.\nCS 188, Fall 2018, Note 6\n10\nX\nZ\nY\nW'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 10, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='We start with an algorithm that is based on a notion of reachability from node X to node Y. (Note: this\nalgorithm is not quite correct! We’ll see how to ﬁx it in a moment.)\n1. Shade all observed nodes {Z1,...Zk} in the graph.\n2. If there exists an undirected path from X and Y that is not blocked by a shaded node, X and Y are\n“connected”.\n3. If X and Y are connected, they’re not conditionally independent given {Z1,...Zk}. Otherwise, they\nare.\nHowever, this algorithm only works if the Bayes’ Net has no Common Effect structure within the graph, be-\ncause if it exists, then two nodes are “reachable” when the Y node in Common Effect is activated (observed).\nTo adjust for this, we arrive at the following d-separation algorithm:\n1. Shade all observed nodes {Z1,...,Zk} in the graph.\n2. Enumerate all undirected paths from X to Y.\n3. For each path:\n(a) Decompose the path into triples (segments of 3 nodes).\n(b) If all triples are active, this path is active and d-connects X to Y.'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 10, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content='(b) If all triples are active, this path is active and d-connects X to Y.\n4. If no path d-connects X and Y, then X and Y are d-separated, so they are conditionally independent\ngiven {Z1,...,Zk}\nAny path in a graph from X to Y can be decomposed into a set of 3 consecutive nodes and 2 edges - each\nof which is called a triple. A triple is active or inactive depending on whether or not the middle node is\nobserved. If all triples in a path are active, then the path is active and d-connects X to Y, meaning X is\nnot guaranteed to be conditionally independent of Y given the observed nodes. If all paths from X to Y are\ninactive, then X and Y are conditionally independent given the observed nodes.\nActive triples: We can enumerate all possibilities of active and inactive triples using the three canonical\ngraphs we presented above in Figure 8 and 9.\nCS 188, Fall 2018, Note 6\n11'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 11, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="Figure 8: Active triples\nFigure 9: Inactive triples\nExamples\nHere are some examples of applying the d-separation algorithm:\nThis graph contains the common effect and causual\nchain canonical graphs.\na) R ?? B – Guaranteed\nb) R ?? B | T – Not guaranteed\nc) R ?? B | T 0 – Not guaranteed\nd) R ?? T 0 | T – Guaranteed\nCS 188, Fall 2018, Note 6\n12\n0008\nC\nOO\n00\nO\nOQ\nOQR\nB\nT\nT'"), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'file_path': 'Data\\AI-Lecture Note 6.pdf', 'page': 12, 'total_pages': 13, 'format': 'PDF 1.4', 'title': 'n6.pdf', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Preview', 'producer': 'macOS Version 11.5.2 (Build 20G95) Quartz PDFContext', 'creationDate': "D:20230917164637Z00'00'", 'modDate': "D:20230917164637Z00'00'", 'trapped': '', 'Instructor_name': 'Dr. Seemab Latif'}, page_content="This graph contains combinations of all three canon-\nical graphs (can you list them all?).\na) L ?? T 0 | T – Guaranteed\nb) L ?? B – Guaranteed\nc) L ?? B | T – Not guaranteed\nd) L ?? B | T 0 – Not guaranteed\ne) L ?? B | T,R – Guaranteed\nThis graph contains combinations of all three canon-\nical graphs.\na) T ?? D – Not guaranteed\nb) T ?? D | R – Guaranteed\nc) T ?? D | R,S – Not guaranteed\nConclusion\nTo summarize, Bayes’ Nets is a powerful representation of joint probability distributions. Its topological\nstructure encodes independence and conditional independence relationships, and we can use it to model\narbitrary distributions to perform inference and sampling.\nCS 188, Fall 2018, Note 6\n13\nL\nR\nB\nD\nT\nT'R\nT\nD\nS")