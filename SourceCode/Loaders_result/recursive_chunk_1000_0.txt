Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 1
10th Sept 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 1}
Content: A tech enthusiasts committed to AI 
Who Am I…
5 Years of AI Innovation
Team
• 11-member team with AI experience
Dr. Seemab Latif
• HoD AI&DS
• Tenured Associate Professor
• PhD Artificial Intelligence (Uni of Manchester)
• 70 plus publications
• 7 commercial IPs
• CPInS Lab
• Funded Research and Commercial Projects
• UG, PG, PhD Research
YCHINA-PAKISTAN
INTELLIGENTSYSTEMS(EPInS)LABaawazeS
OneScreen'SKYELECTRICG
C
SkyLabs
AI

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 2}
Content: What is AI
• Artificial intelligence is the ability of machines to perform tasks that 
are typically associated with human intelligence, such as learning and 
problem-solving.
• Such machines have the ability to learn about the world that 
surrounds them and take actions that will have the best chances of 
achieving success. 
• Research into AI involves using a lot of tools from other sciences such 
as psychology, linguistics, computer science, and many others.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 3}
Content: AI vs. HI
• Human intelligence and 
artificial intelligence go handin-hand. 
• AI can handle repetitive, datadriven tasks and give databased results. 
• Human intelligence can work 
on creative, emotional and 
critically complex tasks
0:

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 4}
Content: Areas of AI
ARTIFICIALINTELLIGENCE
Programswiththeabilityto
learnandreasonlikehumans
MACHINELEARNING
Algorithmswiththeabilitytolearn
withoutbeingexplicitlyprogrammed
DEEPLEARNING
Subsetofmachine learning
inwhichartificialneural
networksadaptandlearn
fromvastamountsofdata

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 5}
Content: Applications 
of Artificial 
Intelligence
Automatic
Language
Recognition
Image
Translation
Medical
Diagnosis
Speech
Stock
Recognition
Market
trading
Applications
of
Online
Fraud
Traffic
Machinelearning
Detection
Prediction
Virtual
Product
Personal
recommend
Email Spam
Assistant
andMalware
-ations
Self
driving
Filtering
cars

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 6}
Content: WHAT
CAN DO

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 7}
Content: Source: https://kling.kuaishou.com
K可灵AI可灵AI可灵AI可灵AIC可灵AIo-

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 8}
Content: Create Audio
• AI music generators, audio tools, and voice generators, you can add 
a soundtrack to a podcast, add a voice overlay to marketing videos, 
and even create your own music to enhance projects.
• For example, using Mubert, you can easily generate music with a text 
prompt and a few clicks. We used the prompt "First lecture of the 
semester, hurray” and created this clip in less than a minute.
ProductsBlog Aboutus Contact BecomeAffiliateForArtists
Sign Up LogIn
PRODUCTHUNT
#1Product of theDay
Human
XAl GenerativeMusic
For your video content,podcastsand apps
Generateatracknow
MubertRender
Mubert Studio
For contentcreators
Forartists
Createasoundtrackthatwill
Earn moneyon tracks,samples
fit yourcontent'smood,
andloops.TeamupwithAl to
durationandtempo.Instantly,
producesomethingincredible.
easily,perfectly.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 9}
Content: Keep Track of Your Business
Running a business is time-consuming, especially if it’s a one-person operation. Thanks 
to advancements in artificial intelligence, hiring a human to assist you is no longer 
required. Whether you need help writing a blog post, managing appointments, or 
financial tips, AI assistants can help with several tasks to help you be more productive. 
For example, using a financial assistant, such as Tykr, you can manage your stock 
portfolio, get insights on investment opportunities, and learn the basics of investing.
ykr
Benefits
Reviews
Pricing
Courses
Webinars
Education
Support
Login
Yes,Iwanta free trial
Beating inflation
is way easier
Tykr Score
than you think
89元100
Tykr isa stockscreener and educationplatform all-
in-one that helpsyoumakebetterinvestment
This stock hasa scoreof 89/100and anMargin
decisions andbeatinflation.
of Safety of 72%thereforeit'sOnSale
OnSale
5160
Yes,wantareetria
5120
5.40
No credit card required
Trustindex
Freefor14days
6Sep

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 9}
Content: 16Nov
Cancel at any time
Basedon161reviews
S150.82LoW
52WChange
S180.56High

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 10}
Content: Create Avatars
AI avatar generators are all the rage right now. Thanks to companies like Picsart, Lensa AI, 
and Synthesia, you can create static and video avatars. Whether you’re looking to establish a 
more professional representation for your company, revamp your social media avatars, or even 
fashion lifelike 3D avatars for marketing videos or online business chat, these tools have you 
covered.
Lensa
Get the app
Profile
Create Your Own
Magic Al Avatars
Generatepersonalized avatars infive steps
Getstarted

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 11}
Content: Help with Branding
If you’re looking to rebrand your current business or start a new venture, artificial intelligence 
can help. There are some great logo generators to spark creativity and fantastic features 
available through companies like Wix Logo Generator that will make you feel like a graphic 
design professional in no time. Whether you want to create social media posts or build out your 
brand standards, branding has never been easier.
WIX
Logo Maker
TopLogoCategories
LogosbyDesign
More
christinagwira
Free Business Tools> Logo Maker
LOGOMAKER
Design a professional
logo in minutes
GetMyLogo→
G
Home
Shop
Our Story.
米
米
GNDOS

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 12}
Content: Sales and Marketing Tools
If you want to create leads and grow revenue, some excellent AI sales and marketing tools help 
you take your business to the next level. These tools can help you build an email list, reach out 
to them with video assets, and explore opportunities to analyse your client interactions. For 
example, using Seamless.AI‘s built-in search engine, you can leverage the power of AI to update 
your contact lists with the most up-to-date contact information. Additionally, using software 
such as Ocoya, you can create social media posts and schedule them, giving you more time to 
focus on other aspects of your business.
Seamless.Al
Products
Customers
Company
Pricing
Login
GetaDemo
GetStartedFree
TheWorld'sBest
Contacts
Companies
SalesLeads
R
Jessica Smith
Senior SalesDirector,Captive Tech
Ouralessoftwarefindsverifiedcelphones,mails,and
directdialsforanyoneyouneedtosellto.Get50freecredits
withnocreditcarddownanddiscoverwhy4o0,o00+
Mason Johnson

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 12}
Content: Creative Manager,OrgCache
companiesuseSeamless.Altogrowtheirbusiness.
501-456-9876
X
m.johnson@orgcache.com
Business Email
GetStartedFree
AshleyWilliams
Hiring Manager,HotshotDesign Co.
NoCreditCard Required
★★★★★
4.2/5G2Rating
OR
SignupwithGoogle
SignupwithLinkedln
Christopher Miller
VPof Sales,City CRM
By submitting thisform,youagreeto theSeamless.AlTermsofUse&PrivacyPolicy
Trustedby4o0,0o0+users

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 13}
Content: Assist in Image Editing
If you’re a freelancer or web agency owner, you know how challenging it can be to get good 
client photos. Thankfully, artificial intelligence can turn you into a photo editing wizard while 
saving a ton of time. Whether you’re looking to upscale images, enhance them, or create 
new compositions with Photoshop, image editing tools help you finish the job quickly.
E

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 14}
Content: Have In
-Depth Conversations
One of the most creative ways to use artificial intelligence is through chatbots like Writesonic. 
Built on OpenAI’s GPT-4, you can converse with it to get answers to questions, help you develop 
blog post ideas, and more. One of the coolest things about Writesonic is their Botsonic tool, 
which allows you to create your own chatbot in a few minutes. This is a great tool to create 
personalized chat experiences for your site’s visitors providing answers directly related to your 
products and services. Alternatively, you can incorporate Character.AI to gain a unique 
perspective by chatting with historical figures, celebrities or any personality you choose.
IntroducingthesuperiorAlArticleWriter5.0
AIArticle
Craftfactual,document-awarearticlesinyourownbrandvoice
TryItOutNow
Writer5.0
inminutes.
Writesonic
Features
Resources
Chatsonic
Botsonic
Pricing
Academy
Signin
Getstarted
Best Al Writer for Creating
Freelancer
Marketer

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 14}
Content: CreateSEO-optimized andplagiarism-freecontent
foryourblogs,ads,emails,andwebsitexfastr.
Blogger
StartWriting ForFree
Nocreditcardrequired.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 15}
Content: Increase Productivity
No matter what you need AI for, whether it’s to get insights on how your website is 
performing or advanced SEO techniques, there are many AI productivity tools to help you 
get the job done fast. Need a good CRM? No problem. Freshworks Freddy AI can help with 
marketing automation, all while building a better system to interact with your customers. 
Alternatively, if you need a way to transcribe meeting notes, check out Otter AI.
freshworks
Products
Platform
Resources
Demo
Pricing
FreeTrial
Freshsales
Features
Integrations
What'sNew
FREDDYAI
Redefinecustomerrelationshipswith
artificialintelligence
Getactionableinsightsacrossthecustomerjourneyanddeliver
highlypersonalized engagementwith anAl-powered CRM.
SIGNUP

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 16}
Content: Develop AI Software
One of the most powerful uses of artificial intelligence is the ability to create your own. 
With AI development tools, you can propel your business into the future by incorporating 
systems to assist with time-consuming tasks, freeing you up to perform more important 
feats. For example, if you’re planning on starting your own international ecommerce 
marketplace, you could incorporate Google’s Translation AIto provide content based on a 
user’s location, then translating your text into the appropriate language.
Google Cloud
Overview
SolutionsProductsPricing
Resources
ContactUs
α
Docs Support
English
Console
：
Startfree
Alandmachine learning products
InnovativeAl andmachine learningproducts,solutions,andservicespoweredby Google'sresearch
and technology.Newcustomersget$3oo infreecreditstorun,test,and deployworkloads.
Getstartedforfree
Contact sales
GenerativeAl
Data Science
ResponsibleAl
BuildgenerativeAlapplicationsquickly,

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 16}
Content: Generateinsightsfromdatawithourcomplete
Discovertoolsandframeworkstounderstandand
efficientlyandresponsiblypoweredbyGoogles
suiteofdatamanagement,analytics,and
interpret your machine learning models.Learn
most advanced technology.
machine learning tools.
whyvalues-basedAlisnecessaryfordevelopers.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 17}
Content: Write Copy
One of the most notable uses for artificial intelligence is for writing copy. Using AI writing 
software such as Copy.AI, Writesonic, or ChatGPT, you can create new copy for social 
media posts, informational text, blogs, and more.
Alternatively, if you need a little help making your copy more focused and concise, you can 
use a rewriter tool, such as Quillbot, to help you clarify your original content to flow better, 
all while retaining the authentic tone it was written in.
QuillBot
Paraphraser
Upgrade to Premium
中
Modes:
StandardFluency
FormalSimpleCreativeExpand
Shorten
Synonyms:
目
QuillBotwill rewriteyour text.Startbywritingorpastingsomethinghereand
thenpresstheParaphrasebutton.
66
Try SampleText
PasteText
?
UploadDoc
Paraphrase
口

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 18}
Content: Make Creative Assets
Imagery is one of the most important aspects of any website or marketing material. With 
the introduction of AI art generators in late 2022, anyone, regardless of the level of artistic 
ability, can create beautiful images for their projects. Between Adobe Firefly, Stable 
Diffusion, Midjourney, and others, there is no shortage of platforms. If you need more than 
images, these AI design tools can help you create brochures, social media templates, color
palettes, and more.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 19}
Content: Write Code
In the past, if you wanted to create a custom plugin or an entire static HTML, you’d need to 
prepare yourself for a long project. With the number of AI coding assistants hitting the 
market at the speed of light, you no longer have to spend countless hours coding on your 
own. Using software such as GitHub Copilot, you can code faster and more efficiently than 
ever before. The best part about these coding assistants is the majority of them will work 
alongside your favorite code editor, so you don’t have to spend time learning a new 
platform.
Search or jump to...
Pullrequests
Issues
CodespacesMarketplaceExplore
Features
Actions
Packages
Security
Codespaces
Copilot
Codereview
Search
Issues
Discussions
YourAlpairprogrammer
GitHub Copilot uses the OpenAl Codex to suggest code and entire functions in
real-time,rightfrom your editor.
GetCopilot>
Compare plans
sentiments.ts
oowrite_sql.go
parse_expenses.py
addresses.rb

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 20}
Content: Build Websites
Making a website is a lot easier now, thanks to AI website builders. Big players in the industry, 
such as Wix and Hostinger, have started offering services to streamline the process. You can 
answer a few questions through Wix and have a custom website in a few minutes. Other 
tools, like Framer, can generate a fully functional website with a text prompt.
Alternatively, you can combine tools like Midjourney and ChatGPT to generate images and 
content for your new site. If you’re a WordPress user, there are even a few AI plugins to make 
working in WordPress a lot easier.
VessTete
Conafi Frrunss
Det Our Reorn Wetout
Weo
Srenet Cortiee
BSOWGOSS
CAIHONSTS
FADOr
Cor
lon6m.
sooeble
boliamie

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 21}
Content: Generate Video Assets
Another excellent use of artificial intelligence is AI video generators. You can create 
marketing videos from a blog post URL using Pictory. Alternatively, you can create realistic 
training videos with animated avatars using Synthesia.
There are even programs, such as Runway, that will give you the tools to improve existing 
videos by adding slow-motion effects, making color enhancements, and removing artifacts.
synthesia
Create a free AIvideo
Thisiswhatyourvideowilllooklike
Selectatemplate&edityourscript.Political,sexualanddiscriminatorycontent
will notbeapproved.
1
SELECTVIDEOTEMPLATE
SynthesiaDemo
SalesPitch
Learning&Development
ACMECOMPANY
Compliment
How-ToVideo
Evelyn Johnson
Jonathan
EDITYOURVIDEOSCRIPT
2
You canuse anypopularlanguage
Evelyn
HeyJohn,EvelynfromAcmehere！Aspromised,Iam
sendingyouabriefsummaryofthekeypointsofthe
Hey John,
proposalwediscussedtoday.Iwillputyouintouchwith
Evelyn fromAcmeherelAspromised,Iamsendingyou abrief

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 21}
Content: summary of thekeypointsof theproposal we discussed today.
youronboardingpartnerrightaway.
Iwill put you in touch withyouronboarding partner right away.
-aura
Bridget
English(US)-Meticulous
Play script
3charactersleft
Kenneth
Continue

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 22}
Content: Improve SEO
The last AI tools on our list can help you get your site’s SEO up to par. With AI SEO tools, you 
can generate page titles and meta descriptions in bulk, rewrite copy, check for plagiarism, and 
develop SEO-friendly outlines, and more. Some tools, such as Alli AI, will help you quickly 
identify any SEO problem areas and provide steps to correct them.
AlliAI
Features
Pricing
Company
Helpdesk
FAQ
Demo
Login
Starta Trial
Optimize, Automate,
Deploy and Scale SEO.
AttentionAgenciesandSEO teams:NoCodingRequired.Workswithany
CMs.Makethousandstomillionsofcodeandcontentchangesinminutes.
ManageallyourSEofromonedashboard.Installautomate,andscale.
StartaFreeTrial

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 23}
Content: ousoNs?
?

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 24}
Content: We become what we behold.
We shape our tools,
and thereafter our tools shape us.
-Marshall McLuhan

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 3
1 Oct 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 1}
Content: AI: Representation and Problem Solving
Informed Search
Slide credits: CMU AI, http://ai.berkeley.edu
Mdrmil

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 2}
Content: Uninformed vs Informed Search
XD
裕

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 3}
Content: Today
•Informed Search
• Heuristics
• Greedy Search
• A* Search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 4}
Content: Search Heuristics
A heuristic is:
 A function that estimates how close a state is to a goal
 Designed for a particular search problem
 Examples: Manhattan distance, Euclidean distance for 
pathing
10
5
11.2
52NOPE.
GOAL!
Heuristi-TronNoPE.
GOAL!
Heuristi-Tron

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 5}
Content: Example: Euclidean distance to Bucharest
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86
h(state)  value
Arad
366
Mehadia
241
Bucharest
0
Neamt
234
Craiova
160
Oradea
380
Drobeta
242
Pitesti
100
Eforie
161
Rimnicu Vilcea
193
Fagaras
176
Sibiu
253
Giurgiu
77
Timisoara
329
Hirsova
151
Urziceni
80
Iasi
226
Vaslui
199
Lugoj
244
Zerind
374

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 6}
Content: Effect of heuristics
Guide search towards the goal instead of all over the place
Start Goal Start Goal
Informed Uninformed

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 7}
Content: Greedy Search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 8}
Content: Sibiu Fagaras
Pitesti
Rimnicu Vilcea
Bucharest
99
80
97
101
211
• Expand the node that seems closest…(order frontier by h)
• What can possibly go wrong?
h=193
h= 253
h=100
h=0
h=176
Sibiu-Fagaras-Bucharest =
99+211 = 310
Sibiu-Rimnicu Vilcea-Pitesti-Bucharest =
80+97+101 = 278
1000000
Greedy Search
？!

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 9}
Content: Greedy Search
• Strategy: expand a node that seems closest to 
a goal state, according to h
• Problem 1: it chooses a node even if it’s at the 
end of a very long and winding road
• Problem 2: it takes h literally even if it’s 
completely wrong
…
b

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 10}
Content: A* Search
D
裕

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 11}
Content: A* Search
UCS Greedy
A*

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 12}
Content: Combining UCS and Greedy
• Uniform-cost orders by path cost, or backward cost g(n)
• Greedy orders by goal proximity, or forward cost h(n)
• A* Search orders by the sum: f(n) = g(n) + h(n)
S a d
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0
c
h=7
3
e h=1
1
Example: Teg Grenager
S
a
b
c
d e
G d
G
g = 0 
h=6
g = 1 
h=5
g = 2 
h=6
g = 3 
h=7
g = 4 
h=2
g = 6 
h=0
g = 9 
h=1
g = 10 
h=2
g = 12 
h=0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 13}
Content: S a d
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0
c
h=7
3
e h=1
1
Current Open list (Greedy) Close
S(6) A(5) -
A(5) E(1), D(2), B(6) S(6)
E(1) D(2), B(6) S(6), A(5)
D(2) G(0), B(6) S(6), A(5), E(1)
G(0) Goal S(6), A(5), E(1), D(2)
Path = S-A-E-D-G
Cost = 1+8+1+2 = 12
A* Search orders by the sum: f(n) = g(n) + h(n)
Current Open list (UCS) Close
S(0) A(1)
A(1) B(2), D(4), E(9) S(0)
B(2) C(3) , D(4), E(9) S(0), A(1)
C(3) D(4), E(9) S(0), A(1), B(2)
D(4) G(6), E(9) S(0), A(1), B(2), 
C(3)
G(6) = goal 
node
E(9) S(0), A(1), B(2), 
C(3)D(4)
Current Open list (A*) Close
S(0+6=6) A(1+5=6)
A(6) D(6), B(8), E(10) S(6)
D(6) G(6), B(8), E(10) S(6), A(6)
G(6) = goal node B(8), E(10) S(6), A(6), D(6)
Path = S-A-D-G
Cost = 6

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 14}
Content: A*
Sad
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0 c
h=7
3
e h=1
1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 15}
Content: function UNIFORM-COST-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using g(n) as the priority
add initial state of problem to frontier with priority g(S) = 0
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher g(n) then
replace that frontier node with child

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 16}
Content: function A-STAR-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using f(n) = g(n) + h(n) as the priority
add initial state of problem to frontier with priority f(S) = 0 + h(S)
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher f(n) then
replace that frontier node with child

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 17}
Content: A* Search Algorithms 
• A* Tree Search 
• Same tree search algorithm but with a frontier that is a priority queue using 
priority f(n) = g(n) + h(n) 
• A* Graph Search 
• Same as UCS graph search algorithm but with a frontier that is a priority 
queue using priority f(n) = g(n) + h(n)

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 18}
Content: UCS vs A* Contours 
UCS vs A* Contours
A* expands mainly toward the 
goal, but does hedge its bets 
to ensure optimality 
Start Goal
Start Goal

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 19}
Content: Greedy Uniform Cost A*
Comparison
5
SCORE:55
C
SCORE:aiogW%
SCORE:

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 20}
Content: Is A* Optimal?
• What went wrong?
• Actual bad goal cost < estimated good goal cost
• We need estimates to be less than actual costs!
A
G
S
1 3
h = 6
h = 0
5
h = 7

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 21}
Content: Admissible Heuristics
GETTING
YAY!
CLASER...
->
Heuristi-Tron

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 22}
Content: Admissible Heuristics
• A heuristic h is admissible (optimistic) if:
0  h(n)  h*(n) 
where h*(n) is the true cost to a nearest goal
• Example:
• Coming up with admissible heuristics is most of what’s 
involved in using A* in practice.
15
52

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 23}
Content: Optimality of A* Tree Search
巧

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 24}
Content: A* Tree Search
S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
S (0+2)
A (1+4)
C (2+1)
G (5+0)
C (3+1)
G (6+0)
State space graph Search tree

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 25}
Content: Optimality of A* Tree Search
Assume:
• A is an optimal goal node
• B is a suboptimal goal node
• h is admissible
Claim:
• A will be chosen for exploration (popped off the frontier) before B
…
A
B

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 26}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, 
too (Maybe the start state; maybe A
itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
f(n) = g(n) + h(n) Definition of f-cost
f(n)  g(A) Admissibility of h
…
g(A) = f(A) h = 0 at a goal
A
B
n

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 27}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, 
too (Maybe the start state; maybe A
itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
2. f(A) is less than f(B)
…
A
B
n
g(A) < g(B) Suboptimality of B
f(A) < f(B) h = 0 at a goal

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 28}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, too 
(Maybe the start state; maybe A itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
2. f(A) is less than f(B)
3. n is explored before B
• All ancestors of A are explored before B
• A is explored before B
• A* search is optimal
…
A
B
n
f(n)  f(A) < f(B)

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 29}
Content: Optimality of A* Graph Search
巧

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 30}
Content: S
A
C
G
1
3
1
3
What paths does A* graph search consider during its search?
A) S, S-A, S-C, S-C-G
C) S, S-A, S-A-C, S-A-C-G
D) S, S-A, S-C, S-A-C, S-A-C-G
B) S, S-A, S-C, S-A-C, S-C-G
0
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 31}
Content: S
A
C
G
1
3
1
3
What paths does A* graph search consider during its search?
A) S, S-A, S-C, S-C-G
C) S, S-A, S-A-C, S-A-C-G
D) S, S-A, S-C, S-A-C, S-A-C-G
B) S, S-A, S-C, S-A-C, S-C-G
0
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 32}
Content: A* Graph Search
S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
What does the resulting graph tree look like?
S
A
C
G
S
A
C
G
A)
B)
C & D)
S
A
C
G

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 33}
Content: A* Graph Search Gone Wrong?
S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
S (0+2)
A (1+4) C (3+1)
G (6+0)
State space graph Search tree
Simple check against explored set blocks C
Fancy check allows new C if cheaper than old
but requires recalculating C’s descendants

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 34}
Content: Admissibility of Heuristics
• Main idea: Estimated heuristic values ≤ actual costs
• Admissibility:
heuristic value ≤ actual cost to goal
h(A) ≤ actual cost from A to G
3
A
C
G
h=4
1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 35}
Content: Consistency of Heuristics
• Main idea: Estimated heuristic costs ≤ actual costs
• Admissibility:
heuristic cost ≤ actual cost to goal
h(A) ≤ actual cost from A to G
• Consistency:
“heuristic step cost” ≤ actual cost for each step
h(A) – h(C) ≤ cost(A to C)
triangle inequality
h(A) ≤ cost(A to C) + h(C)
• Consequences of consistency:
• The f value along a path never decreases
• A* graph search is optimal
A
C
G
h=4 h=1
1
h=2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 36}
Content: Optimality of A* Graph Search
• Sketch: consider what A* does with a 
consistent heuristic:
• Fact 1: In tree search, A* expands nodes 
in increasing total f value (f-contours)
• Fact 2: For every state s, nodes that 
reach s optimally are explored before 
nodes that reach s suboptimally
• Result: A* graph search is optimal
…
f  3
f  2
f  1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 37}
Content: Optimality
• Tree search:
• A* is optimal if heuristic is admissible
• UCS is a special case (h = 0)
• Graph search:
• A* optimal if heuristic is consistent
• UCS optimal (h = 0 is consistent)
• Consistency implies admissibility
• In general, most natural admissible heuristics tend to be 
consistent, especially if from relaxed problems
巧

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 38}
Content: Creating Heuristics
YOUGOT
HEURISTIC
UPGRRDE!

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 39}
Content: Creating Admissible Heuristics
• Most of the work in solving hard search problems optimally is in 
coming up with admissible heuristics
• Often, admissible heuristics are solutions to relaxed problems, where 
new actions are available
15
366
52Straight-line distance
Oradea
to Bucharest
71
Neamt
Arad
366
口
87
Bucharest
0
Zerind
151
75
Craiova
160
lasi
Dobreta
242
Arad
140
Eforie
161
92
Fagaras
Sibiu
178
99
Fagaras
Giurgiu
118
77
口
Vaslui
Hirsova
80
151
Iasi
226
RimnicuVilcea
Timisoara
Lugoj
244
142
Mehadia
241
11↑
211
Pitesti
Neamt
97
234
Lugoj
Oradea
380
70
98
Pitesti
98
146
101
85
口
Hirsova
Mehadia
Urziceni
Rimnicu Vilcea
■
193
75
86
Sibiu
138
253
Bucharest
Timisoara
120
329
Dobreta
90
口
Urziceni
80
■
Craiova
Eforie
Vaslui
199
Giurgiu
Zerind
374

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 40}
Content: Example: 8 Puzzle
• What are the states?
• How many states?
• What are the actions?
• How many actions from the start state?
• What should the step costs be?
Start State Actions Goal State
3
7
1
2
4
5
8
672
4
5
6
83
11
2
34
5
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 41}
Content: 8 Puzzle I
• Heuristic: Number of tiles misplaced
• Why is it admissible?
• h(start) =
• This is a relaxed-problem heuristic
8
Average nodes expanded when 
the optimal path has…
…4 steps …8 steps …12 steps
UCS 112 6,300 3.6 x 106
A*TILES 13 39 227
Start State Goal State
Statistics from Andrew Moore
1
5
+
87
2
4
1
2
34
5
6
5
83
1
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 42}
Content: 8 Puzzle II
• What if we had an easier 8-puzzle 
where any tile could slide any 
direction at any time, ignoring 
other tiles?
• Total Manhattan distance
• Why is it admissible?
• h(start) = 3 + 1 + 2 + … = 18
Average nodes expanded when 
the optimal path has…
…4 steps …8 steps …12 steps
A*TILES 13 39 227
A*MANHATTAN 12 25 73
Start State Goal State
7
2
4
1
2
34
5
6
5
83
1
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 43}
Content: Combining heuristics
• Dominance: ha ≥ hcif 
n ha(n)  hc(n)
• Roughly speaking, larger is better as long as both are admissible
• The zero heuristic is pretty bad (what does A* do with h=0?)
• The exact heuristic is pretty good, but usually too expensive!
• What if we have two heuristics, neither dominates the other?
• Form a new heuristic by taking the max of both:
h(n) = max( ha(n), hb(n) )
• Max of admissible heuristics is admissible and dominates both!

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 44}
Content: A*: Summary

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 45}
Content: A*: Summary
• A* uses both backward costs and (estimates of) forward costs
• A* is optimal with admissible / consistent heuristics
• Heuristic design is key: often use relaxed problems
D

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 46}
Content: In-Class Activity 
• Q1: Practice creating heuristics and running Greedy and A* search 
• Q2: Walk through Amazon Robot Example

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 47}
Content: • Notes added on LMS
• https://www.oreilly.com/library/view/graph-algorithms/9781492047674/ch04.html
Recommended
Reading

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 48}
Content: ousoNs?
?

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 1
These lecture notes are heavily based on notes originally written by Nikhil Sharma.
Agents
In artificial intelligence, the central problem at hand is that of the creation of a rational agent, an entity that
has goals or preferences and tries to perform a series of actions that yield the best/optimal expected outcome
given these goals. Rational agents exist in an environment, which is specific to the given instantiation of
the agent. As a very simple example, the environment for a checkers agent is the virtual checkers board on
which it plays against opponents, where piece moves are actions. Together, an environment and the agents
that reside within it create a world.
A reflex agent is one that doesn’t think about the consequences of its actions, but rather selects an action
based solely on the current state of the world. These agents are typically outperformed by planning agents,

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: which maintain a model of the world and use this model to simulate performing various actions. Then, the
agent can determine hypothesized consequences of the actions and can select the best one. This is simulated
"intelligence" in the sense that it’s exactly what humans do when trying to determine the best possible move
in any situation - thinking ahead.
State Spaces and Search Problems
In order to create a rational planning agent, we need a way to mathematically express the given environment
in which the agent will exist. To do this, we must formally express a search problem - given our agent’s
current state (its configuration within its environment), how can we arrive at a new state that satisfies its
goals in the best possible way? Formulating such a problem requires four things:
• A state space - The set of all possible states that are possible in your given world

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: • A successor function - A function that takes in a state and an action and computes the cost of performing that action as well as the successor state, the state the world would be in if the given agent
performed that action
• A start state - The state in which an agent exists initially
• A goal test - A function that takes a state as input, and determines whether it is a goal state
Fundamentally, a search problem is solved by first considering the start state, then exploring the state space
using the successor function, iteratively computing successors of various states until we arrive at a goal state,
at which point we will have determined a path from the start state to the goal state (typically called a plan).
The order in which states are considered is determined using a predetermined strategy. We’ll cover types
of strategies and their usefulness shortly.
Before we continue with how to solve search problems, it’s important to note the difference between a world

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: state, and a search state. A world state contains all information about a given state, whereas a search state
CS 188, Fall 2018, Note 1 1
CS-370 Artificial Intelligence
Fall 2023
CS-370, Fall 2023, Note 1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: contains only the information about the world that’s necessary for planning (primarily for space effiency
reasons). To illustrate these concepts, we’ll introduce the hallmark motivating example of this course -
Pacman. The game of Pacman is simple: Pacman must navigate a maze and eat all the (small) food pellets
in the maze without being eaten by the malicious patrolling ghosts. If Pacman eats one of the (large) power
pellets, he becomes ghost-immune for a set period of time and gains the ability to eat ghosts for points.
Let’s consider a variation of the game in which the maze contains only Pacman and food pellets. We can
pose two distinct search problems in this scenario: pathing and eat-all-dots. Pathing attempts to solve
the problem of getting from position (x1, y1) to position (x2, y2) in the maze optimally, while eat all dots
attempts to solve the problem of consuming all food pellets in the maze in the shortest time possible. Below,

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: the states, actions, successor function, and goal test for both problems are listed:
• Pathing
– States: (x,y) locations
– Actions: North, South, East, West
– Successor: Update location only
– Goal test: Is (x,y)=END?
• Eat-all-dots
– States: (x,y) location, dot booleans
– Actions: North, South, East, West
– Successor: Update location and booleans
– Goal test: Are all dot booleans false?
Note that for pathing, states contain less information than states for eat-all-dots, because for eat-all-dots we
must maintain an array of booleans corresponding to each food pellet and whether or not it’s been eaten in
the given state. A world state may contain more information still, potentially encoding information about
things like total distance traveled by Pacman or all positions visited by Pacman on top of its current (x,y)
location and dot booleans.
State Space Size
An important question that often comes up while estimating the computational runtime of solving a search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: problem is the size of the state space. This is done almost exclusively with the fundamental counting
principle, which states that if there are n variable objects in a given world which can take on x1, x2, ..., xn
different values respectively, then the total number of states is x1 · x2 · ... · xn. Let’s use Pacman to show this
concept by example:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 2
84.3.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: Let’s say that the variable objects and their corresponding number of possiblilites are as follows:
• Pacman positions - Pacman can be in 120 distinct (x, y) positions, and there is only one Pacman
• Pacman Direction - this can be North, South, East, or West, for a total of 4 possibilities
• Ghost positions - There are two ghosts, each of which can be in 12 distinct (x, y) positions
• Food pellet configurations - There are 30 food pellets, each of which can be eaten or not eaten
Using the fundamental counting principle, we have 120 positions for Pacman, 4 directions Pacman can be
facing, 12·12 ghost configurations (12 for each ghost), and 2·2·...·2 = 230 food pellet configurations (each
of 30 food pellets has two possible values - eaten or not eaten). This gives us a total state space size of
120 · 4 · 122 · 230 .
State Space Graphs and Search Trees
Now that we’ve established the idea of a state space and the four components necessary to completely define

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: one, we’re almost ready to begin solving search problems. The final piece of the puzzle is that of state space
graphs and search trees.
Recall that a graph is defined by a set of nodes and a set of edges connecting various pairs of nodes. These
edges may also have weights associated with them. A state space graph is constructed with states representing nodes, with directed edges existing from a state to its successors. These edges represent actions,
and any associated weights represent the cost of performing the corresponding action. Typically, state space
graphs are much too large to store in memory (even our simple Pacman example from above has ⇡ 1013
possible states, yikes!), but they’re good to keep in mind conceptually while solving problems. It’s also
important to note that in a state space graph, each state is represented exactly once - there’s simply no need
to represent a state multiple times, and knowing this helps quite a bit when trying to reason about search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: problems.
Unlike state space graphs, our next structure of interest, search trees, have no such restriction on the number
of times a state can appear. This is because though search trees are also a class of graph with states as nodes
and actions as edges between states, each state/node encodes not just the state itself, but the entire path
(or plan) from the start state to the given state in the state space graph. Observe the state space graph and
corresponding search tree below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 3

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: The highlighted path (S ! d ! e ! r ! f ! G) in the given state space graph is represented in the
corresponding search tree by following the path in the tree from the start state S to the highlighted goal state
G. Similarly, each and every path from the start node to any other node is represented in the search tree by a
path from the root S to some descendant of the root corresponding to the other node. Since there often exist
multiple ways to get from one state to another, states tend to show up multiple times in search trees. As a
result, search trees are greater than or equal to their corresponding state space graph in size.
We’ve already determined that state space graphs themselves can be enormous in size even for simple problems, and so the question arises - how can we perform useful computation on these structures if they’re too
big to represent in memory? The answer lies in successor functions - we only store states we’re immediately

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: working with, and compute new ones on-demand using the corresponding successor function. Typically,
search problems are solved using search trees, where we very carefully store a select few nodes to observe
at a time, iteratively replacing nodes with their successors until we arrive at a goal state. There exist various
methods by which to decide the order in which to conduct this iterative replacement of search tree nodes,
and we’ll present these methods now.
Uninformed Search
The standard protocol for finding a plan to get from the start state to a goal state is to maintain an outer
fringe of partial plans derived from the search tree. We continually expand our fringe by removing a node
(which is selected using our given strategy) corresponding to a partial plan from the fringe, and replacing
it on the fringe with all its children. Removing and replacing an element on the fringe with its children

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: corresponds to discarding a single length n plan and bringing all length (n+1) plans that stem from it into
consideration. We continue this until eventually removing a goal state off the fringe, at which point we
conclude the partial plan corresponding to the removed goal state is in fact a path to get from the start state
to the goal state. Practically, most implementations of such algorithms will encode information about the
parent node, distance to node, and the state inside the node object. This procedure we have just outlined is
known as tree search, and the pseudocode for it is presented below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 4
Each NODE in in
State Space Graph
the search tree is
Search Tree
an entire PATH in
the state space
S
graph.
n
6
C
a
a
S
We construct both
on demand - and
q
C
G
we construct as
q
C
a
G
little as possible.
a

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: When we have no knowledge of the location of goal states in our search tree, we are forced to select our
strategy for tree search from one of the techniques that falls under the umbrella of uninformed search.
We’ll now cover three such strategies in succession: depth-first search, breadth-first search, and uniform
cost search. Along with each strategy, some rudimentary properties of the strategy are presented as well, in
terms of the following:
• The completeness of each search strategy - if there exists a solution to the search problem, is the
strategy guaranteed to find it given infinite computational resources?
• The optimality of each search strategy - is the strategy guaranteed to find the lowest cost path to a
goal state?
• The branching factor b - The increase in the number of nodes on the fringe each time a fringe node
is dequeued and replaced with its children is O(b). At depth k in the search tree, there exists O(bk)
nodes.
• The maximum depth m.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: • The depth of the shallowest solution s.
Depth-First Search
• Description - Depth-first search (DFS) is a strategy for exploration that always selects the deepest
fringe node from the start node for expansion.
• Fringe representation - Removing the deepest node and replacing it on the fringe with its children
necessarily means the children are now the new deepest nodes - their depth is one greater than the
depth of the previous deepest node. This implies that to implement DFS, we require a structure that
always gives the most recently added objects highest priority. A last-in, first-out (LIFO) stack does
exactly this, and is what is traditionally used to represent the fringe when implementing DFS.
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 5
function TREE-SEARCH(problem, fringe)return a solution,or failure
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loop
do
iffringeisemptythenreturnfailure
node←REMOVE-FRONT(fringe)

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: if GOAL-TEsT(problem,sTATE[node])then return node
for child-nodeinEXPAND(sTATE[node],problem)do
fringe←INsERT(child-node,fringe)
end
end1 node
b nodes
b2 nodes
m tiers
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: • Completeness - Depth-first search is not complete. If there exist cycles in the state space graph, this
inevitably means that the corresponding search tree will be infinite in depth. Hence, there exists the
possibility that DFS will faithfully yet tragically get "stuck" searching for the deepest node in an
infinite-sized search tree, doomed to never find a solution.
• Optimality - Depth-first search simply finds the "leftmost" solution in the search tree without regard
for path costs, and so is not optimal.
• Time Complexity - In the worst case, depth first search may end up exploring the entire search tree.
Hence, given a tree with maximum depth m, the runtime of DFS is O(bm).
• Space Complexity - In the worst case, DFS maintains b nodes at each of m depth levels on the fringe.
This is a simple consequence of the fact that once b children of some parent are enqueued, the nature

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: of DFS allows only one of the subtrees of any of these children to be explored at any given point in
time. Hence, the space complexity of BFS is O(bm).
Breadth-First Search
• Description - Breadth-first search is a strategy for exploration that always selects the shallowest fringe
node from the start node for expansion.
• Fringe representation - If we want to visit shallower nodes before deeper nodes, we must visit nodes
in their order of insertion. Hence, we desire a structure that outputs the oldest enqueued object to
represent our fringe. For this, BFS uses a first-in, first-out (FIFO) queue, which does exactly this.
• Completeness - If a solution exists, then the depth of the shallowest node s must be finite, so BFS
must eventually search this depth. Hence, it’s complete.
• Optimality - BFS is generally not optimal because it simply does not take costs into consideration
when determining which node to replace on the fringe. The special case where BFS is guaranteed to

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: be optimal is if all edge costs are equivalent, because this reduces BFS to a special case of uniform
cost search, which is discussed below.
• Time Complexity - We must search 1+b+b2 +...+bs nodes in the worst case, since we go through
all nodes at every depth from 1 to s. Hence, the time complexity is O(bs).
• Space Complexity - The fringe, in the worst case, contains all the nodes in the level corresponding to
the shallowest solution. Since the shallowest solution is located at depth s, there are O(bs) nodes at
this depth.
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 6
1 node
b nodes
s tiers
b2 nodes
bs nodes
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: Uniform Cost Search
• Description - Uniform cost search (UCS), our last strategy, is a strategy for exploration that always
selects the lowest cost fringe node from the start node for expansion.
• Fringe representation - To represent the fringe for UCS, the choice is usually a heap-based priority
queue, where the weight for a given enqueued node v is the path cost from the start node to v, or the
backward cost of v. Intuitively, a priority queue constructed in this manner simply reshuffles itself to
maintain the desired ordering by path cost as we remove the current minimum cost path and replace
it with its children.
• Completeness - Uniform cost search is complete. If a goal state exists, it must have some finite length
shortest path; hence, UCS must eventually find this shortest length path.
• Optimality - UCS is also optimal if we assume all edge costs are nonnegative. By construction, since

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: we explore nodes in order of increasing path cost, we’re guaranteed to find the lowest-cost path to a
goal state. The strategy employed in Uniform Cost Search is identical to that of Dijkstra’s algorithm,
and the chief difference is that UCS terminates upon finding a solution state instead of finding the
shortest path to all states. Note that having negative edge costs in our graph can make nodes on a
path have decreasing length, ruining our guarantee of optimality. (See Bellman-Ford algorithm for a
slower algorithm that handles this possibility)
• Time Complexity - Let us define the optimal path cost as C⇤ and the minimal cost between two nodes
in the state space graph as e. Then, we must roughly explore all nodes at depths ranging from 1 to
C⇤/e, leading to an runtime of O(bC⇤/e ).
• Space Complexity - Roughly, the fringe will contain all nodes at the level of the cheapest solution, so
the space complexity of UCS is estimated as O(bC⇤/e ).

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: As a parting note about uninformed search, it’s critical to note that the three strategies outlined above are
fundamentally the same - differing only in expansion strategy, with their similarities being captured by the
tree search pseudocode presented above.
Informed Search
Uniform cost search is good because it’s both complete and optimal, but it can be fairly slow because it
expands in every direction from the start state while searching for a goal. If we have some notion of the
direction in which we should focus our search, we can significantly improve performance and "hone in" on
a goal much more quickly. This is exactly the focus of informed search.
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 7
b
c≤1
c≤2
C*/e "tiers'
C≤3

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: Heuristics
Heuristics are the driving force that allow estimation of distance to goal states - they’re functions that take
in a state as input and output a corresponding estimate. The computation performed by such a function is
specific to the search problem being solved. For reasons that we’ll see in A* search, below, we usually want
heuristic functions to be a lower bound on this remaining distance to the goal, and so heuristics are typically
solutions to relaxed problems (where some of the constraints of the original problem have been removed).
Turning to our Pacman example, let’s consider the pathing problem described earlier. A common heuristic
that’s used to solve this problem is the Manhattan distance, which for two points (x1, y1) and (x2, y2) is
defined as follows:
Manhattan(x1, y1, x2, y2) = |x1 x2|+|y1 y2|
The above visualization shows the relaxed problem that the Manhattan distance helps solve - assuming

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: Pacman desires to get to the bottom left corner of the maze, it computes the distance from Pacman’s current
location to Pacman’s desired location assuming a lack of walls in the maze. This distance is the exact goal
distance in the relaxed search problem, and correspondingly is the estimated goal distance in the actual
search problem. With heuristics, it becomes very easy to implement logic in our agent that enables them
to "prefer" expanding states that are estimated to be closer to goal states when deciding which action to
perform. This concept of preference is very powerful, and is utilized by the following two search algorithms
that implement heuristic functions: greedy search and A*.
Greedy Search
• Description - Greedy search is a strategy for exploration that always selects the fringe node with the
lowest heuristic value for expansion, which corresponds to the state it believes is nearest to a goal.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: • Fringe representation - Greedy search operates identically to UCS, with a priority queue fringe representation. The difference is that instead of using computed backward cost (the sum of edge weights
in the path to the state) to assign priority, greedy search uses estimated forward cost in the form of
heuristic values.
• Completeness and Optimality - Greedy search is not guaranteed to find a goal state if one exists, nor is
it optimal, particularly in cases where a very bad heuristic function is selected. It generally acts fairly
unpredictably from scenario to scenario, and can range from going straight to a goal state to acting
like a badly-guided DFS and exploring all the wrong areas.
A* Search
• Description - A* search is a strategy for exploration that always selects the fringe node with the lowest
estimated total cost for expansion, where total cost is the entire cost from the start node to the goal
node.
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 8
15

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: (a) Greedy search on a good day :) (b) Greedy search on a bad day :(
• Fringe representation - Just like greedy search and UCS, A* search also uses a priority queue to
represent its fringe. Again, the only difference is the method of priority selection. A* combines the
total backward cost (sum of edge weights in the path to the state) used by UCS with the estimated
forward cost (heuristic value) used by greedy search by adding these two values, effectively yielding
an estimated total cost from start to goal. Given that we want to minimize the total cost from start to
goal, this is an excellent choice.
• Completeness and Optimality - A* search is both complete and optimal, given an appropriate heuristic
(which we’ll cover in a minute). It’s a combination of the good from all the other search strategies
we’ve covered so far, incorporating the generally high speed of greedy search with the optimality and
completeness of UCS!
Admissibility and Consistency

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: Now that we’ve discussed heuristics and how they are applied in both greedy and A* search, let’s spend
some time discussing what constitutes a good heuristic. To do so, let’s first reformulate the methods used
for determining priority queue ordering in UCS, greedy search, and A* with the following definitions:
• g(n) - The function representing total backwards cost computed by UCS.
• h(n) - The heuristic value function, or estimated forward cost, used by greedy search.
• f(n) - The function representing estimated total cost, used by A* search. f(n) = g(n) +h(n).
Before attacking the question of what constitutes a "good" heuristic, we must first answer the question of
whether A* maintains its properties of completeness and optimality regardless of the heuristic function we
use. Indeed, it’s very easy to find heuristics that break these two coveted properties. As an example, consider

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: the heuristic function h(n) = 1g(n). Regardless of the search problem, using this heuristic yields
f(n) = g(n) +h(n)
= g(n)+(1g(n))
= 1
Hence, such a heuristic reduces A* search to BFS, where all edge costs are equivalent. As we’ve already
shown, BFS is not guaranteed to be optimal in the general case where edge weights are not constant.
The condition required for optimality when using A* tree search is known as admissibility. The admissibility constraint states that the value estimated by an admissible heuristic is neither negative nor an overestimate. Defining h⇤(n) as the true optimal forward cost to reach a goal state from a given node n, we can
formulate the admissibility constraint mathematically as follows:
8n, 0  h(n)  h⇤(n)
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 9
bb

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: Theorem. For a given search problem, if the admissibility constraint is satisfied by a heuristic function h,
using A* tree search with h on that search problem will yield an optimal solution.
Proof. Assume two reachable goal states are located in the search tree for a given search problem, an optimal
goal A and a suboptimal goal B. Some ancestor n of A (including perhaps A itself) must currently be on the
fringe, since A is reachable from the start state. We claim n will be selected for expansion before B, using
the following three statements:
1. g(A) < g(B). Because A is given to be optimal and B is given to be suboptimal, we can conclude that
A has a lower backwards cost to the start state than B.
2. h(A) = h(B) = 0, because we are given that our heuristic satisfies the admissibility constraint. Since
both A and B are both goal states, the true optimal cost to a goal state from A or B is simply h⇤(n) = 0;
hence 0  h(n)  0.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: 3. f(n)  f(A), because, through admissibility of h, f(n) = g(n) +h(n)  g(n) +h⇤(n) = g(A) = f(A).
The total cost through node n is at most the true backward cost of A, which is also the total cost of A.
We can combine statements 1. and 2. to conclude that f(A) < f(B) as follows:
f(A) = g(A) +h(A) = g(A) < g(B) = g(B) +h(B) = f(B)
A simple consequence of combining the above derived inequality with statement 3. is the following:
f(n)  f(A)^ f(A) < f(B) =) f(n) < f(B)
Hence, we can conclude that n is expanded before B. Because we have proven this for arbitrary n, we can
conclude that all ancestors of A (including A itself) expand before B. 2
One problem we found above with tree search was that in some cases it could fail to ever find a solution,
getting stuck searching the same cycle in the state space graph infinitely. Even in situations where our
search technique doesn’t involve such an infinite loop, it’s often the case that we revisit the same node

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: multiple times because there’s multiple ways to get to that same node. This leads to exponentially more
work, and the natural solution is to simply keep track of which states you’ve already expanded, and never
expand them again. More explicitly, maintain a "closed" set of expanded nodes while utilizing your search
method of choice. Then, ensure that each node isn’t already in the set before expansion and add it to the
set after expansion if it’s not. Tree search with this added optimization is known as graph search, and the
pseudocode for it is presented below:
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 10
function GRAPH-SEARCH(problem, fringe)return a solution,or failure
closed←an empty set
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loopdo
iffringeis empty thenreturnfailure
mode
e←REMOVE-FRONT(fringe)
if GOAL-TEsT(problem,STATE[node])thenreturn node
if sTATE[node]is not in closed then
addsTATE[node]toclosed
for child-node in EXPAND(sTATE[node],problem)do

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: fringe←INsERT(child-node,fringe)
end
end

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: Note that in implementation, it’s critically important to store the closed set as a disjoint set and not a list.
Storing it as a list requires costs O(n) operations to check for membership, which eliminates the performance
improvement graph search is intended to provide. An additional caveat of graph search is that it tends to
ruin the optimality of A*, even under admissible heuristics. Consider the following simple state space graph
and corresponding search tree, annotated with weights and heuristic values:
In the above example, it’s clear that the optimal route is to follow S ! A !C ! G, yielding a total path cost
of 1+1+3 = 5. The only other path to the goal, S ! B !C ! G has a path cost of 1+2+3 = 6. However,
because the heuristic value of node A is so much larger than the heuristic value of node B, node C is first
expanded along the second, suboptimal path as a child of node B. It’s then placed into the "closed" set, and

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: so A* graph search fails to reexpand it when it visits it as a child of A, so it never finds the optimal solution.
Hence, to maintain completeness and optimality under A* graph search, we need an even stronger property
than admissibility, consistency. The central idea of consistency is that we enforce not only that a heuristic
underestimates the total distance to a goal from any given node, but also the cost/weight of each edge in the
graph. The cost of an edge as measured by the heuristic function is simply the difference in heuristic values
for two connected nodes. Mathematically, the consistency constraint can be expressed as follows:
8A,C h(A)h(C)  cost(A,C)
Theorem. For a given search problem, if the consistency constraint is satisfied by a heuristic function h,
using A* graph search with h on that search problem will yield an optimal solution.
Proof. In order to prove the above theorem, we first prove that when running A* graph search with a

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: consistent heuristic, whenever we remove a node for expansion, we’ve found the optimal path to that node.
Using the consistency constraint, we can show that the values of f(n) for nodes along any plan are nondecreasing. Define two nodes, n and n0
, where n0 is a successor of n. Then:
f(n0) = g(n0) +h(n0)
= g(n) +cost(n,n0) +h(n0)
 g(n) +h(n)
= f(n)
If for every parent-child pair (n,n0) along a path, f(n0)  f(n), then it must be the case that the values of
f(n) are nondecreasing along that path. We can check that the above graph violates this rule between f(A)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 11
A
S (0+2)
1
1
S
h=4
c
h=1
A (1+4)
B (1+1)
h=2
1
2
3
C (2+1)
C (3+1)
B
h=1
G (5+0)
G (6+0)
G
h=0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: and f(C). With this information, we can now show that whenever a node n is removed for expansion, its
optimal path has been found. Assume towards a contradiction that this is false - that when n is removed
from the fringe, the path found to n is suboptimal. This means that there must be some ancestor of n, n00, on
the fringe that was never expanded but is on the optimal path to n. Contradiction! We’ve already shown that
values of f along a path are nondecreasing, and so n00 would have been removed for expansion before n.
All we have left to show to complete our proof is that an optimal goal A will always be removed for expansion
and returned before any suboptimal goal B. This is trivial, since h(A) = h(B) = 0, so
f(A) = g(A) < g(B) = f(B)
just as in our proof of optimality of A* tree search under the admissibility constraint. Hence, we can
conclude that A* graph search is optimal under a consistent heuristic. 2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: A couple of important highlights from the discussion above before we proceed: for heuristics that are either
admissible/consistent to be valid, it must by definition be the case that h(G) = 0 for any goal state G. Additionally, consistency is not just a stronger constraint than admissibility, consistency implies admissibility.
This stems simply from the fact that if no edge costs are overestimates (as guaranteed by consistency), the
total estimated cost from any node to a goal will also fail to be an overestimate.
Consider the following three-node network for an example of an admissible but inconsistent heuristic:
The red dotted line corresponds to the total estimated goal distance. If h(A) = 4, then the heuristic is
admissible, as the distance from A to the goal is 4  h(A), and same for h(C) = 1  3. However, the
heuristic cost from A to C is h(A)h(C) = 41 = 3. Our heuristic estimates the cost of the edge between

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: A and C to be 3 while the true value is cost(A,C) = 1, a smaller value. Since h(A)  h(C) ⇥ cost(A,C),
this heuristic is not consistent. Running the same computation for h(A) = 2, however, yields h(A)h(C) =
21 = 1  cost(A,C). Thus, using h(A) = 2 makes our heuristic consistent.
Dominance
Now that we’ve established the properties of admissibility and consistency and their roles in maintaining
the optimality of A* search, we can return to our original problem of creating "good" heuristics, and how to
tell if one heuristic is better than another. The standard metric for this is that of dominance. If heuristic a is
dominant over heuristic b, then the estimated goal distance for a is greater than the estimated goal distance
for b for every node in the state space graph. Mathematically,
8n : ha(n)  hb(n)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 12
A
1
h-4
C
h=1
h=2
3

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: Dominance very intuitively captures the idea of one heuristic being better than another - if one admissible/consistent heuristic is dominant over another, it must be better because it will always more closely estimate the distance to a goal from any given state. Additionally, the trivial heuristic is defined as h(n) = 0,
and using it reduces A* search to UCS. All admissible heuristics dominate the trivial heuristic. The trivial
heuristic is often incorporated at the base of a semi-lattice for a search problem, a dominance hierarchy of
which it is located at the bottom. Below is an example of a semi-lattice that incorporates various heuristics
ha,hb, and hc ranging from the trivial heuristic at the bottom to the exact goal distance at the top:
As a general rule, the max function applied to multiple admissible heuristics will also always be admissible.
This is simply a consequence of all values output by the heuristics for any given state being constrained by

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: the admissibility condition, 0  h(n)  h⇤(n). The maximum of numbers in this range must also fall in the
same range. The same can be shown easily for multiple consistent heuristics as well. It’s common practice
to generate multiple admissible/consistent heuristics for any given search problem and compute the max
over the values output by them to generate a heuristic that dominates (and hence is better than) all of them
individually.
Summary
In this note, we discussed search problems, which we characterize formally with four components: a state
space, a successor function, a start state, and a goal state. Search problems can be solved using a variety of
search techniques, including but not limited to the five we study in CS 188:
• Breadth-first Search
• Depth-first Search
• Uniform Cost Search
• Greedy Search
• A* Search
The first three search techniques listed above are examples of uninformed search, while the latter two are

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: examples of informed search which use heuristics to estimate goal distance and optimize performance.
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 13
ecact
max(ha,hb)
ha
hb
hc
zero

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 2
17th Sept 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 1}
Content: Today
• Agents and Environment (Recap)
• Search Problems
• Uninformed Search Methods
• Depth-First Search
• Breadth-First Search
• Uniform-Cost Search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 2}
Content: Search Problems
王

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 3}
Content: Search Problems
• A search problem consists of:
• A state space
• For each state, a set Actions(s) of allowable actions
• A transition model Result(s,a)
• A step cost function c(s,a,s’)
• A start state and a goal test
• A solution is a sequence of actions (a plan) which transforms 
the start state to a goal state
N
E
{N, E}
1
1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 4}
Content: Search Problems Are Models

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 5}
Content: Example: Travelling in Romania
• State space:
• Cities
• Actions:
• Go to adjacent city
• Transition model
• Result(A, Go(B)) = B
• Step cost
• Distance along road link 
• Start state:
• Arad
• Goal test:
• Is state == Bucharest?
• Solution?
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 6}
Content: What’s in a State Space?
• Problem: Pathing
• State representation: (x,y) location
• Actions: NSEW
• Transition model: update location
• Goal test: is (x,y)=END
• Problem: Eat-All-Dots
• State representation: {(x,y), dot booleans}
• Actions: NSEW
• Transition model: update location and 
possibly a dot boolean
• Goal test: dots all false
The real-world state includes every detail of the environment
A search state abstracts away details not needed to solve the problem
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 7}
Content: State Space Sizes?
• World state:
• Agent positions: 120
• Food count: 30
• Ghost positions: 12
• Agent facing: NSEW
• How many
• World states?
120x(230)x(122)x4
• States for pathing?
120
• States for eat-all-dots?
120x(230)
00

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 8}
Content: Safe Passage
• Problem: eat all dots while keeping the ghosts scared
• What does the state representation have to specify?
• (agent position, dot booleans, power pellet booleans, remaining scared time)
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 9}
Content: State Space Graphs and Search Trees

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 10}
Content: State Space Graphs
• State space graph: A mathematical 
representation of a search problem
• Nodes are (abstracted) world configurations
• Arcs represent transitions resulting from actions
• The goal test is a set of goal nodes (maybe only one)
• In a state space graph, each state occurs only 
once!
• We can rarely build this full graph in memory 
(it’s too big), but it’s a useful idea

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 11}
Content: More Examples
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 12}
Content: More Examples
R
L
S S
S S
R
L
R
L
R
L
S
S S
S
L
L
L L R
R
R
R

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 13}
Content: State Space Graphs vs. Search Trees
S G
b
a
Consider this 4-state graph: 
Important: Lots of repeated structure in the search tree!
How big is its search tree (from S)?
S
a b
b G G a
G a b G
∞

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 14}
Content: Tree Search vs Graph Search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 15}
Content: function TREE_SEARCH(problem) returns a solution, or failure
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
for each resulting child from node
add child to the frontier

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 16}
Content: function GRAPH_SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 17}
Content: • What is the relationship between these sets of states after each loop 
iteration in GRAPH_SEARCH?
• (Loop invariants!!!)
A
Explored Never Seen
Frontier
B
Explored Never Seen
Frontier
C
Explored Never Seen
Frontier
0
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 18}
Content: • What is the relationship between these sets of states after each loop iteration in GRAPH_SEARCH?
• (Loop invariants!!!)
A
Explored Never Seen
Frontier
B
Explored Never Seen
Frontier
C
Explored Never Seen
Frontier
• The frontier states separate the explored states from never seen states
• Frontier is sub-set of Explored, as loop progresses, number of explored states will be 
more than the number of states infrontier
• Nodes that are not explored (Never Seen) are distinct from the other two 
0
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 19}
Content: Graph Search
• This graph search algorithm overlays a tree on a graph
• The frontier states separate the explored states from never seen 
states
Images: AIMA, Figure 3.8, 3.9

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 20}
Content: BFS vs DFS

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 21}
Content: • Is the following demo Part 1 using BFS or DFS
0
P

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 22}
Content: Video of Demo Maze Water DFS/BFS (part 1)
Search Strategies Demo

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 23}
Content: Video of Demo Maze Water DFS/BFS (part 2)
Search Strategies Demo

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 24}
Content: A Note on Implementation
Nodes have
state, parent, action, path-cost
1
3 2
5 4
6
7
1 8
3 2
5 4
6
7
8
Node
STATE
PARENT
ACTION = Right
PATH-COST = 6
A child of node by action a has
state = result(node.state,a)
parent = node
action = a
path-cost = node.path_cost +
step_cost(node.state, a, self.state)
Extract solution by tracing back parent pointers, collecting actions

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 25}
Content: Walk-through DFS Graph Search
S
G
d
b
p
q
c
e
h
a
f
r
Q,Q,R,E,P
Q,R,E,P
R,E,P
F,E,P
G,E,P
P
Q
R
F
G=Goal
S,D,B,A,C,E,H
S,D,B,A,C,E,H,P,
S,D,B,A,C,E,H,P,Q
S,D,B,A,C,E,H,P,Q,
F
S-D-B-A-C-E-H-P
S-D-B-A-C-E-H-PQ
S,D,B,A,C,E,H,P,Q,
R
S,D,B,A,C,E,H,P,Q,
R,F

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 26}
Content: BFS vs DFS
• When will BFS outperform DFS?
• When will DFS outperform BFS?

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 27}
Content: Search Algorithm Properties

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 28}
Content: Search Algorithm Properties
• Complete: Guaranteed to find a solution if one exists?
• Optimal: Guaranteed to find the least cost path?
• Time complexity?
• Space complexity?
• Cartoon of search tree:
• b is the branching factor
• m is the maximum depth
• solutions at various depths
• Number of nodes in entire tree?
• 1 + b + b2 + …. b
m = O(bm)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 29}
Content: Search Algorithm Properties
• Complete: Guaranteed to find a solution if one exists?
• Optimal: Guaranteed to find the least cost path?
• Time complexity?
• Space complexity?
• Cartoon of search tree:
• b is the branching factor
• m is the maximum depth
• solutions at various depths
• Number of nodes in entire tree?
• 1 + b + b2 + …. b
m = O(bm)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 30}
Content: • Are these the properties for BFS or DFS?
• Takes O(b
m) time
• Uses O(bm) space on frontier
• Complete with graph search
• Not optimal unless all goals are in the same level 
(and the same step cost everywhere)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 31}
Content: Depth-First Search (DFS) Properties
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers
• What nodes does DFS expand?
• Some left prefix of the tree.
• Could process the whole tree!
• If m is finite, takes time O(b
m)
• How much space does the frontier take?
• Only has siblings on path to root, so O(bm)
• Is it complete?
• m could be infinite, so only if we prevent 
cycles (graph search)
• Is it optimal?
• No, it finds the “leftmost” solution, 
regardless of depth or cost

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 32}
Content: Breadth-First Search (BFS) Properties
• What nodes does BFS expand?
• Processes all nodes above shallowest solution
• Let depth of shallowest solution be s
• Search takes time O(b
s
)
• How much space does the frontier take?
• Has roughly the last tier, so O(b
s
)
• Is it complete?
• s must be finite if a solution exists, so yes!
• Is it optimal?
• Only if costs are all the same (more on costs 
later)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
s tiers
b
s nodes

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 33}
Content: Iterative Deepening
…
b
• Idea: get DFS’s space advantage with BFS’s 
time / shallow-solution advantages
• Run a DFS with depth limit 1. If no solution…
• Run a DFS with depth limit 2. If no solution…
• Run a DFS with depth limit 3. …..
• Isn’t that wastefully redundant?
• Generally most work happens in the lowest level 
searched, so not so bad!

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 34}
Content: Finding a Least-Cost Path
START
GOAL
d
b
p
q
c
e
h
a
f
r
2
9 2
1 8
8
2
3
2
4
4
15
1
3
2
2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 35}
Content: Depth-First (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
S
G
d
b
p
q
c
e
h
a
f
r
q
p
h
d
b
a
c
e
r
Strategy: expand a 
deepest node first
Implementation: 
Frontier is a LIFO stack

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 36}
Content: Breadth-First (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
S
G
d
b
p
q
c
e
h
a
f
r
Search
Tiers
Strategy: expand a 
shallowest node first
Implementation: 
Frontier is a FIFO queue

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 37}
Content: Uniform Cost (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
Strategy: expand a cheapest 
node first:
Frontier is a priority queue 
(priority: cumulative cost)
S
G
d
b
p
q
c
e
h
a
f
r
3
9 1
4 16
11
5
13 7
8
11 10
17 11
0
6
3
9
1
1
2
8
8
2
15
1
2
Cost 
contours
2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 38}
Content: Uniform Cost Search

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 39}
Content: function GRAPH_SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 40}
Content: function UNIFORM-COST-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using node path_cost as the priority
add initial state of problem to frontier with path_cost = 0
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher path_cost then
replace that frontier node with child
S
A
B
C
D
G
1
4
2
4
1
3

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 41}
Content: Walk-through UCS
S
A
B
C
D
G
1
4
2
4
1
3
Priotity Queue Current Node Explored\Close List Path followed
A(1),B(4) S S
B(4), C(3) A(1) S S-A
B(4), D(7) C(3) S,A S-A-C
D(7),D(5) B(4) S,A,C S-A-C-B
D(7), G(8) D(5) S,A,C,B S-A-C-B-D
G(8) S,A,C,B,D
G(8) = goal S,A,C,B,D,G S-A-C-B-D-G
Path = S-B-D-G
Cost = 8

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 42}
Content: Walk
-through UCS
START
GOAL
d
b
p
q
c
e
h
a
f
r
2
9
2
18
8
2
3
2
4
4
15
1
3
2
2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 43}
Content: …
Uniform Cost Search (UCS) Properties
• What nodes does UCS expand?
• Processes all nodes with cost less than cheapest solution!
• If that solution costs C* and arcs cost at least  , then the 
“effective depth” is roughly C*/
• Takes time O(b
C*/
) (exponential in effective depth)
• How much space does the frontier take?
• Has roughly the last tier, so O(b
C*/
)
• Is it complete?
• Assuming best solution has a finite cost and minimum arc cost 
is positive, yes!
• Is it optimal?
• Yes! (Proof next lecture via A*)
b
C*/ “tiers”
c  3
c  2
c  1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 44}
Content: Uniform Cost Issues
• Remember:
 UCS explores increasing cost contours
• The good:
 UCS is complete and optimal!
• The bad:
• Explores options in every “direction”
• No information about goal location
• We’ll fix that soon!
Start Goal
…
c  3
c  2
c  1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 45}
Content: • Notes added on LMS
• https://www.oreilly.com/library/view/graph-algorithms/9781492047674/ch04.html
Recommended
Reading

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 46}
Content: ousoNs?
?

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 47}
Content: We become what we behold.
We shape our tools,
and thereafter our tools shape us.
-Marshall McLuhan

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 6
These lecture notes are heavily based on notes originally written by Josh Hug and Jacky Liang.
Probabilistic Inference
In artificial intelligence, we often want to model the relationships between various nondeterministic events.
If the weather predicts a 40% chance of rain, should I carry my umbrella? How many scoops of ice cream
should I get if the more scoops I get, the more likely I am to drop it all? If there was an accident 15 minutes
ago on the freeway on my route to Oracle Arena to watch the Warriors’ game, should I leave now or in 30
minutes? All of these questions (and innumerable more) can be answered with probabilistic inference.
We’re assuming that you’ve learned the foundations of probability in CS70, so these notes will not review
basic concepts of probability like PDFs, conditional probabilities, independence, and conditional independence.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 0}
Content: In previous sections of this class, we modeled the world as existing in a specific state that is always known.
For the next several weeks, we will instead use a new model where each possible state for the world has
its own probability. For example, we might build a weather model, where the state consists of the season,
temperature and weather. Our model might say that P(winter, 35, cloudy) = 0.023. This number represents
the probability of the specific outcome that it is winter, 35, and cloudy.
More precisely, our model is a joint distribution, i.e. a table of probabilities which captures the likelihood
of each possible outcome, also known as an assignment. As an example, consider the table below:
Season Temperature Weather Probability
summer hot sun 0.30
summer hot rain 0.05
summer cold sun 0.10
summer cold rain 0.05
winter hot sun 0.10
winter hot rain 0.05
winter cold sun 0.15
winter cold rain 0.20

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 0}
Content: This model allows us to answer questions that might be of interest to us, for example:
• What is the probability that it is sunny? P(W = sun)
• What is the probability distribution for the weather, given that we know it is winter? P(W | S = winter)
• What is the probability that it is winter, given that we know it is rainy and cold? P(S = winter | T =
cold,W = rain)
• What is the probability distribution for the weather and season give that we know that it is cold?
P(S,W | T = cold)
CS 188, Fall 2018, Note 6 1

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: Given a joint PDF, we can trivially perform compute any desired probablity distribution P(Q1 ...Qk | e1 ...ek)
using a simple and intuitive procedure known as inference by enumeration, for which we define three types
of variables we will be dealing with:
1. Query variables Qi, which are unknown and appear on the left side of the probability distribution we
are trying to compute.
2. Evidence variables ei, which are observed variables whose values are known and appear on the right
side of the probability distribution we are trying to compute.
3. Hidden variables, which are values present in the overall joint distribution but not in the distribution
we are currently trying to compute.
In this procedure, we collect all the rows consistent with the observed evidence variables, sum out all the
hidden variables, and finally normalize the table so that it is a probability distribution (i.e. values sum to 1).

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: For example, if we wanted to compute P(W | S = winter), we’d select the four rows where S is winter, then
sum out over T and normalize. This yields the following probability table:
W S Unnormalized Sum Probability
sun winter 0.10+0.15 = 0.25 0.25/(0.25+0.25) = 0.5
rain winter 0.05+0.20 = 0.25 0.25/(0.25+0.25) = 0.5
Hence P(W = sun | S = winter) = 0.5 and P(W = rain | S = winter) = 0.5, and we learn that in winter
there’s a 50% chance of sun and a 50% chance of rain (classic California weather).
So long as we have the joint PDF table, inference by enumeration (IBE) can be used to compute any desired
probablity distribution, even for multiple query variables Q1...Qk.
Bayes Nets (Representation)
While inference by enumeration can compute probabilities for any query we might desire, representing an
entire joint distribution in the memory of a computer is impractical for real problems - if each of n variables

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: we wish to represent can take on d possible values (it has a domain of size d), then our joint distribution
table will have dn entries, exponential in the number of variables and quite impractical to store!
Bayes nets avoid this issue by taking advantage of the idea of conditional probability. Rather than storing
information in a giant table, probabilities are instead distributed across a large number of smaller local
probability tables along with a directed acyclic graph (DAG) which captures the relationships between
variables. The local probability tables and the DAG together encode enough information to compute any
probability distribution that we could have otherwise computed given the entire joint distribution.
Specifically, each node in the graph represents a single random variable and each directed edge represents
one of the conditional probability distributions we choose to store (i.e. an edge from node A to node B

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: indicates that we store the probability table for P(B|A)). Each node is conditionally independent of all its
ancestor nodes in the graph, given all of its parents. Thus, if we have a node representing variable X, we
store P(X|A1,A2,...,AN), where A1,...,AN are the parents of X.
As an example of a Bayes Net, consider a model where we have five binary random variables described
below:
CS 188, Fall 2018, Note 6 2

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 2}
Content: • B: Burglary occurs.
• A: Alarm goes off.
• E: Earthquake occurs.
• J: John calls.
• M: Mary calls.
Assume the alarm can go off if either a burglary or an earthquake occurs, and that Mary and John will call
if they hear the alarm. We can represent these dependencies with the graph shown below.
As a reality check, it’s important to internalize that Bayes Nets are only a type of model. Models attempt
to capture the way the world works, but because they are always a simplification they are always wrong.
However, with good modeling choices they can still be good enough approximations that they are useful for
solving real problems in the real world. In general, they will not account for every variable or even every
interaction between variables.
Returning to our discussion, we formally define a Bayes Net as consisting of:
• A directed acyclic graph of nodes, one per variable X.
• A conditional distribution for each node P(X|A1 ...An), where Ai is the i

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 2}
Content: th parent of X, stored as a
conditional probability table or CPT. Each CPT has n+2 columns: one for the values of each of the
n parent variables A1 ...An, one for the values of X, and one for the conditional probability of X.
In the alarm model above, we would store probability tables P(B),P(E),P(A | B,E),P(J | A) and P(M | A).
Given all of the CPTs for a graph, we can calculate the probability of a given assignment using the chain
rule: P(X1,X2,...,Xn) = ’n
i=1 P(Xi|parents(Xi)).
For the alarm model above, we might calculate the probability of one event as follows: P(b,e,+a,+j,m) =
P(b)·P(e)·P(+a|b,e)·P(+j|+a)·P(m|+a).
CS 188, Fall 2018, Note 6 3
B
E
A
T
M

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: This works because of the conditional independence relationships given by the graph. Specifically, we rely
on the fact that P(xi|x1,..., xi1) = P(xi|parents(Xi)). Or in other words, that the probability of a specific
value of Xi depends only on the values assigned to Xi’s parents.
Bayes Nets (Inference)
Inference is the process of calculating the joint PDF for some set of query variables based on some set
of observed variables. We can solve this problem naively by forming the joint PDF and using inference by
enumeration as described above. This requires the creation of and iteration over an exponentially large table.
An alternate approach is to eliminate variables one by one. To eliminate a variable X, we:
1. Join (multiply together) all factors involving X.
2. Sum out X.
A factor is defined simply as an unnormalized probability. At all points during variable elimination, each

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: factor will be proportional to the probability it corresponds to but the underlying distribution for each factor
won’t necessarily sum to 1 as a probability distribution should.
Let’s make these ideas more concrete with an example. Suppose we have a model as shown below, where
T, C, S, and E can take on binary values, as shown below. Here, T represents the chance that an adventurer
takes a treasure, C represents the chance that a cage falls on the adventurer given that he takes the treasure,
S represents the chance that snakes are released if an adventurer takes the treasure, and E represents the
chance that the adventurer escapes given information about the status of the cage and snakes.
In this case, we have the factors P(T), P(C|T), P(S|T), and P(E|C,S). Suppose we want to calculate
P(T| + e). The inference by enumeration approach would be to form the 16 row joint PDF P(T,C,S,E),
select only the rows corresponding to +e, then summing out C and S and finally normalizing.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: The alternate approach is to eliminate C, then S, one variable at a time. We’d proceed as follows:
• Join (multiply) all the factors involving C, forming P(C,+e|T,S) = P(C|T)·P(+e|C,S).
• Sum out C from this new factor, leaving us with a new factor P(+e|T,S).
• Join all factors involving S, forming P(+e,S|T) = P(S|T)·P(+e|T,S).
CS 188, Fall 2018, Note 6 4
T
C
S
E

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 4}
Content: • Sum out S, yielding P(+e|T).
Once we have P(+e|T), we can easily compute P(T|+e).
While this process is more involved from a conceptual point of view, the maximum size of any factor
generated is only 8 rows instead of 16 as it would be if we formed the entire joint PDF.
An alternate way of looking at the problem is to observe that the calculation of P(+e,T) can either be done,
as it is in inference by enumeration, as follows:
ÂsÂc
P(T)P(s|T)P(c|T)P(+e|c,s)
Variable elimination is equivalent to calculating P(+e,T) as follows:
P(T)ÂsP(s|T)ÂcP(c|T)P(+e|c,s)
Bayes Nets (Sampling)
An alternate approach for probabilistic reasoning is to implicitly calculate the probabilities for our query by
simply counting samples.
For example, suppose we wanted to calculate P(T| + e). If we had a magic machine that could generate
samples from our distribution, we could collect all samples for which the adventurer escapes the maze, and

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 4}
Content: then compute the fraction of those escapes for which the adventurer also took the treasure. Put differently,
if we could run simulations of say, a few million adventurers, we’d easily be able to compute any inference
we’d want just by looking at the samples.
Given a Bayes Net model, we can easily write a simulator. For example, consider the CPTs given below for
the simplified model with only two variables T and C.
CS 188, Fall 2018, Note 6 5
T
P(T)
+t
0.99
T
-t
0.01
T
C
P(C|T)
+t
+C
0.95
+t
-C
0.05
-t
+C
0.0
-t
-C
1.0

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: A simple simulator in Python would be as follows:
import random
def get_t():
if random.random() < 0.99:
return True
return False
def get_c(t):
if t and random.random() < 0.95:
return True
return False
def get_sample():
t = get_t()
c = get_c(t)
return [t, c]
We call this simple approach prior sampling. The downside of this approach is that it may require the
generation of a very large number of samples in order to perform analysis of unlikely scenarios. If we
wanted to compute P(C|t), we’d have to throw away 99% of our samples.
One way to mitigate this this problem, we can modify our procedure to early reject any sample inconsistent
with our evidence. For example, for the query P(C|t), we’d avoid generating a value for C unless t is true.
This still means we have to throw away most of our samples, but at least the bad samples we generate take
less time to create. We call this approach rejection sampling.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: These two approaches work for the same reason, which is that any valid sample occurs with the same
probability as specified in the joint PDF. In other words, the probability of every sample is based on the
product of every CPT, or as I personally call it, the "every CPT participates principle".
A more exotic approach is likelihood weighting, which ensures that we never generate a bad sample. In
this approach, we manually set all variables equal to the evidence in our query. For example, if we wanted
to compute P(C|t), we’d simply declare that t is false. The problem here is that this may yield samples
that are inconsistent with the correct distribution. As an example, consider the more complex four variable
model for T, C, S, and E given earlier in these notes. If we wanted to compute P(T,S,+c,+e), and simply
picked values for T and S without taking into account the fact that c = false, and e = true, then there’s no

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: guarantee that our samples actually obey the joint PDF given by the Bayes Net. For example, if the cage
only ever falls if the treasure is taken, then we’d want to ensure that T is always true instead of using the
P(T) distribution given in the Bayes Net.
Put differently, if we simply force some variables equal to the evidence, then our samples occur with probability given only equal to the products of the CPTs of the non-evidence variables. This means the joint PDF
has no guarantee of being correct (though may be for some cases like our two variable Bayes Net). Instead,
if we have sampled variables Z1 through Zp and fixed evidence variables E1 through Em a sample is given
by the probability P(Z1...Zp,E1...Em) = ’p
i P(Zi)|Parents(Zi). What is missing is that the probability of a
sample does not include all the probabilities of P(Ei|Parents(Ei)), i.e. not every CPT participates.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: Likelihood weighting solves this issue by using a weight for each sample, which is the probability of the
evidence variables given the sampled variables. That is, instead of counting all samples equally, we can
CS 188, Fall 2018, Note 6 6

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: define a weight wj for sample j that reflects how likely the observed values for the evidence variables are,
given the sampled values. In this way, we ensure that every CPT participates. To do this, we iterate through
each variable in the Bayes net, as we do for normal sampling), sampling a value if the variable is not an
evidence variable, or changing the weight for the sample if the variable is evidence.
For example, suppose we want to calculate P(T|+c,+e). For the jth sample, we’d perform the following
algorithm:
• Set wj to 1.0, and c = true and e = true.
• For T: This is not an evidence variable, so we sample tj from P(T).
• For C: This is an evidence variable, so we multiply the weight of the sample by P(+c|tj), i.e. wj =
wj ·P(+c|tj).
• For S: sample sj from P(S | tj).
• For E: multiply the weight of the sample by P(+e|+c,sj), i.e. wj = wj ·P(+e|+c,sj).
Then when we perform the usual counting process, we weight sample j by wj instead of 1, where 0 <=

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: wj <= 1. This approach works because in the final calculations for the probabilities, the weights effectively
serve to replace the missing CPTs. In effect, we ensure that the weighted probability of each sample is given
by P(z1...zp, e1...em)=[’p
i P(zi | Parents(zi))] · [’m
i P(ei) | Parents(ei))].
For all three of our sampling methods (prior sampling, rejection sampling, and likelihod weighting), we
can get increasing amounts of accuracy by generating additional samples. However, of the three, likelihood
weighting is the most computationally efficient, for reasons beyond the scope of this course.
Gibbs Sampling is a fourth approach for sampling. In this approach, we first set all variables to some totally
random value (not taking into account any CPTs). We then repeatedly pick one variable at a time, clear its
value, and resample it given the values currently assigned to all other variables.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: For the T,C,S,E example above, we might assign t = true, c = true, s = false, and e = true. We then pick
one of our four variables to resample, say S, and clear it. We then pick a new variable from the distribution
P(S| +t,+c,+e). This requires us knowing this conditional distribution. It turns out that we can easily
compute the distribution of any single variable given all other variables. More specifically, P(S|T,C,E) can
be calculated only using the CPTs that connect S with its neighbors. Thus, in a typical Bayes Net, where
most variables have only a small number of neighbors, we can precompute the conditional distributions for
each variable given all of its neighbors in linear time.
We will not prove this, but if we repeat this process enough times, our later samples will eventually converge
to the correct distribution even though we may start from a low-probability assignment of values. If you’re

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: curious, there are some caveats beyond the scope of the course that you can read about under the Failure
Modes section of the Wikipedia article for Gibbs Sampling.
Bayes Nets (D-Separation)
One useful question to ask about a set of random variables is whether or not one variable is independent from
another, or if one random variable is conditionally independent of another given a third random variable.
Bayes’ Nets representation of joint probability distributions gives us a way to quickly answer such questions
by inspecting the topological structure of the graph.
CS 188, Fall 2018, Note 6 7

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 7}
Content: We already mentioned that a node is conditionally independent of all its ancestor nodes in the graph
given all of its parents.
We will present all three canonical cases of connected three-node two-edge Bayes’ Nets, or triples, and the
conditional independence relationships they express.
Causal Chains
Figure 1: Causal Chain with no observations. Figure 2: Causal Chain with Y observed.
Figure 1 is a configuration of three nodes known as a causal chain. It expresses the following representation
of the joint distribution over X, Y, and Z:
P(x, y,z) = P(z|y)P(y|x)P(x)
It’s important to note that X and Z are not guaranteed to be independent, as shown by the following counterexample:
P(y|x) = (
1 if x = y
0 else
P(z|y) = (
1 if z = y
0 else
In this case, P(z|x) = 1 if x = z and 0 otherwise, so X and Z are not independent.
However, we can make the statement that X ?? Z | Y, as in Figure 2. Recall that this conditional indepdence
means:
P(X|Z,Y) = P(X|Y)

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 7}
Content: We can prove this statement as follows:
P(X|Z, y) = P(X,Z, y)
P(Z, y) = P(Z|y)P(y|X)P(X)
Âx P(X, y,Z) = P(Z|y)P(y|X)P(X)
P(Z|y)Âx P(y|x)P(x)
= P(y|X)P(X)
Âx P(y|x)P(x) = P(y|X)P(X)
P(y) = P(X|y)
An analogous proof can be used to show the same thing for the case where X has multiple parents. To
summarize, in the causal chain chain configuration, X ?? Z | Y.
Common Cause
Another possible configuration for a triple is the common cause. It expresses the following representation:
P(x, y,z) = P(x|y)P(z|y)P(y)
Just like with causal chain, we can show that X is not guaranteed to be independent of Z with the following
counterexample distribution:
CS 188, Fall 2018, Note 6 8
X
Y
ZX
Y
Z

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 8}
Content: Figure 3: Common Cause with no observations. Figure 4: Common Cause with Y observed.
P(x|y) = (
1 if x = y
0 else
P(z|y) = (
1 if z = y
0 else
Then P(x|z) = 1 if x = z and 0 otherwise, so X and Z are not independent.
But it is true that X ?? Z | Y. That is, X and Z are independent if Y is observed as in Figure 4. We can show
this as follows:
P(X|Z, y) = P(X,Z, y)
P(Z, y) = P(X|y)P(Z|y)P(y)P(Z|y)P(y) = P(X|y)
Common E↵ect
The final possible configuration for a triple is the common effect, as shown in the figures below.
Figure 5: Common Effect with no observations. Figure 6: Common Effect with Y observed.
It expresses the representation:
P(x, y,z) = P(y|x,z)P(x)P(z)
In the configuration shown in Figure 5, X and Z are independent: X ?? Z. However, they are not necessarily
independent when conditioned on Y (Figure 6). As an example, suppose all three are binary variables. X
CS 188, Fall 2018, Note 6 9
Y
X
ZY
X
ZX
Z
YX
Z
Y

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 9}
Content: and Z are true and false with equal probability:
P(X = true) = P(X = f alse) = 0.5
P(Z = true) = P(Z = f alse) = 0.5
and Y is determined by whether X and Z have the same value:
P(Y|X,Z) =
8
><
>:
1 if X = Z and Y = true
1 if X 6= Z and Y = f alse
0 else
Then X and Z are independent if Y is unobserved. But if Y is observed, then knowing X will tell us the value
of Z, and vice-versa. So X and Z are not conditionally independent given Y.
Common Effect can be viewed as “opposite” to Causal Chains and Common Cause – X and Z are guaranteed
to be independent if Y is not conditioned on. But when conditioned on Y, X and Z may be dependent
depending on the specific probability values for P(Y | X,Z)).
This same logic applies when conditioning on descendents of Y in the graph. If one of Y’s descendent nodes
is observed, as in Figure 7, X and Z are not guaranteed to be independent.
Figure 7: Common Effect with child observations.
General Case, and D-separation

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 9}
Content: We can use the previous three cases as building blocks to help us answer conditional independence questions
on an arbitrary Bayes’ Net with more than three nodes and two edges. We formulate the problem as follows:
Given a Bayes Net G, two nodes X and Y, and a (possibly empty) set of nodes {Z1,...Zk} that represent
observed variables, must the following statement be true: X ?? Y|{Z1,... Zk}?
D-separation (directed separation) is a property of the structure of the Bayes Net graph that implies this
conditional independence relationship, and generalizes the cases we’ve seen above. If a set of variables
Z1,···Zk d-separates X and Y, then X ?? Y | {Z1,···Zk} in all possible distributions that can be encoded by
the Bayes net.
CS 188, Fall 2018, Note 6 10
X
Z
Y
W

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 10}
Content: We start with an algorithm that is based on a notion of reachability from node X to node Y. (Note: this
algorithm is not quite correct! We’ll see how to fix it in a moment.)
1. Shade all observed nodes {Z1,...Zk} in the graph.
2. If there exists an undirected path from X and Y that is not blocked by a shaded node, X and Y are
“connected”.
3. If X and Y are connected, they’re not conditionally independent given {Z1,...Zk}. Otherwise, they
are.
However, this algorithm only works if the Bayes’ Net has no Common Effect structure within the graph, because if it exists, then two nodes are “reachable” when the Y node in Common Effect is activated (observed).
To adjust for this, we arrive at the following d-separation algorithm:
1. Shade all observed nodes {Z1,...,Zk} in the graph.
2. Enumerate all undirected paths from X to Y.
3. For each path:
(a) Decompose the path into triples (segments of 3 nodes).
(b) If all triples are active, this path is active and d-connects X to Y.

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 10}
Content: 4. If no path d-connects X and Y, then X and Y are d-separated, so they are conditionally independent
given {Z1,...,Zk}
Any path in a graph from X to Y can be decomposed into a set of 3 consecutive nodes and 2 edges - each
of which is called a triple. A triple is active or inactive depending on whether or not the middle node is
observed. If all triples in a path are active, then the path is active and d-connects X to Y, meaning X is
not guaranteed to be conditionally independent of Y given the observed nodes. If all paths from X to Y are
inactive, then X and Y are conditionally independent given the observed nodes.
Active triples: We can enumerate all possibilities of active and inactive triples using the three canonical
graphs we presented above in Figure 8 and 9.
CS 188, Fall 2018, Note 6 11

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 11}
Content: Figure 8: Active triples Figure 9: Inactive triples
Examples
Here are some examples of applying the d-separation algorithm:
This graph contains the common effect and causual
chain canonical graphs.
a) R ?? B – Guaranteed
b) R ?? B | T – Not guaranteed
c) R ?? B | T0 – Not guaranteed
d) R ?? T0 | T – Guaranteed
CS 188, Fall 2018, Note 6 12
0008
C
OO
00
O
OQ
OQR
B
T
T'

Metadata: {'Instructor': 'Seemab Latif', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 12}
Content: This graph contains combinations of all three canonical graphs (can you list them all?).
a) L ?? T0 | T – Guaranteed
b) L ?? B – Guaranteed
c) L ?? B | T – Not guaranteed
d) L ?? B | T0 – Not guaranteed
e) L ?? B | T,R – Guaranteed
This graph contains combinations of all three canonical graphs.
a) T ?? D – Not guaranteed
b) T ?? D | R – Guaranteed
c) T ?? D | R,S – Not guaranteed
Conclusion
To summarize, Bayes’ Nets is a powerful representation of joint probability distributions. Its topological
structure encodes independence and conditional independence relationships, and we can use it to model
arbitrary distributions to perform inference and sampling.
CS 188, Fall 2018, Note 6 13
L
R
B
D
T
T'R
T
D
S

