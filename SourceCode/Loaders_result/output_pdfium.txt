[Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 0}, page_content='Dr. Seemab latif\r\nLecture 7\r\n12 Nov 2024\r\nA R T I F I C I A L I N T E L L I G E N C E\nArtificialIntelligence\n03\n+\nMachineLearning\nDeepLearning\n:0'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 1}, page_content='[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley.]\r\nProbability\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 2}, page_content='Today\r\n\uf0a7 Probability\r\n\uf0a7 Random Variables\r\n\uf0a7 Joint and Marginal Distributions\r\n\uf0a7 Conditional Distribution\r\n\uf0a7 Product Rule, Chain Rule, Bayes’ Rule\r\n\uf0a7 Inference\r\n\uf0a7 Independence\r\n\uf0a7 You’ll need all this stuff A LOT for the \r\nnext few weeks, so make sure you go \r\nover it now!\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 3}, page_content='Inference in Ghostbusters\r\n\uf0a7 A ghost is in the grid \r\nsomewhere\r\n\uf0a7 Sensor readings tell how \r\nclose a square is to the \r\nghost\r\n\uf0a7 On the ghost: red\r\n\uf0a7 1 or 2 away: orange\r\n\uf0a7 3 or 4 away: yellow\r\n\uf0a7 5+ away: green\r\nP(red | 3) P(orange | 3) P(yellow | 3) P(green | 3)\r\n0.05 0.15 0.5 0.3\r\n\uf0a7 Sensors are noisy, but we know P(Color | Distance)\r\n[Demo: Ghostbuster – no probability (L12D1) ]\n网'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 4}, page_content='Video of Demo Ghostbuster – No probability\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 5}, page_content='Uncertainty\r\n\uf0a7 General situation:\r\n\uf0a7 Observed variables (evidence): Agent knows certain \r\nthings about the state of the world (e.g., sensor \r\nreadings or symptoms)\r\n\uf0a7 Unobserved variables: Agent needs to reason about \r\nother aspects (e.g. where an object is or what disease is \r\npresent)\r\n\uf0a7 Model: Agent knows something about how the known \r\nvariables relate to the unknown variables\r\n\uf0a7 Probabilistic reasoning gives us a framework for \r\nmanaging our beliefs and knowledge\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.110.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.17<0.01\n<0.01\n0.03\n<0.01\n0.05\n0.05\n<0.01\n0.05\n0.81<0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 6}, page_content='Random Variables\r\n\uf0a7 A random variable is some aspect of the world about \r\nwhich we (may) have uncertainty\r\n\uf0a7 R = Is it raining?\r\n\uf0a7 T = Is it hot or cold?\r\n\uf0a7 D = How long will it take to drive to work?\r\n\uf0a7 L = Where is the ghost?\r\n\uf0a7 We denote random variables with capital letters\r\n\uf0a7 Random variables have domains\r\n\uf0a7 R in {true, false} (often write as {+r, -r})\r\n\uf0a7 T in {hot, cold}\r\n\uf0a7 D in [0, \uf0a5)\r\n\uf0a7 L in possible locations, maybe {(0,0), (0,1), …}\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 7}, page_content='Probability Distributions\r\n\uf0a7 Associate a probability with each value\r\n\uf0a7 Temperature:\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.1\r\nfog 0.3\r\nmeteor 0.0\r\n\uf0a7 Weather: \nPM\nP\n100'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 8}, page_content='Shorthand notation:\r\nOK if all domain entries are unique\r\nProbability Distributions\r\n\uf0a7 Unobserved random variables have distributions\r\n\uf0a7 A distribution is a TABLE of probabilities of values\r\n\uf0a7 A probability (lower case value) is a single number\r\n\uf0a7 Must have: and\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.1\r\nfog 0.3\r\nmeteor 0.0\nPM\nP\n1P(W :\n=rain)\n0.1\nIP(X\nVc\n0\n=P(X\nP\n1P(\nP(W\nrazn\nraznP(hot\nP(T\nhot\n川P(cold)\nP(T\n(1o'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 9}, page_content='Joint Distributions\r\n\uf0a7 A joint distribution over a set of random variables:\r\nspecifies a real number for each assignment (or outcome): \r\n\uf0a7 Must obey:\r\n\uf0a7 Size of distribution if n variables with domain sizes d?\r\n\uf0a7 For all but the smallest distributions, impractical to write out!\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\n7\n2\nn\n了\n？P(X1\n=c1, X2\n川\nCnD\ncn.\nC\nC\n1\n2\n?\n了\n了P(\nα1,C2,.\nCn）\n0P(1,c2,.\nCn)\n1\n=\nx1,c2,...MnP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 10}, page_content='Probabilistic Models\r\n\uf0a7 A probabilistic model is a joint distribution \r\nover a set of random variables\r\n\uf0a7 Probabilistic models:\r\n\uf0a7 (Random) variables with domains \r\n\uf0a7 Assignments are called outcomes\r\n\uf0a7 Joint distributions: say whether assignments \r\n(outcomes) are likely\r\n\uf0a7 Normalized: sum to 1.0\r\n\uf0a7 Ideally: only certain variables directly interact\r\n\uf0a7 Constraint satisfaction problems:\r\n\uf0a7 Variables with domains\r\n\uf0a7 Constraints: state whether assignments are \r\npossible\r\n\uf0a7 Ideally: only certain variables directly interact\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nT W P\r\nhot sun T\r\nhot rain F\r\ncold sun F\r\ncold rain T\r\nDistribution over T,W\r\nConstraint over T,W\n二00\n？'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 11}, page_content='Events\r\n\uf0a7 An event is a set E of outcomes\r\n\uf0a7 From a joint distribution, we can \r\ncalculate the probability of any event\r\n\uf0a7 Probability that it’s hot AND sunny?\r\n\uf0a7 Probability that it’s hot?\r\n\uf0a7 Probability that it’s hot OR sunny?\r\n\uf0a7 Typically, the events we care about \r\nare partial assignments, like P(T=hot)\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\nP(E)\nP(c1\nCn\nc1...Cn\nn)EEP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 12}, page_content='Quiz: Events\r\n\uf0a7 P(+x, +y) ? \uf0a7 P(+x) ? \uf0a7 P(-y OR +x) ?\r\nXYP\r\n+x +y 0.2\r\n+x-y 0.3\r\n-x +y 0.4\r\n-x-y 0.1\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 13}, page_content='Marginal Distributions\r\n\uf0a7 Marginal distributions are sub-tables which eliminate variables \r\n\uf0a7 Marginalization (summing out): Combine collapsed rows by adding\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.4\nP\nM\n+\n了P(t)\nP(t,s)\nSPP(s)\nP(t, s)\nfM\nP\n1P(X1 =c1) =\nP(X1 = x1, X2 = ∞2)\nC2P(x,y)\nP(x)\nP(v)=\nMP(x,y)\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 14}, page_content='Quiz: Marginal Distributions\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\nX P\r\n+x\r\n-x\r\nY P\r\n+y\r\n-y\nDP(c)\nP(\n(x,y)\nhD(y)\nP(\nP(?\nx,y)\nIDP(x,y)\nP(x)\nP(v)=\nMP(x,y)\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 15}, page_content='Conditional Probabilities\r\n\uf0a7 A simple relation between joint and conditional probabilities\r\n\uf0a7 In fact, this is taken as the definition of a conditional probability\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nP(a) P(b)\r\nP(a,b)\nP(a,b)\nP(a|b)\nP\n6P\nM\n+\n了P(W\n[T\n=???\nS\nC)P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.5×P(W\n= s,T\n= c) + P(W\n= r,T :\n=C)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 16}, page_content='Quiz: Conditional Probabilities\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\n\uf0a7 P(+x | +y) ?\r\n\uf0a7 P(-x | +y) ?\r\n\uf0a7 P(-y | +x) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 17}, page_content='Conditional Distributions\r\n\uf0a7 Conditional distributions are probability distributions over \r\nsome variables given fixed values of others\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.8\r\nrain 0.2\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nConditional Distributions Joint Distribution\nM\nhot\n>P\nW\ncold\n7\n1P\nM\n+\n了一'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 18}, page_content='Normalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\nP(W :\n==\nCM)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了P(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 19}, page_content='SELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNormalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nT W P\r\ncold sun 0.2\r\ncold rain 0.3\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\nP(W :\nT=\n=???\n三r\n c)M)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了P(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nCM\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 20}, page_content='Normalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nT W P\r\ncold sun 0.2\r\ncold rain 0.3\r\nSELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\r\n\uf0a7 Why does this work? Sum of selection is P(evidence)! (P(T=c), here)\nP\nM\n+\n了M\nD\nCM\nD\nCP(\n(1,2)\nP(\n(C1,C2)\nP(c1|∞2)\n=\nP(c2)\nZc1\nP(x1,2)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 21}, page_content='Quiz: Normalization Trick\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\nSELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\r\n\uf0a7 P(X | Y=-y) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 22}, page_content='\uf0a7 (Dictionary) To bring or restore to a normal condition\r\n\uf0a7 Procedure:\r\n\uf0a7 Step 1: Compute Z = sum over all entries\r\n\uf0a7 Step 2: Divide every entry by Z\r\n\uf0a7 Example 1\r\nTo Normalize\r\nAll entries sum to ONE\r\nW P\r\nsun 0.2\r\nrain 0.3 Z = 0.5\r\nW P\r\nsun 0.4\r\nrain 0.6\r\n\uf0a7 Example 2\r\nT W P\r\nhot sun 20\r\nhot rain 5\r\ncold sun 10\r\ncold rain 15\r\nNormalize\r\nZ = 50\r\nNormalize\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 23}, page_content='Probabilistic Inference\r\n\uf0a7 Probabilistic inference: compute a desired probability \r\nfrom other known probabilities (e.g. conditional from \r\njoint)\r\n\uf0a7 We generally compute conditional probabilities \r\n\uf0a7 P(airport on time | no reported accidents) = 0.90\r\n\uf0a7 These represent the agent’s beliefs given the evidence\r\n\uf0a7 Probabilities change with new evidence:\r\n\uf0a7 P(airport on time | no accidents, 5 a.m.) = 0.95\r\n\uf0a7 P(airport on time | no accidents, 5 a.m., raining) = 0.80\r\n\uf0a7 Observing new evidence causes beliefs to be updated\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 24}, page_content='Inference by Enumeration\r\n\uf0a7 General case:\r\n\uf0a7 Evidence variables: \r\n\uf0a7 Query* variable:\r\n\uf0a7 Hidden variables: All variables\r\n* Works fine with \r\nmultiple query \r\nvariables, too\r\n\uf0a7 We want:\r\n\uf0a7 Step 1: Select the \r\nentries consistent \r\nwith the evidence\r\n\uf0a7 Step 2: Sum out H to get joint \r\nof Query and evidence\r\n\uf0a7 Step 3: Normalize\n1\nN\nn\n7\n7E1\nEk\n:e1\nekH\nH\n7P\ne\n1\ne\nk\n√\n&D\ne\n1\ne\nk\n&\n7P(Q,1\nh1\nhr,\ne1\nek)\nh...hr1\nN\nn\n7\n7P(x)\n-3\n0.05\n-1\n0.25\n0.07\n1\n0.2\n5\n0.01\n2\n0.15Z\nP(Q,e\ne1\nek\nb1\nP(Qle1\nP(Q,6\nek\n-Z\ne1\nek'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 25}, page_content='28\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\nhot\nfog\nsummer\n0.01\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 26}, page_content='29\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nhot\nfog\nsummer\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\ncold\nfog\nsummer\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 27}, page_content='30\nInference by Enumeration\nSeason\nTemp\nWeather\nhot\nsummer\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\nwinter\nhot\nsun\n0.10\nMintei\nhot\nrain\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\nwinter\ncold\nfog\n0.18\nwjnter\ncald\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 28}, page_content='31\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nsummer\ncold\nrain\n0.05\nvariable(s)\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 29}, page_content='32\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nhot\nsummer\nsun\n0.35\nP(SIsun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 30}, page_content='33\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 31}, page_content='34\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(SI sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nP(S|sun)=\nwinter\nhot\nsun\n0.10\n{summer:0.45/(0.45+0.25),\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter: 0.25/(0.45+0.25))\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 32}, page_content='Inference by Enumeration\r\n\uf0a7 P(W)?\r\n\uf0a7 P(W | winter)?\r\n\uf0a7 P(W | winter, hot)?\r\nS T W P\r\nsummer hot sun 0.30\r\nsummer hot rain 0.05\r\nsummer cold sun 0.10\r\nsummer cold rain 0.05\r\nwinter hot sun 0.10\r\nwinter hot rain 0.05\r\nwinter cold sun 0.15\r\nwinter cold rain 0.20\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 33}, page_content='\uf0a7 Obvious problems:\r\n\uf0a7 Worst-case time complexity O(d\r\nn\r\n) \r\n\uf0a7 Space complexity O(d\r\nn\r\n) to store the joint distribution\r\nInference by Enumeration\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 34}, page_content='The Product Rule\r\n\uf0a7 Sometimes have conditional distributions but want the joint\nP(x,y)\nP(xly):\nP(\n(y)P(?\n(y)P(\nP(\ny)\nx,y\nI8\nX\n='), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 35}, page_content='The Product Rule\r\n\uf0a7 Example:\r\nR P\r\nsun 0.8\r\nrain 0.2\r\nD W P\r\nwet sun 0.1\r\ndry sun 0.9\r\nwet rain 0.7\r\ndry rain 0.3\r\nD W P\r\nwet sun 0.08\r\ndry sun 0.72\r\nwet rain 0.14\r\ndry rain 0.06\nP\nM\n?\n1D\nM\n了M\nP\n1P(?\n(y)P(\nP(\ny)\nx,y\nI'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 36}, page_content='The Chain Rule\r\n\uf0a7 More generally, can always write any joint distribution as an \r\nincremental product of conditional distributions\r\n\uf0a7 Why is this always true?\nP(x1,2,3) = P(x1)P(x2|x1)P(x3|1,2)P(x1,x2,...cn) =\n11\nP(xic...i-1)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 37}, page_content='Bayes Rule\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 38}, page_content='Bayes’ Rule\r\n\uf0a7 Two ways to factor a joint distribution over two variables:\r\n\uf0a7 Dividing, we get:\r\n\uf0a7 Why is this at all helpful?\r\n\uf0a7 Lets us build one conditional from its reverse\r\n\uf0a7 Often one conditional is tricky but the other one is simple\r\n\uf0a7 Foundation of many systems we’ll see later (e.g. ASR, MT)\r\n\uf0a7 In the running for most important AI equation!\r\nThat’s my rule!\nP(c,y)\nP(c|y)P(y)P(y|∞)\nP(cly) :\n(P(c)\n=\nP\n(y)y'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 39}, page_content='Inference with Bayes’ Rule\r\n\uf0a7 Example: Diagnostic probability from causal probability:\r\n\uf0a7 Example:\r\n\uf0a7 M: meningitis, S: stiff neck\r\n\uf0a7 Note: posterior probability of meningitis still very small\r\n\uf0a7 Note: you should still get stiff necks checked out! Why?\nP(s I m) = 0.8\nExample\nP(m) = 0.0001\ngivens\nP(s) = 0.01\nP(s|m)P(m)\n0.8x0.0001\nP(m|s)=\nP(s)\n0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 40}, page_content='Quiz: Bayes’ Rule\r\n\uf0a7 Given:\r\n\uf0a7 What is P(W | dry) ? \r\nR P\r\nsun 0.8\r\nrain 0.2\r\nD W P\r\nwet sun 0.1\r\ndry sun 0.9\r\nwet rain 0.7\r\ndry rain 0.3\nP\nM\n?\n1M\nP\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 41}, page_content='Ghostbusters, Revisited\r\n\uf0a7 Let’s say we have two distributions:\r\n\uf0a7 Prior distribution over ghost location: P(G)\r\n\uf0a7 Let’s say this is uniform\r\n\uf0a7 Sensor reading model: P(R | G)\r\n\uf0a7 Given: we know what our sensors do\r\n\uf0a7 R = reading color measured at (1,1)\r\n\uf0a7 E.g. P(R = yellow | G=(1,1)) = 0.1\r\n\uf0a7 We can calculate the posterior \r\ndistribution P(G|r) over ghost locations \r\ngiven a reading using Bayes’ rule:\r\n[Demo: Ghostbuster – with probability (L12D2) ]\n0.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.170.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11(46)d\n(6)d(6|)d x'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 42}, page_content='Video of Demo Ghostbusters with Probability\n')][Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 0}, page_content='Dr. Seemab latif\r\nLecture 7\r\n12 Nov 2024\r\nA R T I F I C I A L I N T E L L I G E N C E\nArtificialIntelligence\n03\n+\nMachineLearning\nDeepLearning\n:0'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 1}, page_content='[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley.]\r\nProbability\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 2}, page_content='Today\r\n\uf0a7 Probability\r\n\uf0a7 Random Variables\r\n\uf0a7 Joint and Marginal Distributions\r\n\uf0a7 Conditional Distribution\r\n\uf0a7 Product Rule, Chain Rule, Bayes’ Rule\r\n\uf0a7 Inference\r\n\uf0a7 Independence\r\n\uf0a7 You’ll need all this stuff A LOT for the \r\nnext few weeks, so make sure you go \r\nover it now!\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 3}, page_content='Inference in Ghostbusters\r\n\uf0a7 A ghost is in the grid \r\nsomewhere\r\n\uf0a7 Sensor readings tell how \r\nclose a square is to the \r\nghost\r\n\uf0a7 On the ghost: red\r\n\uf0a7 1 or 2 away: orange\r\n\uf0a7 3 or 4 away: yellow\r\n\uf0a7 5+ away: green\r\nP(red | 3) P(orange | 3) P(yellow | 3) P(green | 3)\r\n0.05 0.15 0.5 0.3\r\n\uf0a7 Sensors are noisy, but we know P(Color | Distance)\r\n[Demo: Ghostbuster – no probability (L12D1) ]\n网'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 4}, page_content='Video of Demo Ghostbuster – No probability\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 5}, page_content='Uncertainty\r\n\uf0a7 General situation:\r\n\uf0a7 Observed variables (evidence): Agent knows certain \r\nthings about the state of the world (e.g., sensor \r\nreadings or symptoms)\r\n\uf0a7 Unobserved variables: Agent needs to reason about \r\nother aspects (e.g. where an object is or what disease is \r\npresent)\r\n\uf0a7 Model: Agent knows something about how the known \r\nvariables relate to the unknown variables\r\n\uf0a7 Probabilistic reasoning gives us a framework for \r\nmanaging our beliefs and knowledge\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.110.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.17<0.01\n<0.01\n0.03\n<0.01\n0.05\n0.05\n<0.01\n0.05\n0.81<0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 6}, page_content='Random Variables\r\n\uf0a7 A random variable is some aspect of the world about \r\nwhich we (may) have uncertainty\r\n\uf0a7 R = Is it raining?\r\n\uf0a7 T = Is it hot or cold?\r\n\uf0a7 D = How long will it take to drive to work?\r\n\uf0a7 L = Where is the ghost?\r\n\uf0a7 We denote random variables with capital letters\r\n\uf0a7 Random variables have domains\r\n\uf0a7 R in {true, false} (often write as {+r, -r})\r\n\uf0a7 T in {hot, cold}\r\n\uf0a7 D in [0, \uf0a5)\r\n\uf0a7 L in possible locations, maybe {(0,0), (0,1), …}\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 7}, page_content='Probability Distributions\r\n\uf0a7 Associate a probability with each value\r\n\uf0a7 Temperature:\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.1\r\nfog 0.3\r\nmeteor 0.0\r\n\uf0a7 Weather: \nPM\nP\n100'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 8}, page_content='Shorthand notation:\r\nOK if all domain entries are unique\r\nProbability Distributions\r\n\uf0a7 Unobserved random variables have distributions\r\n\uf0a7 A distribution is a TABLE of probabilities of values\r\n\uf0a7 A probability (lower case value) is a single number\r\n\uf0a7 Must have: and\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.1\r\nfog 0.3\r\nmeteor 0.0\nPM\nP\n1P(W :\n=rain)\n0.1\nIP(X\nVc\n0\n=P(X\nP\n1P(\nP(W\nrazn\nraznP(hot\nP(T\nhot\n川P(cold)\nP(T\n(1o'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 9}, page_content='Joint Distributions\r\n\uf0a7 A joint distribution over a set of random variables:\r\nspecifies a real number for each assignment (or outcome): \r\n\uf0a7 Must obey:\r\n\uf0a7 Size of distribution if n variables with domain sizes d?\r\n\uf0a7 For all but the smallest distributions, impractical to write out!\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\n7\n2\nn\n了\n？P(X1\n=c1, X2\n川\nCnD\ncn.\nC\nC\n1\n2\n?\n了\n了P(\nα1,C2,.\nCn）\n0P(1,c2,.\nCn)\n1\n=\nx1,c2,...MnP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 10}, page_content='Probabilistic Models\r\n\uf0a7 A probabilistic model is a joint distribution \r\nover a set of random variables\r\n\uf0a7 Probabilistic models:\r\n\uf0a7 (Random) variables with domains \r\n\uf0a7 Assignments are called outcomes\r\n\uf0a7 Joint distributions: say whether assignments \r\n(outcomes) are likely\r\n\uf0a7 Normalized: sum to 1.0\r\n\uf0a7 Ideally: only certain variables directly interact\r\n\uf0a7 Constraint satisfaction problems:\r\n\uf0a7 Variables with domains\r\n\uf0a7 Constraints: state whether assignments are \r\npossible\r\n\uf0a7 Ideally: only certain variables directly interact\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nT W P\r\nhot sun T\r\nhot rain F\r\ncold sun F\r\ncold rain T\r\nDistribution over T,W\r\nConstraint over T,W\n二00\n？'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 11}, page_content='Events\r\n\uf0a7 An event is a set E of outcomes\r\n\uf0a7 From a joint distribution, we can \r\ncalculate the probability of any event\r\n\uf0a7 Probability that it’s hot AND sunny?\r\n\uf0a7 Probability that it’s hot?\r\n\uf0a7 Probability that it’s hot OR sunny?\r\n\uf0a7 Typically, the events we care about \r\nare partial assignments, like P(T=hot)\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\nP(E)\nP(c1\nCn\nc1...Cn\nn)EEP\nM\n+\n了'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 12}, page_content='Quiz: Events\r\n\uf0a7 P(+x, +y) ? \uf0a7 P(+x) ? \uf0a7 P(-y OR +x) ?\r\nXYP\r\n+x +y 0.2\r\n+x-y 0.3\r\n-x +y 0.4\r\n-x-y 0.1\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 13}, page_content='Marginal Distributions\r\n\uf0a7 Marginal distributions are sub-tables which eliminate variables \r\n\uf0a7 Marginalization (summing out): Combine collapsed rows by adding\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.4\nP\nM\n+\n了P(t)\nP(t,s)\nSPP(s)\nP(t, s)\nfM\nP\n1P(X1 =c1) =\nP(X1 = x1, X2 = ∞2)\nC2P(x,y)\nP(x)\nP(v)=\nMP(x,y)\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 14}, page_content='Quiz: Marginal Distributions\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\nX P\r\n+x\r\n-x\r\nY P\r\n+y\r\n-y\nDP(c)\nP(\n(x,y)\nhD(y)\nP(\nP(?\nx,y)\nIDP(x,y)\nP(x)\nP(v)=\nMP(x,y)\n+0.1+0.3'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 15}, page_content='Conditional Probabilities\r\n\uf0a7 A simple relation between joint and conditional probabilities\r\n\uf0a7 In fact, this is taken as the definition of a conditional probability\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nP(a) P(b)\r\nP(a,b)\nP(a,b)\nP(a|b)\nP\n6P\nM\n+\n了P(W\n[T\n=???\nS\nC)P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.5×P(W\n= s,T\n= c) + P(W\n= r,T :\n=C)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 16}, page_content='Quiz: Conditional Probabilities\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\n\uf0a7 P(+x | +y) ?\r\n\uf0a7 P(-x | +y) ?\r\n\uf0a7 P(-y | +x) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 17}, page_content='Conditional Distributions\r\n\uf0a7 Conditional distributions are probability distributions over \r\nsome variables given fixed values of others\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.8\r\nrain 0.2\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nConditional Distributions Joint Distribution\nM\nhot\n>P\nW\ncold\n7\n1P\nM\n+\n了一'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 18}, page_content='Normalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\nP(W :\n==\nCM)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了P(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 19}, page_content='SELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNormalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nT W P\r\ncold sun 0.2\r\ncold rain 0.3\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\nP(W :\nT=\n=???\n三r\n c)M)d\n川\nr, T\nC\nP\nM\nT\n十\nP\nM\nT\nC\nS,\n川\n川\nC\n r,P(W\n C\n=\nP\nT\n C0.3\n0.6\n0.3\n0.2\n十P\nM\n+\n了P(W\nS, T\nC\nP\nM\nT\n+\nP\nM\nT\nC\nS,\n川\nC\nr.P(W\n川\nS, T\nC\nP\nT\nC0.2\n0.4\n0.3\n0.2\n十M\nD\nCM\nD\nC'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 20}, page_content='Normalization Trick\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nW P\r\nsun 0.4\r\nrain 0.6\r\nT W P\r\ncold sun 0.2\r\ncold rain 0.3\r\nSELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\r\n\uf0a7 Why does this work? Sum of selection is P(evidence)! (P(T=c), here)\nP\nM\n+\n了M\nD\nCM\nD\nCP(\n(1,2)\nP(\n(C1,C2)\nP(c1|∞2)\n=\nP(c2)\nZc1\nP(x1,2)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 21}, page_content='Quiz: Normalization Trick\r\nX Y P\r\n+x +y 0.2\r\n+x -y 0.3\r\n-x +y 0.4\r\n-x -y 0.1\r\nSELECT the joint \r\nprobabilities \r\nmatching the \r\nevidence\r\nNORMALIZE the \r\nselection\r\n(make it sum to one)\r\n\uf0a7 P(X | Y=-y) ?\nD'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 22}, page_content='\uf0a7 (Dictionary) To bring or restore to a normal condition\r\n\uf0a7 Procedure:\r\n\uf0a7 Step 1: Compute Z = sum over all entries\r\n\uf0a7 Step 2: Divide every entry by Z\r\n\uf0a7 Example 1\r\nTo Normalize\r\nAll entries sum to ONE\r\nW P\r\nsun 0.2\r\nrain 0.3 Z = 0.5\r\nW P\r\nsun 0.4\r\nrain 0.6\r\n\uf0a7 Example 2\r\nT W P\r\nhot sun 20\r\nhot rain 5\r\ncold sun 10\r\ncold rain 15\r\nNormalize\r\nZ = 50\r\nNormalize\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 23}, page_content='Probabilistic Inference\r\n\uf0a7 Probabilistic inference: compute a desired probability \r\nfrom other known probabilities (e.g. conditional from \r\njoint)\r\n\uf0a7 We generally compute conditional probabilities \r\n\uf0a7 P(airport on time | no reported accidents) = 0.90\r\n\uf0a7 These represent the agent’s beliefs given the evidence\r\n\uf0a7 Probabilities change with new evidence:\r\n\uf0a7 P(airport on time | no accidents, 5 a.m.) = 0.95\r\n\uf0a7 P(airport on time | no accidents, 5 a.m., raining) = 0.80\r\n\uf0a7 Observing new evidence causes beliefs to be updated\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 24}, page_content='Inference by Enumeration\r\n\uf0a7 General case:\r\n\uf0a7 Evidence variables: \r\n\uf0a7 Query* variable:\r\n\uf0a7 Hidden variables: All variables\r\n* Works fine with \r\nmultiple query \r\nvariables, too\r\n\uf0a7 We want:\r\n\uf0a7 Step 1: Select the \r\nentries consistent \r\nwith the evidence\r\n\uf0a7 Step 2: Sum out H to get joint \r\nof Query and evidence\r\n\uf0a7 Step 3: Normalize\n1\nN\nn\n7\n7E1\nEk\n:e1\nekH\nH\n7P\ne\n1\ne\nk\n√\n&D\ne\n1\ne\nk\n&\n7P(Q,1\nh1\nhr,\ne1\nek)\nh...hr1\nN\nn\n7\n7P(x)\n-3\n0.05\n-1\n0.25\n0.07\n1\n0.2\n5\n0.01\n2\n0.15Z\nP(Q,e\ne1\nek\nb1\nP(Qle1\nP(Q,6\nek\n-Z\ne1\nek'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 25}, page_content='28\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\nhot\nfog\nsummer\n0.01\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 26}, page_content='29\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nhot\nfog\nsummer\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\n0.05\ncold\nfog\nsummer\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 27}, page_content='30\nInference by Enumeration\nSeason\nTemp\nWeather\nhot\nsummer\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\nsummer\ncold\nrain\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\nwinter\nhot\nsun\n0.10\nMintei\nhot\nrain\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\nwinter\ncold\nfog\n0.18\nwjnter\ncald\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 28}, page_content='31\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nsummer\ncold\nrain\n0.05\nvariable(s)\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 29}, page_content='32\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nhot\nsummer\nsun\n0.35\nP(SIsun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 30}, page_content='33\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(S I sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nwinter\nhot\nsun\n0.10\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 31}, page_content='34\nInference by Enumeration\nSeason\nTemp\nWeather\nP\nsummer\nhot\nsun\n0.35\nP(SI sun)?\nsummer\nhot\nrain\n0.01\n1.Enumerateoptionswith\nsummer\nhot\nfog\n0.01\n0.45\nsun\nsummer\nhot\nmeteor\n0.00\nsummer\ncold\nsun\n0.10\n2.Sumoutirrelevant\nvariable(s)\nsummer\ncold\nrain\n0.05\nsummer\ncold\nfog\n0.09\n3.Normalize\nsummer\ncold\nmeteor\n0.00\nP(S|sun)=\nwinter\nhot\nsun\n0.10\n{summer:0.45/(0.45+0.25),\nwinter\nhot\nrain\n0.01\nwinter\nhot\nfog\n0.02\nwinter: 0.25/(0.45+0.25))\n0.25\nwinter\nhot\nmeteor\n0.00\nwinter\ncold\nsun\n0.15\nwinter\ncold\nrain\n0.20\nwinter\ncold\nfog\n0.18\nwinter\ncold\nmeteor\n0.00'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 32}, page_content='Inference by Enumeration\r\n\uf0a7 P(W)?\r\n\uf0a7 P(W | winter)?\r\n\uf0a7 P(W | winter, hot)?\r\nS T W P\r\nsummer hot sun 0.30\r\nsummer hot rain 0.05\r\nsummer cold sun 0.10\r\nsummer cold rain 0.05\r\nwinter hot sun 0.10\r\nwinter hot rain 0.05\r\nwinter cold sun 0.15\r\nwinter cold rain 0.20\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 33}, page_content='\uf0a7 Obvious problems:\r\n\uf0a7 Worst-case time complexity O(d\r\nn\r\n) \r\n\uf0a7 Space complexity O(d\r\nn\r\n) to store the joint distribution\r\nInference by Enumeration\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 34}, page_content='The Product Rule\r\n\uf0a7 Sometimes have conditional distributions but want the joint\nP(x,y)\nP(xly):\nP(\n(y)P(?\n(y)P(\nP(\ny)\nx,y\nI8\nX\n='), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 35}, page_content='The Product Rule\r\n\uf0a7 Example:\r\nR P\r\nsun 0.8\r\nrain 0.2\r\nD W P\r\nwet sun 0.1\r\ndry sun 0.9\r\nwet rain 0.7\r\ndry rain 0.3\r\nD W P\r\nwet sun 0.08\r\ndry sun 0.72\r\nwet rain 0.14\r\ndry rain 0.06\nP\nM\n?\n1D\nM\n了M\nP\n1P(?\n(y)P(\nP(\ny)\nx,y\nI'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 36}, page_content='The Chain Rule\r\n\uf0a7 More generally, can always write any joint distribution as an \r\nincremental product of conditional distributions\r\n\uf0a7 Why is this always true?\nP(x1,2,3) = P(x1)P(x2|x1)P(x3|1,2)P(x1,x2,...cn) =\n11\nP(xic...i-1)'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 37}, page_content='Bayes Rule\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 38}, page_content='Bayes’ Rule\r\n\uf0a7 Two ways to factor a joint distribution over two variables:\r\n\uf0a7 Dividing, we get:\r\n\uf0a7 Why is this at all helpful?\r\n\uf0a7 Lets us build one conditional from its reverse\r\n\uf0a7 Often one conditional is tricky but the other one is simple\r\n\uf0a7 Foundation of many systems we’ll see later (e.g. ASR, MT)\r\n\uf0a7 In the running for most important AI equation!\r\nThat’s my rule!\nP(c,y)\nP(c|y)P(y)P(y|∞)\nP(cly) :\n(P(c)\n=\nP\n(y)y'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 39}, page_content='Inference with Bayes’ Rule\r\n\uf0a7 Example: Diagnostic probability from causal probability:\r\n\uf0a7 Example:\r\n\uf0a7 M: meningitis, S: stiff neck\r\n\uf0a7 Note: posterior probability of meningitis still very small\r\n\uf0a7 Note: you should still get stiff necks checked out! Why?\nP(s I m) = 0.8\nExample\nP(m) = 0.0001\ngivens\nP(s) = 0.01\nP(s|m)P(m)\n0.8x0.0001\nP(m|s)=\nP(s)\n0.01'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 40}, page_content='Quiz: Bayes’ Rule\r\n\uf0a7 Given:\r\n\uf0a7 What is P(W | dry) ? \r\nR P\r\nsun 0.8\r\nrain 0.2\r\nD W P\r\nwet sun 0.1\r\ndry sun 0.9\r\nwet rain 0.7\r\ndry rain 0.3\nP\nM\n?\n1M\nP\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 41}, page_content='Ghostbusters, Revisited\r\n\uf0a7 Let’s say we have two distributions:\r\n\uf0a7 Prior distribution over ghost location: P(G)\r\n\uf0a7 Let’s say this is uniform\r\n\uf0a7 Sensor reading model: P(R | G)\r\n\uf0a7 Given: we know what our sensors do\r\n\uf0a7 R = reading color measured at (1,1)\r\n\uf0a7 E.g. P(R = yellow | G=(1,1)) = 0.1\r\n\uf0a7 We can calculate the posterior \r\ndistribution P(G|r) over ghost locations \r\ngiven a reading using Bayes’ rule:\r\n[Demo: Ghostbuster – with probability (L12D2) ]\n0.17\n0.10\n0.10\n0.09\n0.17\n0.10\n<0.01\n0.09\n0.170.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11\n0.11(46)d\n(6)d(6|)d x'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 1.pdf', 'page': 42}, page_content='Video of Demo Ghostbusters with Probability\n')]
[Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 0}, page_content='Bayes’ Nets\r\n[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.]\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 1}, page_content='Probabilistic Models\r\n\uf0a7 Models describe how (a portion of) the world works\r\n\uf0a7 Models are always simplifications\r\n\uf0a7 May not account for every variable\r\n\uf0a7 May not account for all interactions between variables\r\n\uf0a7 “All models are wrong; but some are useful.”\r\n– George E. P. Box\r\n\uf0a7 What do we do with probabilistic models?\r\n\uf0a7 We (or our agents) need to reason about unknown \r\nvariables, given evidence\r\n\uf0a7 Example: explanation (diagnostic reasoning)\r\n\uf0a7 Example: prediction (causal reasoning)\r\n\uf0a7 Example: value of information\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 2}, page_content='Independence\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 3}, page_content='\uf0a7 Two variables are independent if:\r\n\uf0a7 This says that their joint distribution factors into a product two \r\nsimpler distributions\r\n\uf0a7 Another form:\r\n\uf0a7 We write: \r\n\uf0a7 Independence is a simplifying modeling assumption\r\n\uf0a7 Empirical joint distributions: at best “close” to independent\r\n\uf0a7 What could we assume for {Weather, Traffic, Cavity, Toothache}?\r\nIndependence\ny : P(c,y)\nP(c)P(y)\nVc, y\n=h‘cA\nP(c)\nP(c)\ny)\n='), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 4}, page_content='Example: Independence?\r\nT W P\r\nhot sun 0.4\r\nhot rain 0.1\r\ncold sun 0.2\r\ncold rain 0.3\r\nT W P\r\nhot sun 0.3\r\nhot rain 0.2\r\ncold sun 0.3\r\ncold rain 0.2\r\nT P\r\nhot 0.5\r\ncold 0.5\r\nW P\r\nsun 0.6\r\nrain 0.4\nM\nD\n1\n1DM\nDP\nM\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 5}, page_content='Example: Independence\r\n\uf0a7 N fair, independent coin flips:\r\nH 0.5\r\nT 0.5\r\nH 0.5\r\nT 0.5\r\nH 0.5\r\nT 0.5\nP\n1PP(X1.\nX2,\nP\nXn.on0.411\n7'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 6}, page_content="Conditional Independence\r\n\uf0a7 P(Toothache, Cavity, Catch)\r\n\uf0a7 If I have a cavity, the probability that the probe catches in it \r\ndoesn't depend on whether I have a toothache:\r\n\uf0a7 P(+catch | +toothache, +cavity) = P(+catch | +cavity)\r\n\uf0a7 The same independence holds if I don’t have a cavity:\r\n\uf0a7 P(+catch | +toothache, -cavity) = P(+catch| -cavity)\r\n\uf0a7 Catch is conditionally independent of Toothache given Cavity:\r\n\uf0a7 P(Catch | Toothache, Cavity) = P(Catch | Cavity)\r\n\uf0a7 Equivalent statements:\r\n\uf0a7 P(Toothache | Catch , Cavity) = P(Toothache | Cavity)\r\n\uf0a7 P(Toothache, Catch | Cavity) = P(Toothache | Cavity) P(Catch | Cavity)\r\n\uf0a7 One can be derived from the other easily\n"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 7}, page_content='Conditional Independence\r\n\uf0a7 Unconditional (absolute) independence very rare (why?)\r\n\uf0a7 Conditional independence is our most basic and robust form \r\nof knowledge about uncertain environments.\r\n\uf0a7 X is conditionally independent of Y given Z\r\nif and only if:\r\nor, equivalently, if and only if\nVx, y,2 : P(x, y2)\nP(c|2)P(y2)\n川X\nYVx, y,z : P(xl\nP(c\n2, y)\nZ'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 8}, page_content='Conditional Independence\r\n\uf0a7 What about this domain:\r\n\uf0a7 Traffic\r\n\uf0a7 Umbrella\r\n\uf0a7 Raining\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 9}, page_content='Conditional Independence\r\n\uf0a7 What about this domain:\r\n\uf0a7 Fire\r\n\uf0a7 Smoke\r\n\uf0a7 Alarm\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 10}, page_content='Conditional Independence and the Chain Rule\r\n\uf0a7 Chain rule: \r\n\uf0a7 Trivial decomposition:\r\n\uf0a7 With assumption of conditional independence:\r\n\uf0a7 Bayes’nets / graphical models help us express conditional independence assumptions\nP(Rain)P(Traffic|Rain)P(Umbrella|Rain,\nTrafficP(Traffic,\nRain, UmbrellaP(Traffic,\nRain, UmbrellaP(Rain)P(Traffic|Rain)P(Umbrella\nRainP(X1, X2,... Xn) = P(X1)P(X2|X1)P(X3|X1, X2) ..'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 11}, page_content='Inference in Ghostbusters\r\n\uf0a7 A ghost is in the grid \r\nsomewhere\r\n\uf0a7 Sensor readings tell how \r\nclose a square is to the \r\nghost\r\n\uf0a7 On the ghost: red\r\n\uf0a7 1 or 2 away: orange\r\n\uf0a7 3 or 4 away: yellow\r\n\uf0a7 5+ away: green\n网'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 12}, page_content='Video of Demo Ghostbusters with Probability\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 13}, page_content='15\nGhostbusters model\nVariables and ranges:\n0.11\n0.11\n0.11\nG (ghost location) in {(1,1),..,(3,3)}\n0.11\n0.11\n0.11\nCx,y\n(color measured at square x,y) in\n{red,orange,yellow,green}\n0.11\n0.11\n0.11\nGhostbuster physics:\nUniform prior distribution over ghost location: P(G)\nSensor model: P(Cxy I G) (depends only on distance to G)\n■ E.g. P(C1,1 = yellow | G =(1,1)) = 0.1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 14}, page_content='16\nGhostbusters model, contd.\nP(G, C1,1, .. C3,3)\nhas 9 x 49 = 2,359,296 entries!!!\n0.11\n00\n0.11\nGhostbuster independence:\n0.11\n0.11\n0.11\nAre\ne C1,1 and\nC1,2 independent?\n0.11\n0.11\n0.11\nE.g., does P(C1,1 = yellow) = P(C1,1 = yellow I C1,2 = orange)\nGhostbuster physics again:\nP(Cxy I G) depends only on distance to G\n■ So P(C1,1 = yellow I G = (2,3)) = P(C1,1 = yellow I G = (2,3), C1,2 = orange)\n· I.e., C1,1 is conditionally independent of C1,2 given G '), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 15}, page_content='17\nGhostbusters model, contd.\nApply the chain rule to decompose the joint probability model:\nP(G, C1,1, . C3,3) = P(G) P(C1,1 I G) P(C1,2 I G, C1,1) P(C1,3 I G, C1,1, C1,2) ... P(C3,3 I G, C1,1 , C3,2)\nNow simplify using conditional independence:\nP(G, C1,1, . C3,3) = P(G) P(C1,1 I G) P(C1,2 I G) P(C1,3 I G) .. P(C3,3 I G)\nI.e., conditional independence properties of ghostbuster physics simplify the probability\nmodel from exponential to quadratic in the number of squares'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 16}, page_content='Bayes’Nets: Big Picture\nDISTRIBUTIONS\nIN N EASY STEPS!'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 17}, page_content='Bayes’ Nets: Big Picture\r\n\uf0a7 Two problems with using full joint distribution tables \r\nas our probabilistic models:\r\n\uf0a7 Unless there are only a few variables, the joint is WAY too \r\nbig to represent explicitly\r\n\uf0a7 Hard to learn (estimate) anything empirically about more \r\nthan a few variables at a time\r\n\uf0a7 Bayes’ nets: a technique for describing complex joint \r\ndistributions (models) using simple, local \r\ndistributions (conditional probabilities)\r\n\uf0a7 More properly called graphical models\r\n\uf0a7 We describe how variables locally interact\r\n\uf0a7 Local interactions chain together to give global, indirect \r\ninteractions\r\n\uf0a7 For about 10 min, we’ll be vague about how these \r\ninteractions are specified\n0.411\n7'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 18}, page_content='Graphical Model Notation\r\n\uf0a7 Nodes: variables (with domains)\r\n\uf0a7 Can be assigned (observed) or unassigned \r\n(unobserved)\r\n\uf0a7 Arcs: interactions\r\n\uf0a7 Similar to CSP constraints\r\n\uf0a7 Indicate “direct influence” between variables\r\n\uf0a7 Formally: encode conditional independence \r\n(more later)\r\n\uf0a7 For now: imagine that arrows mean \r\ndirect causation (in general, they don’t!)\nWeatherCavity\ner\nToothache\nCatch'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 19}, page_content='Example: Coin Flips\r\n\uf0a7 N independent coin flips\r\n\uf0a7 No interactions between variables: absolute independence\r\nX1 X2 Xn\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 20}, page_content="\nExample: Traffic\nVariables:\nT: There is traffic\nU: I'm holding my umbrella\nR: It rains\nR\nT\nU"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 21}, page_content='\nExample: Smoke alarm\nVariables:\nF\nF: There is fire\nS: There is smoke\nA: Alarm sounds\nS\nA'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 22}, page_content="\nExample: Ghostbusters\nVariables:\n0.11\n0.11\nG: The ghost's location\nC1,1,..\nC3,3 :\n0.11\n0.11\n0.11\nG\nThe observation at each location\n0.11\n0.11\n0.11\nWant to estimate:\nP( G | C1,1, .. C3,3 )\nC1,1\nC1,2\nC3,3\nThis is called a Naive Bayes model:\nOne discrete query variable (often called the class or category variable)\nAll other variables are (potentially) evidence variables\nEvidence variables are all conditionally independent given the query variable"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 23}, page_content="\nExample Bayes' Net: Car Insurance\nAge\nSocioEcon\nGoodStudent\nExtraCar\nRiskAversion\nMakeModel\nVehicleYear\nYearsLicensed\nMileage\nDrivingSkill\nAntiTheft\nSafetyFeatures\nCarValue\nGaraged\nAirbag\nDrivingRecord\nRuggedness\nDrivingBehavior\nCushioning\nTheft\nAccident\nOwnCarDamage\nOwnCarCost\nOtherCost\nMedicalCost\nLiabilityCost\nPropertyCost"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 24}, page_content="\nExample Bayes' Net: Car Won't Start\nalternator\nfanbelt\nbattery age\nbroken\nbroken\nbattery\nno charging\ndead\nbattery\nbattery\nfuel line\nstarter\nno oil\nflat\nno gas\nblocked\nbroken\nmeter\ncarwon't\nlights\noil light\ngasgauge\ndipstick\nstart"), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 25}, page_content='Bayes’ Net Semantics\nBuild Your\nOwn\nBnyes Net'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 26}, page_content='\nBayes Net Syntax\nA set of nodes, one per variable X;\nP(G)\n(1,1)\n(1,2)\nG\n(1,3)\nA directed, acyclic graph\n0.11\n0.11\n0.11\nA conditional distribution for each node\ngiven its parent variables in the graph\nC3,3)\nCPT (conditional probability table); each row is a\nP(C1,1 I G)\ndistribution for child given values of its parents\ng\n(1,1)\n0.01\n0.1\n0.3\n0.59\n(1,2)\n0.1\n0.3\n0.5\n0.1\n(1,3)\n0.3\n0.5\n0.19\n0.01\nBayes net = Topology (graph) + Local Conditional Probabilities'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 27}, page_content='Example: Alarm Network\r\n\uf0a7 Variables\r\n\uf0a7 B: Burglary\r\n\uf0a7 E: Earthquake\r\n\uf0a7 A: Alarm goes off\r\n\uf0a7 M: Mary calls\r\n\uf0a7 J: John calls\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 28}, page_content='\nExample: Alarm Network\nP(E)\nP(B)\ntrue\nfalse\ntrue\nfalse\nBurglary\nEarthquake\n0.002\n0.998\n0.001\n0.999\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\nAlarm\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nNumber of free parameters\nfalse\nfalse\n0.001\n0.999\nin each CPT:\nJohn\nMary\ncalls\ncalls\nParent range sizes d1,...,dk\nA\nP(JIA)\nA\nP(M|A)\ntrue\nfalse\ntrue\nfalse\nChild range size d\ntrue\n0.9\n0.1\ntrue\n0.7\n0.3\nEach table row must sum to 1\nfalse\n0.05\n0.95\nfalse\n0.01\n0.99\n(d-1) II; d;'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 29}, page_content='\nBayes net global semantics\nBayes nets encode joint distributions as product of\nconditional distributions on each variable:\nP(X1,..,Xn) = II; P(X; I Parents(X;))\nExploits sparse structure: number of parents is\nusuallysmall'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 30}, page_content='33\nSize of a Bayes Net\nHow big is a joint distribution over N\nBoth give you the power to calculate\nvariables, each with d values?\nP(X1, X2, .., XN)\nQN\nBayes Nets: huge space savings with sparsity!\nHow big is an N-node net if nodes\nAlso easier to elicit local CPTs\nhave at most k parents?\nAlso faster to answer queries (coming)\nO(N * dk)\nidwod'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 31}, page_content='34\nExample\nP(E)\nP(b,-e, a, -j, -m) =\nP(B)\ntrue\nfalse\ntrue\nfalse\nBurglary\nEarthquake\nP(b) P(-e) P(a|b,-e) P(-jla) P(-m|a)\n0.002\n0.998\n0.001\n0.999\n=.001x.998x.94x.1x.3=.000028\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\nAlarm\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nfalse\nfalse\n0.001\n0.999\nJohn\nMary\ncalls\ncalls\nA\nP(J|A)\nA\nP(M|A)\ntrue\nfalse\ntrue\nfalse\ntrue\n0.9\n0.1\ntrue\n0.7\n0.3\nfalse\n0.05\n0.95\nfalse\n0.01\n0.99\n32'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 32}, page_content='35\nConditional independence in BNs\nCompare the Bayes net global semantics\nP(X1...,Xn) = II; P(X; I Parents(X;))\nwith the chain rule identity\n(T-!x"x I \'X)d !II = (\'x"Tx)d\nAssume (without loss of generality) that X1...,X, sorted in topological order according to\nthe graph (i.e., parents before children), so Parents(X;) ≤ X1,..,Xi-1\nSo the Bayes net asserts conditional independences P(X, I X1,..,Xi-1) = P(X, I Parents(X;))\nTo ensure these are valid, choose parents for node X, that "shield" it from other predecessors'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 33}, page_content='\nConditional independence semantics\nEvery variable is conditionally independent of its non-descendants given its parents\nConditional independence semantics <=> global semantics\nn\nUm\nX\nZj\nY\n34'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 34}, page_content='\nExample: Burglary\nP(B)\nP(E)\ntrue\nfalse\nBurglary\ntrue\nfalse\n0.001\n0.999\n0.002\n0.998\nEarthquake\nBurglary\nEarthquake\nAlarm\nAlarm\nB\nE\nP(A|B,E)\ntrue\nfalse\ntrue\ntrue\n0.95\n0.05\ntrue\nfalse\n0.94\n0.06\nfalse\ntrue\n0.29\n0.71\nfalse\nfalse\n0.001\n0.999\n35'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 35}, page_content='38\nExample: Burglary\nP(A)\nAlarm\ntrue\nfalse\nAlarm\nBurglary\nEarthquake\nB\nP(E|A,B)\nA\nP(B|A)\ntrue\nfalse\nBurglary\nEarthquake\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\n36'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 36}, page_content='Example: Traffic\r\nR\r\nT\r\n+r 1/4\r\n-r 3/4\r\n+r +t 3/4\r\n-t 1/4\r\n-r +t 1/2\r\n-t 1/2\nRRD\nt\n-r\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 37}, page_content='Example: Traffic\r\n\uf0a7 Causal direction\r\nR\r\nT\r\n+r 1/4\r\n-r 3/4\r\n+r +t 3/4\r\n-t 1/4\r\n-r +t 1/2\r\n-t 1/2\r\n+r +t 3/16\r\n+r -t 1/16\r\n-r +t 6/16\r\n-r -t 6/16\nRR1\n1\nR'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 38}, page_content='Example: Reverse Traffic\r\n\uf0a7 Reverse causality?\r\nT\r\nR\r\n+t 9/16\r\n-t 7/16\r\n+t +r 1/3\r\n-r 2/3\r\n-t +r 1/7\r\n-r 6/7\r\n+r +t 3/16\r\n+r -t 1/16\r\n-r +t 6/16\r\n-r -t 6/16\n1\n1\nRR\nT\n1'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 39}, page_content='Causality?\r\n\uf0a7 When Bayes’ nets reflect the true causal patterns:\r\n\uf0a7 Often simpler (nodes have fewer parents)\r\n\uf0a7 Often easier to think about\r\n\uf0a7 Often easier to elicit from experts\r\n\uf0a7 BNs need not actually be causal\r\n\uf0a7 Sometimes no causal net exists over the domain \r\n(especially if variables are missing)\r\n\uf0a7 E.g. consider the variables Traffic and Drips\r\n\uf0a7 End up with arrows that reflect correlation, not causation\r\n\uf0a7 What do the arrows really mean?\r\n\uf0a7 Topology may happen to encode causal structure\r\n\uf0a7 Topology really encodes conditional independence\n?P(cc1.,...i-1)\n= P(c;parents(Xi))'), Document(metadata={'source': 'Data\\AI-Lecture Note 6 Slides Version 2.pdf', 'page': 40}, page_content='43\nSummary\nIndependence and conditional independence are\nimportant forms of probabilistic knowledge\nBayes net encode joint distributions efficiently by\ntaking advantage of conditional independence\nGlobal joint probability = product of local conditionals\nAllows for flexible tradeoff between model accuracy\nand memory/compute efficiency\nA\nA\nB\nB\nA\nB\nB\nE\nE\nE\nStrict Independence\nNaiveBayes\nSparse Bayes Net\nJoint Distribution')]
[Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 0}, page_content='CS 188 Introduction to Artificial Intelligence\r\nFall 2018 Note 6\r\nThese lecture notes are heavily based on notes originally written by Josh Hug and Jacky Liang.\r\nProbabilistic Inference\r\nIn artificial intelligence, we often want to model the relationships between various nondeterministic events.\r\nIf the weather predicts a 40% chance of rain, should I carry my umbrella? How many scoops of ice cream\r\nshould I get if the more scoops I get, the more likely I am to drop it all? If there was an accident 15 minutes\r\nago on the freeway on my route to Oracle Arena to watch the Warriors’ game, should I leave now or in 30\r\nminutes? All of these questions (and innumerable more) can be answered with probabilistic inference.\r\nWe’re assuming that you’ve learned the foundations of probability in CS70, so these notes will not review\r\nbasic concepts of probability like PDFs, conditional probabilities, independence, and conditional indepen\x02dence.\r\nIn previous sections of this class, we modeled the world as existing in a specific state that is always known.\r\nFor the next several weeks, we will instead use a new model where each possible state for the world has\r\nits own probability. For example, we might build a weather model, where the state consists of the season,\r\ntemperature and weather. Our model might say that P(winter, 35, cloudy) = 0.023. This number represents\r\nthe probability of the specific outcome that it is winter, 35, and cloudy.\r\nMore precisely, our model is a joint distribution, i.e. a table of probabilities which captures the likelihood\r\nof each possible outcome, also known as an assignment. As an example, consider the table below:\r\nSeason Temperature Weather Probability\r\nsummer hot sun 0.30\r\nsummer hot rain 0.05\r\nsummer cold sun 0.10\r\nsummer cold rain 0.05\r\nwinter hot sun 0.10\r\nwinter hot rain 0.05\r\nwinter cold sun 0.15\r\nwinter cold rain 0.20\r\nThis model allows us to answer questions that might be of interest to us, for example:\r\n• What is the probability that it is sunny? P(W = sun)\r\n• What is the probability distribution for the weather, given that we know it is winter? P(W | S = winter)\r\n• What is the probability that it is winter, given that we know it is rainy and cold? P(S = winter | T =\r\ncold,W = rain)\r\n• What is the probability distribution for the weather and season give that we know that it is cold?\r\nP(S,W | T = cold)\r\nCS 188, Fall 2018, Note 6 1\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 1}, page_content='Given a joint PDF, we can trivially perform compute any desired probablity distribution P(Q1 ...Qk | e1 ...ek)\r\nusing a simple and intuitive procedure known as inference by enumeration, for which we define three types\r\nof variables we will be dealing with:\r\n1. Query variables Qi, which are unknown and appear on the left side of the probability distribution we\r\nare trying to compute.\r\n2. Evidence variables ei, which are observed variables whose values are known and appear on the right\r\nside of the probability distribution we are trying to compute.\r\n3. Hidden variables, which are values present in the overall joint distribution but not in the distribution\r\nwe are currently trying to compute.\r\nIn this procedure, we collect all the rows consistent with the observed evidence variables, sum out all the\r\nhidden variables, and finally normalize the table so that it is a probability distribution (i.e. values sum to 1).\r\nFor example, if we wanted to compute P(W | S = winter), we’d select the four rows where S is winter, then\r\nsum out over T and normalize. This yields the following probability table:\r\nW S Unnormalized Sum Probability\r\nsun winter 0.10+0.15 = 0.25 0.25/(0.25+0.25) = 0.5\r\nrain winter 0.05+0.20 = 0.25 0.25/(0.25+0.25) = 0.5\r\nHence P(W = sun | S = winter) = 0.5 and P(W = rain | S = winter) = 0.5, and we learn that in winter\r\nthere’s a 50% chance of sun and a 50% chance of rain (classic California weather).\r\nSo long as we have the joint PDF table, inference by enumeration (IBE) can be used to compute any desired\r\nprobablity distribution, even for multiple query variables Q1...Qk.\r\nBayes Nets (Representation)\r\nWhile inference by enumeration can compute probabilities for any query we might desire, representing an\r\nentire joint distribution in the memory of a computer is impractical for real problems - if each of n variables\r\nwe wish to represent can take on d possible values (it has a domain of size d), then our joint distribution\r\ntable will have dn entries, exponential in the number of variables and quite impractical to store!\r\nBayes nets avoid this issue by taking advantage of the idea of conditional probability. Rather than storing\r\ninformation in a giant table, probabilities are instead distributed across a large number of smaller local\r\nprobability tables along with a directed acyclic graph (DAG) which captures the relationships between\r\nvariables. The local probability tables and the DAG together encode enough information to compute any\r\nprobability distribution that we could have otherwise computed given the entire joint distribution.\r\nSpecifically, each node in the graph represents a single random variable and each directed edge represents\r\none of the conditional probability distributions we choose to store (i.e. an edge from node A to node B\r\nindicates that we store the probability table for P(B|A)). Each node is conditionally independent of all its\r\nancestor nodes in the graph, given all of its parents. Thus, if we have a node representing variable X, we\r\nstore P(X|A1,A2,...,AN), where A1,...,AN are the parents of X.\r\nAs an example of a Bayes Net, consider a model where we have five binary random variables described\r\nbelow:\r\nCS 188, Fall 2018, Note 6 2\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 2}, page_content='• B: Burglary occurs.\r\n• A: Alarm goes off.\r\n• E: Earthquake occurs.\r\n• J: John calls.\r\n• M: Mary calls.\r\nAssume the alarm can go off if either a burglary or an earthquake occurs, and that Mary and John will call\r\nif they hear the alarm. We can represent these dependencies with the graph shown below.\r\nAs a reality check, it’s important to internalize that Bayes Nets are only a type of model. Models attempt\r\nto capture the way the world works, but because they are always a simplification they are always wrong.\r\nHowever, with good modeling choices they can still be good enough approximations that they are useful for\r\nsolving real problems in the real world. In general, they will not account for every variable or even every\r\ninteraction between variables.\r\nReturning to our discussion, we formally define a Bayes Net as consisting of:\r\n• A directed acyclic graph of nodes, one per variable X.\r\n• A conditional distribution for each node P(X|A1 ...An), where Ai is the i\r\nth parent of X, stored as a\r\nconditional probability table or CPT. Each CPT has n+2 columns: one for the values of each of the\r\nn parent variables A1 ...An, one for the values of X, and one for the conditional probability of X.\r\nIn the alarm model above, we would store probability tables P(B),P(E),P(A | B,E),P(J | A) and P(M | A).\r\nGiven all of the CPTs for a graph, we can calculate the probability of a given assignment using the chain\r\nrule: P(X1,X2,...,Xn) = ’n\r\ni=1 P(Xi|parents(Xi)).\r\nFor the alarm model above, we might calculate the probability of one event as follows: P(b,e,+a,+j,m) =\r\nP(b)·P(e)·P(+a|b,e)·P(+j|+a)·P(m|+a).\r\nCS 188, Fall 2018, Note 6 3\nB\nE\nA\nT\nM'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 3}, page_content='This works because of the conditional independence relationships given by the graph. Specifically, we rely\r\non the fact that P(xi|x1,..., xi1) = P(xi|parents(Xi)). Or in other words, that the probability of a specific\r\nvalue of Xi depends only on the values assigned to Xi’s parents.\r\nBayes Nets (Inference)\r\nInference is the process of calculating the joint PDF for some set of query variables based on some set\r\nof observed variables. We can solve this problem naively by forming the joint PDF and using inference by\r\nenumeration as described above. This requires the creation of and iteration over an exponentially large table.\r\nAn alternate approach is to eliminate variables one by one. To eliminate a variable X, we:\r\n1. Join (multiply together) all factors involving X.\r\n2. Sum out X.\r\nA factor is defined simply as an unnormalized probability. At all points during variable elimination, each\r\nfactor will be proportional to the probability it corresponds to but the underlying distribution for each factor\r\nwon’t necessarily sum to 1 as a probability distribution should.\r\nLet’s make these ideas more concrete with an example. Suppose we have a model as shown below, where\r\nT, C, S, and E can take on binary values, as shown below. Here, T represents the chance that an adventurer\r\ntakes a treasure, C represents the chance that a cage falls on the adventurer given that he takes the treasure,\r\nS represents the chance that snakes are released if an adventurer takes the treasure, and E represents the\r\nchance that the adventurer escapes given information about the status of the cage and snakes.\r\nIn this case, we have the factors P(T), P(C|T), P(S|T), and P(E|C,S). Suppose we want to calculate\r\nP(T| + e). The inference by enumeration approach would be to form the 16 row joint PDF P(T,C,S,E),\r\nselect only the rows corresponding to +e, then summing out C and S and finally normalizing.\r\nThe alternate approach is to eliminate C, then S, one variable at a time. We’d proceed as follows:\r\n• Join (multiply) all the factors involving C, forming P(C,+e|T,S) = P(C|T)·P(+e|C,S).\r\n• Sum out C from this new factor, leaving us with a new factor P(+e|T,S).\r\n• Join all factors involving S, forming P(+e,S|T) = P(S|T)·P(+e|T,S).\r\nCS 188, Fall 2018, Note 6 4\nT\nC\nS\nE'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 4}, page_content='• Sum out S, yielding P(+e|T).\r\nOnce we have P(+e|T), we can easily compute P(T|+e).\r\nWhile this process is more involved from a conceptual point of view, the maximum size of any factor\r\ngenerated is only 8 rows instead of 16 as it would be if we formed the entire joint PDF.\r\nAn alternate way of looking at the problem is to observe that the calculation of P(+e,T) can either be done,\r\nas it is in inference by enumeration, as follows:\r\nÂsÂc\r\nP(T)P(s|T)P(c|T)P(+e|c,s)\r\nVariable elimination is equivalent to calculating P(+e,T) as follows:\r\nP(T)ÂsP(s|T)ÂcP(c|T)P(+e|c,s)\r\nBayes Nets (Sampling)\r\nAn alternate approach for probabilistic reasoning is to implicitly calculate the probabilities for our query by\r\nsimply counting samples.\r\nFor example, suppose we wanted to calculate P(T| + e). If we had a magic machine that could generate\r\nsamples from our distribution, we could collect all samples for which the adventurer escapes the maze, and\r\nthen compute the fraction of those escapes for which the adventurer also took the treasure. Put differently,\r\nif we could run simulations of say, a few million adventurers, we’d easily be able to compute any inference\r\nwe’d want just by looking at the samples.\r\nGiven a Bayes Net model, we can easily write a simulator. For example, consider the CPTs given below for\r\nthe simplified model with only two variables T and C.\r\nCS 188, Fall 2018, Note 6 5\nT\nP(T)\n+t\n0.99\nT\n-t\n0.01\nT\nC\nP(C|T)\n+t\n+C\n0.95\n+t\n-C\n0.05\n-t\n+C\n0.0\n-t\n-C\n1.0'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 5}, page_content='A simple simulator in Python would be as follows:\r\nimport random\r\ndef get_t():\r\nif random.random() < 0.99:\r\nreturn True\r\nreturn False\r\ndef get_c(t):\r\nif t and random.random() < 0.95:\r\nreturn True\r\nreturn False\r\ndef get_sample():\r\nt = get_t()\r\nc = get_c(t)\r\nreturn [t, c]\r\nWe call this simple approach prior sampling. The downside of this approach is that it may require the\r\ngeneration of a very large number of samples in order to perform analysis of unlikely scenarios. If we\r\nwanted to compute P(C|t), we’d have to throw away 99% of our samples.\r\nOne way to mitigate this this problem, we can modify our procedure to early reject any sample inconsistent\r\nwith our evidence. For example, for the query P(C|t), we’d avoid generating a value for C unless t is true.\r\nThis still means we have to throw away most of our samples, but at least the bad samples we generate take\r\nless time to create. We call this approach rejection sampling.\r\nThese two approaches work for the same reason, which is that any valid sample occurs with the same\r\nprobability as specified in the joint PDF. In other words, the probability of every sample is based on the\r\nproduct of every CPT, or as I personally call it, the "every CPT participates principle".\r\nA more exotic approach is likelihood weighting, which ensures that we never generate a bad sample. In\r\nthis approach, we manually set all variables equal to the evidence in our query. For example, if we wanted\r\nto compute P(C|t), we’d simply declare that t is false. The problem here is that this may yield samples\r\nthat are inconsistent with the correct distribution. As an example, consider the more complex four variable\r\nmodel for T, C, S, and E given earlier in these notes. If we wanted to compute P(T,S,+c,+e), and simply\r\npicked values for T and S without taking into account the fact that c = false, and e = true, then there’s no\r\nguarantee that our samples actually obey the joint PDF given by the Bayes Net. For example, if the cage\r\nonly ever falls if the treasure is taken, then we’d want to ensure that T is always true instead of using the\r\nP(T) distribution given in the Bayes Net.\r\nPut differently, if we simply force some variables equal to the evidence, then our samples occur with proba\x02bility given only equal to the products of the CPTs of the non-evidence variables. This means the joint PDF\r\nhas no guarantee of being correct (though may be for some cases like our two variable Bayes Net). Instead,\r\nif we have sampled variables Z1 through Zp and fixed evidence variables E1 through Em a sample is given\r\nby the probability P(Z1...Zp,E1...Em) = ’p\r\ni P(Zi)|Parents(Zi). What is missing is that the probability of a\r\nsample does not include all the probabilities of P(Ei|Parents(Ei)), i.e. not every CPT participates.\r\nLikelihood weighting solves this issue by using a weight for each sample, which is the probability of the\r\nevidence variables given the sampled variables. That is, instead of counting all samples equally, we can\r\nCS 188, Fall 2018, Note 6 6\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 6}, page_content='define a weight wj for sample j that reflects how likely the observed values for the evidence variables are,\r\ngiven the sampled values. In this way, we ensure that every CPT participates. To do this, we iterate through\r\neach variable in the Bayes net, as we do for normal sampling), sampling a value if the variable is not an\r\nevidence variable, or changing the weight for the sample if the variable is evidence.\r\nFor example, suppose we want to calculate P(T|+c,+e). For the jth sample, we’d perform the following\r\nalgorithm:\r\n• Set wj to 1.0, and c = true and e = true.\r\n• For T: This is not an evidence variable, so we sample tj from P(T).\r\n• For C: This is an evidence variable, so we multiply the weight of the sample by P(+c|tj), i.e. wj =\r\nwj ·P(+c|tj).\r\n• For S: sample sj from P(S | tj).\r\n• For E: multiply the weight of the sample by P(+e|+c,sj), i.e. wj = wj ·P(+e|+c,sj).\r\nThen when we perform the usual counting process, we weight sample j by wj instead of 1, where 0 <=\r\nwj <= 1. This approach works because in the final calculations for the probabilities, the weights effectively\r\nserve to replace the missing CPTs. In effect, we ensure that the weighted probability of each sample is given\r\nby P(z1...zp, e1...em)=[’p\r\ni P(zi | Parents(zi))] · [’m\r\ni P(ei) | Parents(ei))].\r\nFor all three of our sampling methods (prior sampling, rejection sampling, and likelihod weighting), we\r\ncan get increasing amounts of accuracy by generating additional samples. However, of the three, likelihood\r\nweighting is the most computationally efficient, for reasons beyond the scope of this course.\r\nGibbs Sampling is a fourth approach for sampling. In this approach, we first set all variables to some totally\r\nrandom value (not taking into account any CPTs). We then repeatedly pick one variable at a time, clear its\r\nvalue, and resample it given the values currently assigned to all other variables.\r\nFor the T,C,S,E example above, we might assign t = true, c = true, s = false, and e = true. We then pick\r\none of our four variables to resample, say S, and clear it. We then pick a new variable from the distribution\r\nP(S| +t,+c,+e). This requires us knowing this conditional distribution. It turns out that we can easily\r\ncompute the distribution of any single variable given all other variables. More specifically, P(S|T,C,E) can\r\nbe calculated only using the CPTs that connect S with its neighbors. Thus, in a typical Bayes Net, where\r\nmost variables have only a small number of neighbors, we can precompute the conditional distributions for\r\neach variable given all of its neighbors in linear time.\r\nWe will not prove this, but if we repeat this process enough times, our later samples will eventually converge\r\nto the correct distribution even though we may start from a low-probability assignment of values. If you’re\r\ncurious, there are some caveats beyond the scope of the course that you can read about under the Failure\r\nModes section of the Wikipedia article for Gibbs Sampling.\r\nBayes Nets (D-Separation)\r\nOne useful question to ask about a set of random variables is whether or not one variable is independent from\r\nanother, or if one random variable is conditionally independent of another given a third random variable.\r\nBayes’ Nets representation of joint probability distributions gives us a way to quickly answer such questions\r\nby inspecting the topological structure of the graph.\r\nCS 188, Fall 2018, Note 6 7\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 7}, page_content='We already mentioned that a node is conditionally independent of all its ancestor nodes in the graph\r\ngiven all of its parents.\r\nWe will present all three canonical cases of connected three-node two-edge Bayes’ Nets, or triples, and the\r\nconditional independence relationships they express.\r\nCausal Chains\r\nFigure 1: Causal Chain with no observations. Figure 2: Causal Chain with Y observed.\r\nFigure 1 is a configuration of three nodes known as a causal chain. It expresses the following representation\r\nof the joint distribution over X, Y, and Z:\r\nP(x, y,z) = P(z|y)P(y|x)P(x)\r\nIt’s important to note that X and Z are not guaranteed to be independent, as shown by the following coun\x02terexample:\r\nP(y|x) = (\r\n1 if x = y\r\n0 else\r\nP(z|y) = (\r\n1 if z = y\r\n0 else\r\nIn this case, P(z|x) = 1 if x = z and 0 otherwise, so X and Z are not independent.\r\nHowever, we can make the statement that X ?? Z | Y, as in Figure 2. Recall that this conditional indepdence\r\nmeans:\r\nP(X|Z,Y) = P(X|Y)\r\nWe can prove this statement as follows:\r\nP(X|Z, y) = P(X,Z, y)\r\nP(Z, y) = P(Z|y)P(y|X)P(X)\r\nÂx P(X, y,Z) = P(Z|y)P(y|X)P(X)\r\nP(Z|y)Âx P(y|x)P(x)\r\n= P(y|X)P(X)\r\nÂx P(y|x)P(x) = P(y|X)P(X)\r\nP(y) = P(X|y)\r\nAn analogous proof can be used to show the same thing for the case where X has multiple parents. To\r\nsummarize, in the causal chain chain configuration, X ?? Z | Y.\r\nCommon Cause\r\nAnother possible configuration for a triple is the common cause. It expresses the following representation:\r\nP(x, y,z) = P(x|y)P(z|y)P(y)\r\nJust like with causal chain, we can show that X is not guaranteed to be independent of Z with the following\r\ncounterexample distribution:\r\nCS 188, Fall 2018, Note 6 8\nX\nY\nZX\nY\nZ'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 8}, page_content='Figure 3: Common Cause with no observations. Figure 4: Common Cause with Y observed.\r\nP(x|y) = (\r\n1 if x = y\r\n0 else\r\nP(z|y) = (\r\n1 if z = y\r\n0 else\r\nThen P(x|z) = 1 if x = z and 0 otherwise, so X and Z are not independent.\r\nBut it is true that X ?? Z | Y. That is, X and Z are independent if Y is observed as in Figure 4. We can show\r\nthis as follows:\r\nP(X|Z, y) = P(X,Z, y)\r\nP(Z, y) = P(X|y)P(Z|y)P(y)P(Z|y)P(y) = P(X|y)\r\nCommon E↵ect\r\nThe final possible configuration for a triple is the common effect, as shown in the figures below.\r\nFigure 5: Common Effect with no observations. Figure 6: Common Effect with Y observed.\r\nIt expresses the representation:\r\nP(x, y,z) = P(y|x,z)P(x)P(z)\r\nIn the configuration shown in Figure 5, X and Z are independent: X ?? Z. However, they are not necessarily\r\nindependent when conditioned on Y (Figure 6). As an example, suppose all three are binary variables. X\r\nCS 188, Fall 2018, Note 6 9\nY\nX\nZY\nX\nZX\nZ\nYX\nZ\nY'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 9}, page_content='and Z are true and false with equal probability:\r\nP(X = true) = P(X = f alse) = 0.5\r\nP(Z = true) = P(Z = f alse) = 0.5\r\nand Y is determined by whether X and Z have the same value:\r\nP(Y|X,Z) =\r\n8\r\n><\r\n>:\r\n1 if X = Z and Y = true\r\n1 if X 6= Z and Y = f alse\r\n0 else\r\nThen X and Z are independent if Y is unobserved. But if Y is observed, then knowing X will tell us the value\r\nof Z, and vice-versa. So X and Z are not conditionally independent given Y.\r\nCommon Effect can be viewed as “opposite” to Causal Chains and Common Cause – X and Z are guaranteed\r\nto be independent if Y is not conditioned on. But when conditioned on Y, X and Z may be dependent\r\ndepending on the specific probability values for P(Y | X,Z)).\r\nThis same logic applies when conditioning on descendents of Y in the graph. If one of Y’s descendent nodes\r\nis observed, as in Figure 7, X and Z are not guaranteed to be independent.\r\nFigure 7: Common Effect with child observations.\r\nGeneral Case, and D-separation\r\nWe can use the previous three cases as building blocks to help us answer conditional independence questions\r\non an arbitrary Bayes’ Net with more than three nodes and two edges. We formulate the problem as follows:\r\nGiven a Bayes Net G, two nodes X and Y, and a (possibly empty) set of nodes {Z1,...Zk} that represent\r\nobserved variables, must the following statement be true: X ?? Y|{Z1,... Zk}?\r\nD-separation (directed separation) is a property of the structure of the Bayes Net graph that implies this\r\nconditional independence relationship, and generalizes the cases we’ve seen above. If a set of variables\r\nZ1,···Zk d-separates X and Y, then X ?? Y | {Z1,···Zk} in all possible distributions that can be encoded by\r\nthe Bayes net.\r\nCS 188, Fall 2018, Note 6 10\nX\nZ\nY\nW'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 10}, page_content='We start with an algorithm that is based on a notion of reachability from node X to node Y. (Note: this\r\nalgorithm is not quite correct! We’ll see how to fix it in a moment.)\r\n1. Shade all observed nodes {Z1,...Zk} in the graph.\r\n2. If there exists an undirected path from X and Y that is not blocked by a shaded node, X and Y are\r\n“connected”.\r\n3. If X and Y are connected, they’re not conditionally independent given {Z1,...Zk}. Otherwise, they\r\nare.\r\nHowever, this algorithm only works if the Bayes’ Net has no Common Effect structure within the graph, be\x02cause if it exists, then two nodes are “reachable” when the Y node in Common Effect is activated (observed).\r\nTo adjust for this, we arrive at the following d-separation algorithm:\r\n1. Shade all observed nodes {Z1,...,Zk} in the graph.\r\n2. Enumerate all undirected paths from X to Y.\r\n3. For each path:\r\n(a) Decompose the path into triples (segments of 3 nodes).\r\n(b) If all triples are active, this path is active and d-connects X to Y.\r\n4. If no path d-connects X and Y, then X and Y are d-separated, so they are conditionally independent\r\ngiven {Z1,...,Zk}\r\nAny path in a graph from X to Y can be decomposed into a set of 3 consecutive nodes and 2 edges - each\r\nof which is called a triple. A triple is active or inactive depending on whether or not the middle node is\r\nobserved. If all triples in a path are active, then the path is active and d-connects X to Y, meaning X is\r\nnot guaranteed to be conditionally independent of Y given the observed nodes. If all paths from X to Y are\r\ninactive, then X and Y are conditionally independent given the observed nodes.\r\nActive triples: We can enumerate all possibilities of active and inactive triples using the three canonical\r\ngraphs we presented above in Figure 8 and 9.\r\nCS 188, Fall 2018, Note 6 11\n'), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 11}, page_content="Figure 8: Active triples Figure 9: Inactive triples\r\nExamples\r\nHere are some examples of applying the d-separation algorithm:\r\nThis graph contains the common effect and causual\r\nchain canonical graphs.\r\na) R ?? B – Guaranteed\r\nb) R ?? B | T – Not guaranteed\r\nc) R ?? B | T0 – Not guaranteed\r\nd) R ?? T0 | T – Guaranteed\r\nCS 188, Fall 2018, Note 6 12\n0008\nC\nOO\n00\nO\nOQ\nOQR\nB\nT\nT'"), Document(metadata={'source': 'Data\\AI-Lecture Note 6.pdf', 'page': 12}, page_content="This graph contains combinations of all three canon\x02ical graphs (can you list them all?).\r\na) L ?? T0 | T – Guaranteed\r\nb) L ?? B – Guaranteed\r\nc) L ?? B | T – Not guaranteed\r\nd) L ?? B | T0 – Not guaranteed\r\ne) L ?? B | T,R – Guaranteed\r\nThis graph contains combinations of all three canon\x02ical graphs.\r\na) T ?? D – Not guaranteed\r\nb) T ?? D | R – Guaranteed\r\nc) T ?? D | R,S – Not guaranteed\r\nConclusion\r\nTo summarize, Bayes’ Nets is a powerful representation of joint probability distributions. Its topological\r\nstructure encodes independence and conditional independence relationships, and we can use it to model\r\narbitrary distributions to perform inference and sampling.\r\nCS 188, Fall 2018, Note 6 13\nL\nR\nB\nD\nT\nT'R\nT\nD\nS")]
