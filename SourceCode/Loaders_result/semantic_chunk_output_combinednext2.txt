Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 1
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Agents
In artificial intelligence, the central problem at hand is that of the creation of a rational agent, an entity that
has goals or preferences and tries to perform a series of actions that yield the best/optimal expected outcome
given these goals. Rational agents exist in an environment, which is specific to the given instantiation of
the agent. As a very simple example, the environment for a checkers agent is the virtual checkers board on
which it plays against opponents, where piece moves are actions. Together, an environment and the agents
that reside within it create a world. A reflex agent is one that doesn’t think about the consequences of its actions, but rather selects an action
based solely on the current state of the world. These agents are typically outperformed by planning agents,
which maintain a model of the world and use this model to simulate performing various actions.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: Then, the
agent can determine hypothesized consequences of the actions and can select the best one. This is simulated
"intelligence" in the sense that it’s exactly what humans do when trying to determine the best possible move
in any situation - thinking ahead. State Spaces and Search Problems
In order to create a rational planning agent, we need a way to mathematically express the given environment
in which the agent will exist. To do this, we must formally express a search problem - given our agent’s
current state (its configuration within its environment), how can we arrive at a new state that satisfies its
goals in the best possible way? Formulating such a problem requires four things:
• A state space - The set of all possible states that are possible in your given world
• A successor function - A function that takes in a state and an action and computes the cost of performing that action as well as the successor state, the state the world would be in if the given agent
performed that action
• A start state - The state in which an agent exists initially
• A goal test - A function that takes a state as input, and determines whether it is a goal state
Fundamentally, a search problem is solved by first considering the start state, then exploring the state space
using the successor function, iteratively computing successors of various states until we arrive at a goal state,
at which point we will have determined a path from the start state to the goal state (typically called a plan). The order in which states are considered is determined using a predetermined strategy. We’ll cover types
of strategies and their usefulness shortly. Before we continue with how to solve search problems, it’s important to note the difference between a world
state, and a search state. A world state contains all information about a given state, whereas a search state
CS 188, Fall 2018, Note 1 1
CS-370 Artificial Intelligence
Fall 2023
CS-370, Fall 2023, Note 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: contains only the information about the world that’s necessary for planning (primarily for space effiency
reasons). To illustrate these concepts, we’ll introduce the hallmark motivating example of this course -
Pacman. The game of Pacman is simple: Pacman must navigate a maze and eat all the (small) food pellets
in the maze without being eaten by the malicious patrolling ghosts. If Pacman eats one of the (large) power
pellets, he becomes ghost-immune for a set period of time and gains the ability to eat ghosts for points. Let’s consider a variation of the game in which the maze contains only Pacman and food pellets. We can
pose two distinct search problems in this scenario: pathing and eat-all-dots. Pathing attempts to solve
the problem of getting from position (x1, y1) to position (x2, y2) in the maze optimally, while eat all dots
attempts to solve the problem of consuming all food pellets in the maze in the shortest time possible. Below,
the states, actions, successor function, and goal test for both problems are listed:
• Pathing
– States: (x,y) locations
– Actions: North, South, East, West
– Successor: Update location only
– Goal test: Is (x,y)=END? • Eat-all-dots
– States: (x,y) location, dot booleans
– Actions: North, South, East, West
– Successor: Update location and booleans
– Goal test: Are all dot booleans false? Note that for pathing, states contain less information than states for eat-all-dots, because for eat-all-dots we
must maintain an array of booleans corresponding to each food pellet and whether or not it’s been eaten in
the given state. A world state may contain more information still, potentially encoding information about
things like total distance traveled by Pacman or all positions visited by Pacman on top of its current (x,y)
location and dot booleans. State Space Size
An important question that often comes up while estimating the computational runtime of solving a search
problem is the size of the state space. This is done almost exclusively with the fundamental counting
principle, which states that if there are n variable objects in a given world which can take on x1, x2, ..., xn
different values respectively, then the total number of states is x1 · x2 · ...

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: · xn. Let’s use Pacman to show this
concept by example:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 2
84.3.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: Let’s say that the variable objects and their corresponding number of possiblilites are as follows:
• Pacman positions - Pacman can be in 120 distinct (x, y) positions, and there is only one Pacman
• Pacman Direction - this can be North, South, East, or West, for a total of 4 possibilities
• Ghost positions - There are two ghosts, each of which can be in 12 distinct (x, y) positions
• Food pellet configurations - There are 30 food pellets, each of which can be eaten or not eaten
Using the fundamental counting principle, we have 120 positions for Pacman, 4 directions Pacman can be
facing, 12·12 ghost configurations (12 for each ghost), and 2·2·...·2 = 230 food pellet configurations (each
of 30 food pellets has two possible values - eaten or not eaten). This gives us a total state space size of
120 · 4 · 122 · 230 .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: State Space Graphs and Search Trees
Now that we’ve established the idea of a state space and the four components necessary to completely define
one, we’re almost ready to begin solving search problems. The final piece of the puzzle is that of state space
graphs and search trees. Recall that a graph is defined by a set of nodes and a set of edges connecting various pairs of nodes. These
edges may also have weights associated with them. A state space graph is constructed with states representing nodes, with directed edges existing from a state to its successors. These edges represent actions,
and any associated weights represent the cost of performing the corresponding action. Typically, state space
graphs are much too large to store in memory (even our simple Pacman example from above has ⇡ 1013
possible states, yikes!), but they’re good to keep in mind conceptually while solving problems. It’s also
important to note that in a state space graph, each state is represented exactly once - there’s simply no need
to represent a state multiple times, and knowing this helps quite a bit when trying to reason about search
problems. Unlike state space graphs, our next structure of interest, search trees, have no such restriction on the number
of times a state can appear. This is because though search trees are also a class of graph with states as nodes
and actions as edges between states, each state/node encodes not just the state itself, but the entire path
(or plan) from the start state to the given state in the state space graph. Observe the state space graph and
corresponding search tree below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: The highlighted path (S ! d ! e ! r ! f ! G) in the given state space graph is represented in the
corresponding search tree by following the path in the tree from the start state S to the highlighted goal state
G. Similarly, each and every path from the start node to any other node is represented in the search tree by a
path from the root S to some descendant of the root corresponding to the other node. Since there often exist
multiple ways to get from one state to another, states tend to show up multiple times in search trees. As a
result, search trees are greater than or equal to their corresponding state space graph in size. We’ve already determined that state space graphs themselves can be enormous in size even for simple problems, and so the question arises - how can we perform useful computation on these structures if they’re too
big to represent in memory? The answer lies in successor functions - we only store states we’re immediately
working with, and compute new ones on-demand using the corresponding successor function. Typically,
search problems are solved using search trees, where we very carefully store a select few nodes to observe
at a time, iteratively replacing nodes with their successors until we arrive at a goal state. There exist various
methods by which to decide the order in which to conduct this iterative replacement of search tree nodes,
and we’ll present these methods now. Uninformed Search
The standard protocol for finding a plan to get from the start state to a goal state is to maintain an outer
fringe of partial plans derived from the search tree. We continually expand our fringe by removing a node
(which is selected using our given strategy) corresponding to a partial plan from the fringe, and replacing
it on the fringe with all its children. Removing and replacing an element on the fringe with its children
corresponds to discarding a single length n plan and bringing all length (n+1) plans that stem from it into
consideration. We continue this until eventually removing a goal state off the fringe, at which point we
conclude the partial plan corresponding to the removed goal state is in fact a path to get from the start state
to the goal state. Practically, most implementations of such algorithms will encode information about the
parent node, distance to node, and the state inside the node object. This procedure we have just outlined is
known as tree search, and the pseudocode for it is presented below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 4
Each NODE in in
State Space Graph
the search tree is
Search Tree
an entire PATH in
the state space
S
graph. n
6
C
a
a
S
We construct both
on demand - and
q
C
G
we construct as
q
C
a
G
little as possible.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: a

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: When we have no knowledge of the location of goal states in our search tree, we are forced to select our
strategy for tree search from one of the techniques that falls under the umbrella of uninformed search. We’ll now cover three such strategies in succession: depth-first search, breadth-first search, and uniform
cost search. Along with each strategy, some rudimentary properties of the strategy are presented as well, in
terms of the following:
• The completeness of each search strategy - if there exists a solution to the search problem, is the
strategy guaranteed to find it given infinite computational resources?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: • The optimality of each search strategy - is the strategy guaranteed to find the lowest cost path to a
goal state? • The branching factor b - The increase in the number of nodes on the fringe each time a fringe node
is dequeued and replaced with its children is O(b). At depth k in the search tree, there exists O(bk)
nodes. • The maximum depth m. • The depth of the shallowest solution s. Depth-First Search
• Description - Depth-first search (DFS) is a strategy for exploration that always selects the deepest
fringe node from the start node for expansion. • Fringe representation - Removing the deepest node and replacing it on the fringe with its children
necessarily means the children are now the new deepest nodes - their depth is one greater than the
depth of the previous deepest node. This implies that to implement DFS, we require a structure that
always gives the most recently added objects highest priority. A last-in, first-out (LIFO) stack does
exactly this, and is what is traditionally used to represent the fringe when implementing DFS. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 5
function TREE-SEARCH(problem, fringe)return a solution,or failure
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loop
do
iffringeisemptythenreturnfailure
node←REMOVE-FRONT(fringe)
if GOAL-TEsT(problem,sTATE[node])then return node
for child-nodeinEXPAND(sTATE[node],problem)do
fringe←INsERT(child-node,fringe)
end
end1 node
b nodes
b2 nodes
m tiers
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: • Completeness - Depth-first search is not complete. If there exist cycles in the state space graph, this
inevitably means that the corresponding search tree will be infinite in depth. Hence, there exists the
possibility that DFS will faithfully yet tragically get "stuck" searching for the deepest node in an
infinite-sized search tree, doomed to never find a solution. • Optimality - Depth-first search simply finds the "leftmost" solution in the search tree without regard
for path costs, and so is not optimal. • Time Complexity - In the worst case, depth first search may end up exploring the entire search tree. Hence, given a tree with maximum depth m, the runtime of DFS is O(bm). • Space Complexity - In the worst case, DFS maintains b nodes at each of m depth levels on the fringe. This is a simple consequence of the fact that once b children of some parent are enqueued, the nature
of DFS allows only one of the subtrees of any of these children to be explored at any given point in
time. Hence, the space complexity of BFS is O(bm). Breadth-First Search
• Description - Breadth-first search is a strategy for exploration that always selects the shallowest fringe
node from the start node for expansion. • Fringe representation - If we want to visit shallower nodes before deeper nodes, we must visit nodes
in their order of insertion. Hence, we desire a structure that outputs the oldest enqueued object to
represent our fringe. For this, BFS uses a first-in, first-out (FIFO) queue, which does exactly this. • Completeness - If a solution exists, then the depth of the shallowest node s must be finite, so BFS
must eventually search this depth. Hence, it’s complete. • Optimality - BFS is generally not optimal because it simply does not take costs into consideration
when determining which node to replace on the fringe. The special case where BFS is guaranteed to
be optimal is if all edge costs are equivalent, because this reduces BFS to a special case of uniform
cost search, which is discussed below. • Time Complexity - We must search 1+b+b2 +...+bs nodes in the worst case, since we go through
all nodes at every depth from 1 to s.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: Hence, the time complexity is O(bs). • Space Complexity - The fringe, in the worst case, contains all the nodes in the level corresponding to
the shallowest solution. Since the shallowest solution is located at depth s, there are O(bs) nodes at
this depth. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 6
1 node
b nodes
s tiers
b2 nodes
bs nodes
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: Uniform Cost Search
• Description - Uniform cost search (UCS), our last strategy, is a strategy for exploration that always
selects the lowest cost fringe node from the start node for expansion. • Fringe representation - To represent the fringe for UCS, the choice is usually a heap-based priority
queue, where the weight for a given enqueued node v is the path cost from the start node to v, or the
backward cost of v. Intuitively, a priority queue constructed in this manner simply reshuffles itself to
maintain the desired ordering by path cost as we remove the current minimum cost path and replace
it with its children. • Completeness - Uniform cost search is complete. If a goal state exists, it must have some finite length
shortest path; hence, UCS must eventually find this shortest length path. • Optimality - UCS is also optimal if we assume all edge costs are nonnegative. By construction, since
we explore nodes in order of increasing path cost, we’re guaranteed to find the lowest-cost path to a
goal state. The strategy employed in Uniform Cost Search is identical to that of Dijkstra’s algorithm,
and the chief difference is that UCS terminates upon finding a solution state instead of finding the
shortest path to all states. Note that having negative edge costs in our graph can make nodes on a
path have decreasing length, ruining our guarantee of optimality.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: (See Bellman-Ford algorithm for a
slower algorithm that handles this possibility)
• Time Complexity - Let us define the optimal path cost as C⇤ and the minimal cost between two nodes
in the state space graph as e. Then, we must roughly explore all nodes at depths ranging from 1 to
C⇤/e, leading to an runtime of O(bC⇤/e ). • Space Complexity - Roughly, the fringe will contain all nodes at the level of the cheapest solution, so
the space complexity of UCS is estimated as O(bC⇤/e ). As a parting note about uninformed search, it’s critical to note that the three strategies outlined above are
fundamentally the same - differing only in expansion strategy, with their similarities being captured by the
tree search pseudocode presented above. Informed Search
Uniform cost search is good because it’s both complete and optimal, but it can be fairly slow because it
expands in every direction from the start state while searching for a goal. If we have some notion of the
direction in which we should focus our search, we can significantly improve performance and "hone in" on
a goal much more quickly. This is exactly the focus of informed search. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 7
b
c≤1
c≤2
C*/e "tiers'
C≤3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: Heuristics
Heuristics are the driving force that allow estimation of distance to goal states - they’re functions that take
in a state as input and output a corresponding estimate. The computation performed by such a function is
specific to the search problem being solved. For reasons that we’ll see in A* search, below, we usually want
heuristic functions to be a lower bound on this remaining distance to the goal, and so heuristics are typically
solutions to relaxed problems (where some of the constraints of the original problem have been removed). Turning to our Pacman example, let’s consider the pathing problem described earlier. A common heuristic
that’s used to solve this problem is the Manhattan distance, which for two points (x1, y1) and (x2, y2) is
defined as follows:
Manhattan(x1, y1, x2, y2) = |x1 x2|+|y1 y2|
The above visualization shows the relaxed problem that the Manhattan distance helps solve - assuming
Pacman desires to get to the bottom left corner of the maze, it computes the distance from Pacman’s current
location to Pacman’s desired location assuming a lack of walls in the maze. This distance is the exact goal
distance in the relaxed search problem, and correspondingly is the estimated goal distance in the actual
search problem.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: With heuristics, it becomes very easy to implement logic in our agent that enables them
to "prefer" expanding states that are estimated to be closer to goal states when deciding which action to
perform. This concept of preference is very powerful, and is utilized by the following two search algorithms
that implement heuristic functions: greedy search and A*. Greedy Search
• Description - Greedy search is a strategy for exploration that always selects the fringe node with the
lowest heuristic value for expansion, which corresponds to the state it believes is nearest to a goal. • Fringe representation - Greedy search operates identically to UCS, with a priority queue fringe representation. The difference is that instead of using computed backward cost (the sum of edge weights
in the path to the state) to assign priority, greedy search uses estimated forward cost in the form of
heuristic values. • Completeness and Optimality - Greedy search is not guaranteed to find a goal state if one exists, nor is
it optimal, particularly in cases where a very bad heuristic function is selected. It generally acts fairly
unpredictably from scenario to scenario, and can range from going straight to a goal state to acting
like a badly-guided DFS and exploring all the wrong areas. A* Search
• Description - A* search is a strategy for exploration that always selects the fringe node with the lowest
estimated total cost for expansion, where total cost is the entire cost from the start node to the goal
node. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 8
15

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: (a) Greedy search on a good day :) (b) Greedy search on a bad day :(
• Fringe representation - Just like greedy search and UCS, A* search also uses a priority queue to
represent its fringe. Again, the only difference is the method of priority selection. A* combines the
total backward cost (sum of edge weights in the path to the state) used by UCS with the estimated
forward cost (heuristic value) used by greedy search by adding these two values, effectively yielding
an estimated total cost from start to goal. Given that we want to minimize the total cost from start to
goal, this is an excellent choice. • Completeness and Optimality - A* search is both complete and optimal, given an appropriate heuristic
(which we’ll cover in a minute). It’s a combination of the good from all the other search strategies
we’ve covered so far, incorporating the generally high speed of greedy search with the optimality and
completeness of UCS! Admissibility and Consistency
Now that we’ve discussed heuristics and how they are applied in both greedy and A* search, let’s spend
some time discussing what constitutes a good heuristic. To do so, let’s first reformulate the methods used
for determining priority queue ordering in UCS, greedy search, and A* with the following definitions:
• g(n) - The function representing total backwards cost computed by UCS.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: • h(n) - The heuristic value function, or estimated forward cost, used by greedy search. • f(n) - The function representing estimated total cost, used by A* search. f(n) = g(n) +h(n). Before attacking the question of what constitutes a "good" heuristic, we must first answer the question of
whether A* maintains its properties of completeness and optimality regardless of the heuristic function we
use. Indeed, it’s very easy to find heuristics that break these two coveted properties. As an example, consider
the heuristic function h(n) = 1g(n). Regardless of the search problem, using this heuristic yields
f(n) = g(n) +h(n)
= g(n)+(1g(n))
= 1
Hence, such a heuristic reduces A* search to BFS, where all edge costs are equivalent. As we’ve already
shown, BFS is not guaranteed to be optimal in the general case where edge weights are not constant. The condition required for optimality when using A* tree search is known as admissibility. The admissibility constraint states that the value estimated by an admissible heuristic is neither negative nor an overestimate. Defining h⇤(n) as the true optimal forward cost to reach a goal state from a given node n, we can
formulate the admissibility constraint mathematically as follows:
8n, 0  h(n)  h⇤(n)
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 9
bb

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: Theorem. For a given search problem, if the admissibility constraint is satisfied by a heuristic function h,
using A* tree search with h on that search problem will yield an optimal solution. Proof. Assume two reachable goal states are located in the search tree for a given search problem, an optimal
goal A and a suboptimal goal B. Some ancestor n of A (including perhaps A itself) must currently be on the
fringe, since A is reachable from the start state. We claim n will be selected for expansion before B, using
the following three statements:
1. g(A) < g(B). Because A is given to be optimal and B is given to be suboptimal, we can conclude that
A has a lower backwards cost to the start state than B. 2. h(A) = h(B) = 0, because we are given that our heuristic satisfies the admissibility constraint. Since
both A and B are both goal states, the true optimal cost to a goal state from A or B is simply h⇤(n) = 0;
hence 0  h(n)  0. 3. f(n)  f(A), because, through admissibility of h, f(n) = g(n) +h(n)  g(n) +h⇤(n) = g(A) = f(A). The total cost through node n is at most the true backward cost of A, which is also the total cost of A. We can combine statements 1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: and 2. to conclude that f(A) < f(B) as follows:
f(A) = g(A) +h(A) = g(A) < g(B) = g(B) +h(B) = f(B)
A simple consequence of combining the above derived inequality with statement 3. is the following:
f(n)  f(A)^ f(A) < f(B) =) f(n) < f(B)
Hence, we can conclude that n is expanded before B. Because we have proven this for arbitrary n, we can
conclude that all ancestors of A (including A itself) expand before B. 2
One problem we found above with tree search was that in some cases it could fail to ever find a solution,
getting stuck searching the same cycle in the state space graph infinitely. Even in situations where our
search technique doesn’t involve such an infinite loop, it’s often the case that we revisit the same node
multiple times because there’s multiple ways to get to that same node. This leads to exponentially more
work, and the natural solution is to simply keep track of which states you’ve already expanded, and never
expand them again. More explicitly, maintain a "closed" set of expanded nodes while utilizing your search
method of choice.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: Then, ensure that each node isn’t already in the set before expansion and add it to the
set after expansion if it’s not. Tree search with this added optimization is known as graph search, and the
pseudocode for it is presented below:
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 10
function GRAPH-SEARCH(problem, fringe)return a solution,or failure
closed←an empty set
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loopdo
iffringeis empty thenreturnfailure
mode
e←REMOVE-FRONT(fringe)
if GOAL-TEsT(problem,STATE[node])thenreturn node
if sTATE[node]is not in closed then
addsTATE[node]toclosed
for child-node in EXPAND(sTATE[node],problem)do
fringe←INsERT(child-node,fringe)
end
end

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: Note that in implementation, it’s critically important to store the closed set as a disjoint set and not a list. Storing it as a list requires costs O(n) operations to check for membership, which eliminates the performance
improvement graph search is intended to provide. An additional caveat of graph search is that it tends to
ruin the optimality of A*, even under admissible heuristics. Consider the following simple state space graph
and corresponding search tree, annotated with weights and heuristic values:
In the above example, it’s clear that the optimal route is to follow S ! A !C !

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: G, yielding a total path cost
of 1+1+3 = 5. The only other path to the goal, S ! B !C ! G has a path cost of 1+2+3 = 6. However,
because the heuristic value of node A is so much larger than the heuristic value of node B, node C is first
expanded along the second, suboptimal path as a child of node B. It’s then placed into the "closed" set, and
so A* graph search fails to reexpand it when it visits it as a child of A, so it never finds the optimal solution. Hence, to maintain completeness and optimality under A* graph search, we need an even stronger property
than admissibility, consistency. The central idea of consistency is that we enforce not only that a heuristic
underestimates the total distance to a goal from any given node, but also the cost/weight of each edge in the
graph. The cost of an edge as measured by the heuristic function is simply the difference in heuristic values
for two connected nodes. Mathematically, the consistency constraint can be expressed as follows:
8A,C h(A)h(C)  cost(A,C)
Theorem. For a given search problem, if the consistency constraint is satisfied by a heuristic function h,
using A* graph search with h on that search problem will yield an optimal solution. Proof. In order to prove the above theorem, we first prove that when running A* graph search with a
consistent heuristic, whenever we remove a node for expansion, we’ve found the optimal path to that node. Using the consistency constraint, we can show that the values of f(n) for nodes along any plan are nondecreasing. Define two nodes, n and n0
, where n0 is a successor of n. Then:
f(n0) = g(n0) +h(n0)
= g(n) +cost(n,n0) +h(n0)
 g(n) +h(n)
= f(n)
If for every parent-child pair (n,n0) along a path, f(n0)  f(n), then it must be the case that the values of
f(n) are nondecreasing along that path. We can check that the above graph violates this rule between f(A)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 11
A
S (0+2)
1
1
S
h=4
c
h=1
A (1+4)
B (1+1)
h=2
1
2
3
C (2+1)
C (3+1)
B
h=1
G (5+0)
G (6+0)
G
h=0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: and f(C). With this information, we can now show that whenever a node n is removed for expansion, its
optimal path has been found. Assume towards a contradiction that this is false - that when n is removed
from the fringe, the path found to n is suboptimal. This means that there must be some ancestor of n, n00, on
the fringe that was never expanded but is on the optimal path to n. Contradiction! We’ve already shown that
values of f along a path are nondecreasing, and so n00 would have been removed for expansion before n. All we have left to show to complete our proof is that an optimal goal A will always be removed for expansion
and returned before any suboptimal goal B. This is trivial, since h(A) = h(B) = 0, so
f(A) = g(A) < g(B) = f(B)
just as in our proof of optimality of A* tree search under the admissibility constraint. Hence, we can
conclude that A* graph search is optimal under a consistent heuristic. 2
A couple of important highlights from the discussion above before we proceed: for heuristics that are either
admissible/consistent to be valid, it must by definition be the case that h(G) = 0 for any goal state G. Additionally, consistency is not just a stronger constraint than admissibility, consistency implies admissibility. This stems simply from the fact that if no edge costs are overestimates (as guaranteed by consistency), the
total estimated cost from any node to a goal will also fail to be an overestimate. Consider the following three-node network for an example of an admissible but inconsistent heuristic:
The red dotted line corresponds to the total estimated goal distance. If h(A) = 4, then the heuristic is
admissible, as the distance from A to the goal is 4  h(A), and same for h(C) = 1  3. However, the
heuristic cost from A to C is h(A)h(C) = 41 = 3. Our heuristic estimates the cost of the edge between
A and C to be 3 while the true value is cost(A,C) = 1, a smaller value. Since h(A)  h(C) ⇥ cost(A,C),
this heuristic is not consistent. Running the same computation for h(A) = 2, however, yields h(A)h(C) =
21 = 1  cost(A,C).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: Thus, using h(A) = 2 makes our heuristic consistent. Dominance
Now that we’ve established the properties of admissibility and consistency and their roles in maintaining
the optimality of A* search, we can return to our original problem of creating "good" heuristics, and how to
tell if one heuristic is better than another. The standard metric for this is that of dominance.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: If heuristic a is
dominant over heuristic b, then the estimated goal distance for a is greater than the estimated goal distance
for b for every node in the state space graph. Mathematically,
8n : ha(n)  hb(n)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 12
A
1
h-4
C
h=1
h=2
3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: Dominance very intuitively captures the idea of one heuristic being better than another - if one admissible/consistent heuristic is dominant over another, it must be better because it will always more closely estimate the distance to a goal from any given state. Additionally, the trivial heuristic is defined as h(n) = 0,
and using it reduces A* search to UCS. All admissible heuristics dominate the trivial heuristic. The trivial
heuristic is often incorporated at the base of a semi-lattice for a search problem, a dominance hierarchy of
which it is located at the bottom. Below is an example of a semi-lattice that incorporates various heuristics
ha,hb, and hc ranging from the trivial heuristic at the bottom to the exact goal distance at the top:
As a general rule, the max function applied to multiple admissible heuristics will also always be admissible. This is simply a consequence of all values output by the heuristics for any given state being constrained by
the admissibility condition, 0  h(n)  h⇤(n). The maximum of numbers in this range must also fall in the
same range. The same can be shown easily for multiple consistent heuristics as well.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: It’s common practice
to generate multiple admissible/consistent heuristics for any given search problem and compute the max
over the values output by them to generate a heuristic that dominates (and hence is better than) all of them
individually. Summary
In this note, we discussed search problems, which we characterize formally with four components: a state
space, a successor function, a start state, and a goal state. Search problems can be solved using a variety of
search techniques, including but not limited to the five we study in CS 188:
• Breadth-first Search
• Depth-first Search
• Uniform Cost Search
• Greedy Search
• A* Search
The first three search techniques listed above are examples of uninformed search, while the latter two are
examples of informed search which use heuristics to estimate goal distance and optimize performance. CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 13
ecact
max(ha,hb)
ha
hb
hc
zero

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 5
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Reinforcement Learning
In the previous note, we discussed Markov decision processes, which we solved using techniques such as
value iteration and policy iteration to compute the optimal values of states and extract optimal policies. Solving Markov decision processes is an example of offline planning, where agents have full knowledge
of both the transition function and the reward function, all the information they need to precompute optimal
actions in the world encoded by the MDP without ever actually taking any actions. In this note, we’ll discuss
online planning, during which an agent has no prior knowledge of rewards or transitions in the world (still
represented as a MDP). In online planning, an agent must try exploration, during which it performs actions
and receives feedback in the form of the successor states it arrives in and the corresponding rewards it
reaps. The agent uses this feedback to estimate an optimal policy through a process known as reinforcement
learning before using this estimated policy for exploitation, or reward maximization. Let’s start with some basic terminology. At each timestep during online planning, an agent starts in a state
s, then takes an action a and ends up in a successor state s0, attaining some reward r. Each (s,a,s0,r) tuple is
known as a sample.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 0}
Content: Often, an agent continues to take actions and collect samples in succession until arriving
at a terminal state. Such a collection of samples is known as an episode. Agents typically go through many
episodes during exploration in order to collect sufficient data needed for learning. There are two types of reinforcement learning, model-based learning and model-free learning. Modelbased learning attempts to estimate the transition and reward functions with the samples attained during
exploration before using these estimates to solve the MDP normally with value or policy iteration. Modelfree learning, on the other hand, attempts to estimate the values or q-values of states directly, without ever
using any memory to construct a model of the rewards and transitions in the MDP. Model-Based Learning
In model-based learning an agent generates an approximation of the transition function, Tˆ(s,a,s0), by keeping counts of the number of times it arrives in each state s0 after entering each q-state (s,a). The agent can
CS 188, Fall 2018, Note 5 1
Agent
State: s
Actions:a
Reward:r
Environment

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 1}
Content: then generate the the approximate transition function Tˆ upon request by normalizing the counts it has collected - dividing the count for each observed tuple (s,a,s0
) by the sum over the counts for all instances where
the agent was in q-state (s,a). Normalization of counts scales them such that they sum to one, allowing them
to be interpreted as probabilities. Consider the following example MDP with states S = {A,B,C,D,E, x},
with x representing the terminal state, and discount factor g = 1:
Assume we allow our agent to explore the MDP for four episodes under the policy pexplore delineated above
(a directional triangle indicates motion in the direction the triangle points, and a blue squares represents
taking exit as the action of choice), and yield the following results:
We now have a collective 12 samples, 3 from each episode with counts as follows:
s a s0 count
A exit x 1
B east C 2
C east A 1
C east D 3
D exit x 3
E north C 2
Recalling that T(s,a,s0) = P(s0|a,s), we can estimate the transition function with these counts by dividing
the counts for each tuple (s,a,s0) by the total number of times we were in q-state (s,a) and the reward
function directly from the rewards we reaped during exploration:
CS 188, Fall 2018, Note 5 2
A
B
D
D
△
EEpisode 1
Episode 2
B,east, C, -1
B,east, C, -1
C,east, D, -1
C, east, D, -1
D, exit, x, +10
D, exit, x, +10
Episode 3
Episode 4
E, north, C, -1
E, north, C, -1
C, east,
D, -1
C, east,
 A, -1
D, exit,
x, +10
A, exit,
x, -10

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 2}
Content: • Transition Function: Tˆ(s,a,s0
)
– Tˆ(A, exit, x) = #(A,exit,x)
#(A,exit) = 1
1 = 1
– Tˆ(B, east,C) = #(B,east,C)
#(B,east) = 2
2 = 1
– Tˆ(C, east,A) = #(C,east,A)
#(C,east) = 1
4 = 0.25
– Tˆ(C, east,D) = #(C,east,D)
#(C,east) = 3
4 = 0.75
– Tˆ(D, exit, x) = #(D,exit,x)
#(D,exit) = 3
3 = 1
– Tˆ(E,north,C) = #(E,north,C)
#(E,north) = 2
2 = 1
• Reward Function: Rˆ(s,a,s0)
– Rˆ(A, exit, x) = 10
– Rˆ(B, east,C) = 1
– Rˆ(C, east,A) = 1
– Rˆ(C, east,D) = 1
– Rˆ(D, exit, x)=+10
– Rˆ(E,north,C) = 1
By the law of large numbers, as we collect more and more samples by having our agent experience more
episodes, our models of Tˆ and Rˆ will improve, with Tˆ converging towards T and Rˆ acquiring knowledge of
previously undiscovered rewards as we discover new (s,a,s0) tuples. Whenever we see fit, we can end our
agent’s training to generate a policy pexploit by running value or policy iteration with our current models for
Tˆ and Rˆ and use pexploit for exploitation, having our agent traverse the MDP taking actions seeking reward
maximization rather than seeking learning. We’ll soon discuss methods for how to allocate time between
exploration and explotation effectively. Model-based learning is very simple and intuitive yet remarkably
effective, generating Tˆ and Rˆ with nothing more than counting and normalization. However, it can be
expensive to maintain counts for every (s,a,s0) tuple seen, and so in the next section on model-free learning
we’ll develop methods to bypass maintaining counts altogether and avoid the memory overhead required by
model-based learning. Model-Free Learning
Onward to model-free learning! There are several model-free learning algorithms, and we’ll cover three
of them: direct evaluation, temporal difference learning, and Q-learning. Direct evaluation and temporal
difference learning fall under a class of algorithms known as passive reinforcement learning. In passive
reinforcement learning, an agent is given a policy to follow and learns the value of states under that policy
as it experiences episodes, which is exactly what is done by policy evaluation for MDPs when T and R are
known. Q-learning falls under a second class of model-free learning algorithms known as active reinforcement learning, during which the learning agent can use the feedback it receives to iteratively update its
policy while learning until eventually determining the optimal policy after sufficient exploration. Direct Evaluation
The first passive reinforcement learning technique we’ll cover is known as direct evaluation, a method
that’s as boring and simple as the name makes it sound. All direct evaluation does is fix some policy p and
have the agent that’s learning experience several episodes while following p. As the agent collects samples
through these episodes it maintains counts of the total utility obtained from each state and the number of
times it visited each state. At any point, we can compute the estimated value of any state s by dividing the
total utility obtained from s by the number of times s was visited. Let’s run direct evaluation on our example
from earlier, recalling that g = 1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 2}
Content: CS 188, Fall 2018, Note 5 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 3}
Content: Walking through the first episode, we can see that from state D to termination we acquired a total reward of
10, from state C we acquired a total reward of (1) +10 = 9, and from state B we acquired a total reward
of (1)+(1)+10 = 8. Completing this process yields the total reward across episodes for each state and
the resulting estimated values as follows:
s Total Reward Times Visited Vp (s)
A 10 1 10
B 16 2 8
C 16 4 4
D 30 3 10
E 4 2 2
Though direct evaluation eventually learns state values for each state, it’s often unnecessarily slow to converge because it wastes information about transitions between states. In our example, we computed Vp (E) = 2 and Vp (B) = 8, though based on the feedback we received both
states only have C as a successor state and incur the same reward of 1 when transitioning to C. According
to the Bellman equation, this means that both B and E should have the same value under p. However, of the
4 times our agent was in state C, it transitioned to D and reaped a reward of 10 three times and transitioned
to A and reaped a reward of 10 once. It was purely by chance that the single time it received the 10
reward it started in state E rather than B, but this severely skewed the estimated value for E. With enough
episodes, the values for B and E will converge to their true values, but cases like this cause the process to
take longer than we’d like.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 3}
Content: This issue can be mitigated by choosing to use our second passive reinforcement
learning algorithm, temporal difference learning. CS 188, Fall 2018, Note 5 4
Episode 1
Episode 2
B, east, C, -1
B,east, C, -1
A
C,east, D, -1
C, east, D, -1
D
exit, x, +10
D, exit, x, +10
B
C
D
Episode 3
Episode 4
△
E, north, C, -1
E, north, C, -1
E
C, east,
D, -1
C, east,
A, -1
D
exit,
x, +10
A, exit,
x, -10-10
A
+8
+4
+10
B
C
D
-2
E

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 4}
Content: Temporal Di↵erence Learning
Temporal difference learning (TD learning) uses the idea of learning from every experience, rather than
simply keeping track of total rewards and number of times states are visited and learning at the end as direct
evaluation does. In policy evaluation, we used the system of equations generated by our fixed policy and
the Bellman equation to determine the values of states under that policy (or used iterative updates like with
value iteration). Vp (s) = Â
s0
T(s,p(s),s
0
)[R(s,p(s),s
0
) +gVp (s
0
)]
Each of these equations equates the value of one state to the weighted average over the discounted values of
that state’s successors plus the rewards reaped in transitioning to them. TD learning tries to answer the question of how to compute this weighted average without the weights, cleverly doing so with an exponential
moving average. We begin by initializing 8s, Vp (s) = 0. At each timestep, an agent takes an action p(s)
from a state s, transitions to a state s0, and receives a reward R(s,p(s),s0). We can obtain a sample value by
summing the received reward with the discounted current value of s0 under p:
sample = R(s,p(s),s
0
) +gVp (s
0
)
This sample is a new estimate for Vp (s). The next step is to incorporate this sampled estimate into our
existing model for Vp (s) with the exponential moving average, which adheres to the following update rule:
Vp (s) (1a)Vp (s) +a ·sample
Above, a is a parameter constrained by 0  a  1 known as the learning rate that specifies the weight
we want to assign our existing model for Vp (s), 1a, and the weight we want to assign our new sampled
estimate, a. It’s typical to start out with learning rate of a = 1, accordingly assigning Vp (s) to whatever the
first sample happens to be, and slowly shrinking it towards 0, at which point all subsequent samples will be
zeroed out and stop affecting our model of Vp (s). Let’s stop and analyze the update rule for a minute. Annotating the state of our model at different points in
time by defining Vp
k (s) and samplek as the estimated value of state s after the kth update and the kth sample
respectively, we can reexpress our update rule:
Vp
k (s) (1a)Vpk1(s) +a ·samplek
This recursive definition for Vp
k (s) happens to be very interesting to expand:
Vp
k (s) (1a)Vpk1(s) +a ·samplek
Vp
k (s) (1a)[(1a)Vpk2(s) +a ·samplek1] +a ·samplek
Vp
k (s) (1a)
2
Vp
k2(s)+(1a)·a ·samplek1 +a ·samplek
. .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 4}
Content: . Vp
k (s) (1a)
k
Vp
0 (s) +a · [(1a)
k1 ·sample1 +...+ (1a)·samplek1 +samplek]
Vp
k (s) a · [(1a)
k1 ·sample1 +...+ (1a)·samplek1 +samplek]
Because 0  (1  a)  1, as we raise the quantity (1  a) to increasingly larger powers, it grows closer
and closer to 0. By the update rule expansion we derived, this means that older samples are given exponentially less weight, exactly what we want since these older samples are computed using older (and hence
worse) versions of our model for Vp (s)! This is the beauty of temporal difference learning - with a single
straightfoward update rule, we are able to:
CS 188, Fall 2018, Note 5 5


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 5}
Content: • learn at every timestep, hence using information about state transitions as we get them since we’re
using iteratively updating versions ofVp (s0)in our samples rather than waiting until the end to perform
any computation. • give exponentially less weight to older, potentially less accurate samples.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 5}
Content: • converge to learning true state values much faster with fewer episodes than direct evaluation. Q-Learning
Both direct evaluation and TD learning will eventually learn the true value of all states under the policy
they follow. However, they both have a major inherent issue - we want to find an optimal policy for our
agent, which requires knowledge of the q-values of states. To compute q-values from the values we have,
we require a transition function and reward function as dictated by the Bellman equation. Q⇤(s,a) = Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
Resultingly, TD learning or direct evaluation are typically used in tandem with some model-based learning
to acquire estimates of T and R in order to effectively update the policy followed by the learning agent. This
became avoidable by a revolutionary new idea known as Q-learning, which proposed learning the q-values
of states directly, bypassing the need to ever know any values, transition functions, or reward functions. As a result, Q-learning is entirely model-free. Q-learning uses the following update rule to perform what’s
known as q-value iteration:
Qk+1(s,a) Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +g max
a0 Qk(s
0
,a0)]
Note that this update is only a slight modification over the update rule for value iteration. Indeed, the only
real difference is that the position of the max operator over actions has been changed since we select an
action before transitioning when we’re in a state, but we transition before selecting a new action when we’re
in a q-state. With this new update rule under our belt, Q-learning is derived essentially the same way as TD learning, by
acquiring q-value samples:
sample = R(s,a,s
0
) +g max
a0 Q(s
0
,a0)
and incoporating them into an exponential moving average. Q(s,a) (1a)Q(s,a) +a ·sample
As long as we spend enough time in exploration and decrease the learning rate a at an appropriate pace, Qlearning learns the optimal q-values for every q-state. This is what makes Q-learning so revolutionary - while
TD learning and direct evaluation learn the values of states under a policy by following the policy before
determining policy optimality via other techniques, Q-learning can learn the optimal policy directly even by
taking suboptimal or random actions. This is called off-policy learning (contrary to direct evaluation and
TD learning, which are examples of on-policy learning). Approximate Q-Learning
Q-learning is an incredible learning technique that continues to sit at the center of developments in the field
of reinforcement learning. Yet, it still has some room for improvement. As it stands, Q-learning just stores
CS 188, Fall 2018, Note 5 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 6}
Content: all q-values for states in tabular form, which is not particularly efficient given that most applications of
reinforcement learning have several thousands or even millions of states. This means we can’t visit all states
during training and can’t store all q-values even if we could for lack of memory. Figure 1 Figure 2 Figure 3
Above, if Pacman learned that Figure 1 is unfavorable after running vanilla Q-learning, it would still have
no idea that Figure 2 or even Figure 3 are unfavorable as well. Approximate Q-learning tries to account
for this by learning about a few general situations and extrapolating to many similar situations. The key to
generalizing learning experiences is the feature-based representation of states, which represents each state
as a vector known as a feature vector. For example, a feature vector for Pacman may encode
• the distance to the closest ghost. • the distance to the closest food pellet. • the number of ghosts.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 6}
Content: • is Pacman trapped? 0 or 1
With feature vectors, we can treat values of states and q-states as linear value functions:
V(s) = w1 · f1(s) +w2 · f2(s) +...+wn · fn(s) = ~w· ~f(s)
Q(s,a) = w1 · f1(s,a) +w2 · f2(s,a) +...+wn · fn(s,a) = ~w· ~f(s,a)
where ~f(s) = ⇥
f1(s) f2(s) ... fn(s)
⇤T and ~f(s,a) = ⇥
f1(s,a) f2(s,a) ... fn(s,a)
⇤T represent the
feature vectors for state s and q-state (s,a) respectively and ~w = ⇥
w1 w2 ... wn
⇤
represents a weight
vector. Defining di f f erence as
di f f erence = [R(s,a,s
0
) +g max
a0 Q(s
0
,a0)]Q(s,a)
approximate Q-learning works almost identically to Q-learning, using the following update rule:
wi wi +a · di f f erence · fi(s,a)
Rather than storing Q-values for each and every state, with approximate Q-learning we only need to store a
single weight vector and can compute Q-values on-demand as needed. As a result, this gives us not only a
more generalized version of Q-learning, but a significantly more memory-efficient one as well. As a final note on Q-learning, we can reexpress the update rule for exact Q-learning using di f f erence as
follows:
Q(s,a) Q(s,a) +a · di f f erence
CS 188, Fall 2018, Note 5 7
11

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 7}
Content: This second notation gives us a slightly different but equally valuable interpration of the update: it’s computing the difference between the sampled estimated and the current model of Q(s,a), and shifting the model
in the direction of the estimate with the magnitude of the shift being proportional to the magnitude of the
difference. Exploration and Exploitation
We’ve now covered several different methods for an agent to learn an optimal policy, and harped on the
fact that "sufficient exploration" is necessary for this without really elaborating on what’s really meant
by "sufficient". In the upcoming two sections, we’ll discuss two methods for distributing time between
exploration and exploitation: e-greedy policies and exploration functions. e-Greedy Policies
Agents following an e-greedy policy define some probability 0  e  1, and act randomly and explore with
probability e. Accordingly, they follow their current established policy and exploit with probability (1e).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 7}
Content: This is a very simple policy to implement, yet can still be quite difficult to handle. If a large value for e is
selected, then even after learning the optimal policy, the agent will still behave mostly randomly. Similarly,
selecting a small value for e means the agent will explore infrequently, leading Q-learning (or any other
selected learning algorithm) to learn the optimal policy very slowly. To get around this, e must be manually
tuned and lowered over time to see results. Exploration Functions
This issue of manually tuning e is avoided by exploration functions, which use a modified q-value iteration
update to give some preference to visiting less-visited states. The modified update is as follows:
Q(s,a) (1a)Q(s,a) +a · [R(s,a,s
0
) +g max
a0 f(s
0
,a0)]
where f denotes an exploration function. There exists some degree of flexibility in designing an exploration
function, but a common choice is to use
f(s,a) = Q(s,a) + k
N(s,a)
with k being some predetermined value, and N(s,a) denoting the number of times q-state (s,a) has been
visited. Agents in a state s always select the action that has the highest f(s,a) from each state, and hence
never have to make a probabilistic decision between exploration and exploitation. Instead, exploration is
automatically encoded by the exploration function, since the term k
N(s,a) can give enough of a "bonus" to
some infrequently-taken action such that it is selected over actions with higher q-values. As time goes on
and states are visited more frequently, this bonus decreases towards 0 for each state and f(s,a) regresses
towards Q(s,a), making exploitation more and more exclusive. CS 188, Fall 2018, Note 5 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 8}
Content: Summary
It’s very important to remember that reinforcement learning has an underlying MDP, and the goal of reinforcement learning is to solve this MDP by deriving an optimal policy. The difference between using
reinforcement learning and using methods like value iteration and policy iteration is the lack of knowledge
of the transition function T and the reward function R for the underlying MDP. As a result, agents must
learn the optimal policy through online trial-by-error rather than pure offline computation. There are many
ways to do this:
• Model-based learning - Runs computation to estimate the values of the transition function T and the
reward function R and uses MDP-solving methods like value or policy iteration with these estimates. • Model-free learning - Avoids estimation of T and R, instead using other methods to directly estimate
the values or q-values of states. – Direct evaluation - follows a policy p and simply counts total rewards reaped from each state
and the total number of times each state is visited. If enough samples are taken, this converges to
the true values of states under p, albeit being slow and wasting information about the transitions
between states. – Temporal difference learning - follows a policy p and uses an exponential moving average with
sampled values until convergence to the true values of states under p. TD learning and direct
evaluation are examples of on-policy learning, which learn the values for a specific policy before
deciding whether that policy is suboptimal and needs to be updated. – Q-Learning - learns the optimal policy directly through trial and error with q-value iteration
updates. This an example of off-policy learning, which learns an optimal policy even when
taking suboptimal actions. – Approximate Q-Learning - does the same thing as Q-learning but uses a feature-based representation for states to generalize learning.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes A.pdf', 'page': 8}
Content: CS 188, Fall 2018, Note 5 9


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 7
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Decision Networks
In the third note, we learned about game trees and algorithms such as minimax and expectimax which we
used to determine optimal actions that maximized our expected utility. Then in the sixth note, we discussed
Bayes’ nets and how we can use evidence we know to run probabilistic inference to make predictions. Now
we’ll discuss a combination of both Bayes’ nets and expectimax known as a decision network that we can
use to model the effect of various actions on utilities based on an overarching graphical probabilistic model. Let’s dive right in with the anatomy of a decision network:
• Chance nodes - Chance nodes in a decision network behave identically to Bayes’ nets. Each outcome
in a chance node has an associated probability, which can be determined by running inference on the
underlying Bayes’ net it belongs to. We’ll represent these with ovals. • Action nodes - Action nodes are nodes that we have complete control over; they’re nodes representing
a choice between any of a number of actions which we have the power to choose from. We’ll represent
action nodes with rectangles. • Utility nodes - Utility nodes are children of some combination of action and chance nodes. They
output a utility based on the values taken on by their parents, and are represented as diamonds in our
decision networks. Consider a situation when you’re deciding whether or not to take an umbrella when you’re leaving for class
in the morning, and you know there’s a forecasted 30% chance of rain.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 0}
Content: Should you take the umbrella? If
there was a 80% chance of rain, would your answer change? This situation is ideal for modeling with a
decision network, and we do it as follows:
CS 188, Fall 2018, Note 7 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 1}
Content: As we’ve done throughout this course with the various modeling techniques and algorithms we’ve discussed,
our goal with decision networks is again to select the action which yields the maximum expected utility
(MEU). This can be done with a fairly straightforward and intuitive procedure:
• Start by instantiating all evidence that’s known, and run inference to calculate the posterior probabilities of all chance node parents of the utility node into which the action node feeds. • Go through each possible action and compute the expected utility of taking that action given the
posterior probabilities computed in the previous step. The expected utility of taking an action a given
evidence e and n chance nodes is computed with the following formula:
EU(a|e) = Âx1,...,xnP(x1,...,xn|e)U(a, x1,...,xn)
where each xi represents a value that the i
th chance node can take on. We simply take a weighted
sum over the utilities of each outcome under our given action with weights corresponding to the
probabilities of each outcome.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 1}
Content: • Finally, select the action which yielded the highest utility to get the MEU. Let’s see how this actually looks by calculating the optimal action (should we leave or take our umbrella) for
our weather example, using both the conditional probability table for weather given a bad weather forecast
(forecast is our evidence variable) and the utility table given our action and the weather:
CS 188, Fall 2018, Note 7 2
Umbrella
Weather
Forecast

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 2}
Content: Note that we have omitted the inference computation for the posterior probabilities P(W|F = bad), but we
could compute these using any of the inference algorithms we discussed for Bayes Nets. Instead, here we
simply assume the above table of posterior probabilities for P(W|F = bad) as given. Going through both
our actions and computing expected utilities yields:
EU(leave|bad) = ÂwP(w|bad)U(leave,w)
= 0.34 · 100+0.66 · 0 = 34
EU(take|bad) = ÂwP(w|bad)U(take,w)
= 0.34 · 20+0.66 · 70 = 53
All that’s left to do is take the maximum over these computed utilities to determine the MEU:
MEU(F = bad) = maxa EU(a|bad) = 53
The action that yields the maximum expected utility is take, and so this is the action recommended to us
by the decision network. More formally, the action that yields the MEU can be determined by taking the
argmax over expected utilities. Outcome Trees
We mentioned at the start of this note that decision networks involved some expectimax-esque elements, so
let’s discuss what exactly that means. We can unravel the selection of an action corresponding to the one
that maximizes expected utility in a decision network as an outcome tree. Our weather forecast example
from above unravels into the following outcome tree:
CS 188, Fall 2018, Note 7 3
A
W
U(A,W)
Umbrella
leave
sun
100
leave
rain
0
take
sun
20
take
rain
70
Weather
W
P(W|F=bad)
sun
0.34
rain
0.66
Forecast
peq=
?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 3}
Content: The root node at the top is a maximizer node, just like in expectimax, and is controlled by us. We select
an action, which takes us to the next level in the tree, controlled by chance nodes. At this level, chance
nodes resolve to different utility nodes at the final level with probabilities corresponding to the posterior
probabilities derived from probabilistic inference run on the underlying Bayes’ net. What exactly makes
this different from vanilla expectimax? The only real difference is that for outcome trees we annotate our
nodes with what we know at any given moment (inside the curly braces). The Value of Perfect Information
In everything we’ve covered up to this point, we’ve generally always assumed that our agent has all the
information it needs for a particular problem and/or has no way to acquire new information. In practice, this
is hardly the case, and one of the most important parts of decision making is knowing whether or not it’s
worth gathering more evidence to help decide which action to take. Observing new evidence almost always
has some cost, whether it be in terms of time, money, or some other medium. In this section, we’ll talk
about a very important concept - the value of perfect information (VPI) - which mathematically quantifies
the amount an agent’s maximum expected utility is expected to increase if it observes some new evidence. We can compare the VPI of learning some new information with the cost associated with observing that
information to make decisions about whether or not it’s worthwhile to observe. General Formula
Rather than simply presenting the formula for computing the value of perfect information for new evidence,
let’s walk through an intuitive derivation.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 3}
Content: We know from our above definition that the value of perfect
information is the amount our maximum expected utility is expected to increase if we decide to observe new
evidence. We know our current maximum utility given our current evidence e:
MEU(e) = maxa ÂsP(s|e)U(s,a)
Additionally, we know that if we observed some new evidence e0 before acting, the maximum expected
utility of our action at that point would become
MEU(e, e0) = maxa ÂsP(s|e, e0)U(s,a)
CS 188, Fall 2018, Note 7 4
{b}
leave
take
w|{b}
w|{b}
rain
rain
sun
sun
U(t,s)
U(t,r)
U(l,s)
U(l,r)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 4}
Content: However, note that we don’t know what new evidence we’ll get. For example, if we didn’t know the weather
forecast beforehand and chose to observe it, the forecast we observe might be either good or bad. Because
we don’t know what what new evidence e0 we’ll get, we must represent it as a random variable E0. How
do we represent the new MEU we’ll get if we choose to observe a new variable if we don’t know what the
evidence gained from observation will tell us? The answer is to compute the expected value of the maximum
expected utility which, while being a mouthful, is the natural way to go:
MEU(e,E0) = Â
e0
P(e0|e)MEU(e, e0)
Observing a new evidence variable yields a different MEU with probabilities corresponding to the probabilities of observing each value for the evidence variable, and so by computing MEU(e,E0
) as above, we
compute what we expect our new MEU will be if we choose to observe new evidence. We’re just about done
now - returning to our definition for VPI, we want to find the amount our MEU is expected to increase if we
choose to observe new evidence. We know our current MEU and the expected value of the new MEU if we
choose to observe, so the expected MEU increase is simply the difference of these two terms! Indeed,
V PI(E0|e) = MEU(e,E0) MEU(e)
where we can read V PI(E0|e) as "the value of observing new evidence E’ given our current evidence e". Let’s work our way through an example by revisiting our weather scenario one last time:
If we don’t observe any evidence, then our maximum expected utility can be computed as follows:
MEU(?) = maxa EU(a)
= maxa ÂwP(w)U(a,w)
= max{0.7 · 100+0.3 · 0,0.7 · 20+0.3 · 70}
= max{70,35}
= 70
Note that the convention when we have no evidence is to write MEU(?), denoting that our evidence is the
empty set. Now let’s say that we’re deciding whether or not to observe the weather forecast. We’ve already
computed that MEU(F = bad) = 53, and let’s assume that running an identical computation for F = good
CS 188, Fall 2018, Note 7 5
A
W
U
Umbrella
leave
sun
100
U
leave
rain
0
take
sun
20
Weather
take
rain
70
w
P(W)
F
P(F)
sun
0.7
good
0.59
rain
0.3
Forecast
bad
0.41

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 5}
Content: yields MEU(F = good) = 95. We’re now ready to compute MEU(e,E0
):
MEU(e,E0) = MEU(F)
= Â
e0
P(e0|e)MEU(e, e0)
= Â
f
P(F = f)MEU(F = f)
= P(F = good)MEU(F = good) +P(F = bad)MEU(F = bad)
= 0.59 · 95+0.41 · 53
= 77.78
Hence we conclude V PI(F) = MEU(F) MEU(?) = 77.7870 = 7.78 . Properties of VPI
The value of perfect information has several very important properties, namely:
• Nonnegativity. 8E0, e V PI(E0|e)  0
Observing new information always allows you to make a more informed decision, and so your maximum expected utility can only increase (or stay the same if the information is irrelevant for the
decision you must make). • Nonadditivity.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'e:\\TALIM\\Data\\AI-Notes B.pdf', 'page': 5}
Content: V PI(Ej,Ek|e) 6= V PI(Ej|e) +V PI(Ek|e) in general. This is probably the trickiest of the three properties to understand intuitively. It’s true because generally observing some new evidence Ej might change how much we care about Ek; therefore we
can’t simply add the VPI of observing Ej to the VPI of observing Ek to get the VPI of observing
both of them. Rather, the VPI of observing two new evidence variables is equivalent to observing
one, incorporating it into our current evidence, then observing the other. This is encapsulated by the
order-independence property of VPI, described more below. • Order-independence. V PI(Ej,Ek|e) = V PI(Ej|e) +V PI(Ek|e,Ej) = V PI(Ek|e) +V PI(Ej|e,Ek)
Observing multiple new evidences yields the same gain in maximum expected utility regardless of the
order of observation. This should be a fairly straightforward assumption - because we don’t actually
take any action until after observing any new evidence variables, it doesn’t actually matter whether
we observe the new evidence variables together or in some arbitrary sequential order. CS 188, Fall 2018, Note 7 6


