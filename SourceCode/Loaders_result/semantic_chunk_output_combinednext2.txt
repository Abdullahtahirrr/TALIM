Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 1
10th Sept 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 1}
Content: A tech enthusiasts committed to AI 
Who Am I…
5 Years of AI Innovation
Team
• 11-member team with AI experience
Dr. Seemab Latif
• HoD AI&DS
• Tenured Associate Professor
• PhD Artificial Intelligence (Uni of Manchester)
• 70 plus publications
• 7 commercial IPs
• CPInS Lab
• Funded Research and Commercial Projects
• UG, PG, PhD Research
YCHINA-PAKISTAN
INTELLIGENTSYSTEMS(EPInS)LABaawazeS
OneScreen'SKYELECTRICG
C
SkyLabs
AI

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 2}
Content: What is AI
• Artificial intelligence is the ability of machines to perform tasks that 
are typically associated with human intelligence, such as learning and 
problem-solving. • Such machines have the ability to learn about the world that 
surrounds them and take actions that will have the best chances of 
achieving success. • Research into AI involves using a lot of tools from other sciences such 
as psychology, linguistics, computer science, and many others.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 2}
Content: 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 3}
Content: AI vs. HI
• Human intelligence and 
artificial intelligence go handin-hand. • AI can handle repetitive, datadriven tasks and give databased results. • Human intelligence can work 
on creative, emotional and 
critically complex tasks
0:

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 4}
Content: Areas of AI
ARTIFICIALINTELLIGENCE
Programswiththeabilityto
learnandreasonlikehumans
MACHINELEARNING
Algorithmswiththeabilitytolearn
withoutbeingexplicitlyprogrammed
DEEPLEARNING
Subsetofmachine learning
inwhichartificialneural
networksadaptandlearn
fromvastamountsofdata

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 5}
Content: Applications 
of Artificial 
Intelligence
Automatic
Language
Recognition
Image
Translation
Medical
Diagnosis
Speech
Stock
Recognition
Market
trading
Applications
of
Online
Fraud
Traffic
Machinelearning
Detection
Prediction
Virtual
Product
Personal
recommend
Email Spam
Assistant
andMalware
-ations
Self
driving
Filtering
cars

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 6}
Content: 
WHAT
CAN DO

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 7}
Content: Source: https://kling.kuaishou.com
K可灵AI可灵AI可灵AI可灵AIC可灵AIo-

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 8}
Content: Create Audio
• AI music generators, audio tools, and voice generators, you can add 
a soundtrack to a podcast, add a voice overlay to marketing videos, 
and even create your own music to enhance projects. • For example, using Mubert, you can easily generate music with a text 
prompt and a few clicks.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 8}
Content: We used the prompt "First lecture of the 
semester, hurray” and created this clip in less than a minute. ProductsBlog Aboutus Contact BecomeAffiliateForArtists
Sign Up LogIn
PRODUCTHUNT
#1Product of theDay
Human
XAl GenerativeMusic
For your video content,podcastsand apps
Generateatracknow
MubertRender
Mubert Studio
For contentcreators
Forartists
Createasoundtrackthatwill
Earn moneyon tracks,samples
fit yourcontent'smood,
andloops.TeamupwithAl to
durationandtempo.Instantly,
producesomethingincredible. easily,perfectly.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 9}
Content: Keep Track of Your Business
Running a business is time-consuming, especially if it’s a one-person operation. Thanks 
to advancements in artificial intelligence, hiring a human to assist you is no longer 
required. Whether you need help writing a blog post, managing appointments, or 
financial tips, AI assistants can help with several tasks to help you be more productive. For example, using a financial assistant, such as Tykr, you can manage your stock 
portfolio, get insights on investment opportunities, and learn the basics of investing. ykr
Benefits
Reviews
Pricing
Courses
Webinars
Education
Support
Login
Yes,Iwanta free trial
Beating inflation
is way easier
Tykr Score
than you think
89元100
Tykr isa stockscreener and educationplatform all-
in-one that helpsyoumakebetterinvestment
This stock hasa scoreof 89/100and anMargin
decisions andbeatinflation. of Safety of 72%thereforeit'sOnSale
OnSale
5160
Yes,wantareetria
5120
5.40
No credit card required
Trustindex
Freefor14days
6Sep
16Nov
Cancel at any time
Basedon161reviews
S150.82LoW
52WChange
S180.56High

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 10}
Content: Create Avatars
AI avatar generators are all the rage right now. Thanks to companies like Picsart, Lensa AI, 
and Synthesia, you can create static and video avatars. Whether you’re looking to establish a 
more professional representation for your company, revamp your social media avatars, or even 
fashion lifelike 3D avatars for marketing videos or online business chat, these tools have you 
covered.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 10}
Content: Lensa
Get the app
Profile
Create Your Own
Magic Al Avatars
Generatepersonalized avatars infive steps
Getstarted

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 11}
Content: Help with Branding
If you’re looking to rebrand your current business or start a new venture, artificial intelligence 
can help. There are some great logo generators to spark creativity and fantastic features 
available through companies like Wix Logo Generator that will make you feel like a graphic 
design professional in no time.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 11}
Content: Whether you want to create social media posts or build out your 
brand standards, branding has never been easier. WIX
Logo Maker
TopLogoCategories
LogosbyDesign
More
christinagwira
Free Business Tools> Logo Maker
LOGOMAKER
Design a professional
logo in minutes
GetMyLogo→
G
Home
Shop
Our Story. 米
米
GNDOS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 12}
Content: Sales and Marketing Tools
If you want to create leads and grow revenue, some excellent AI sales and marketing tools help 
you take your business to the next level. These tools can help you build an email list, reach out 
to them with video assets, and explore opportunities to analyse your client interactions. For 
example, using Seamless.AI‘s built-in search engine, you can leverage the power of AI to update 
your contact lists with the most up-to-date contact information. Additionally, using software 
such as Ocoya, you can create social media posts and schedule them, giving you more time to 
focus on other aspects of your business. Seamless.Al
Products
Customers
Company
Pricing
Login
GetaDemo
GetStartedFree
TheWorld'sBest
Contacts
Companies
SalesLeads
R
Jessica Smith
Senior SalesDirector,Captive Tech
Ouralessoftwarefindsverifiedcelphones,mails,and
directdialsforanyoneyouneedtosellto.Get50freecredits
withnocreditcarddownanddiscoverwhy4o0,o00+
Mason Johnson
Creative Manager,OrgCache
companiesuseSeamless.Altogrowtheirbusiness. 501-456-9876
X
m.johnson@orgcache.com
Business Email
GetStartedFree
AshleyWilliams
Hiring Manager,HotshotDesign Co.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 12}
Content: NoCreditCard Required
★★★★★
4.2/5G2Rating
OR
SignupwithGoogle
SignupwithLinkedln
Christopher Miller
VPof Sales,City CRM
By submitting thisform,youagreeto theSeamless.AlTermsofUse&PrivacyPolicy
Trustedby4o0,0o0+users

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 13}
Content: Assist in Image Editing
If you’re a freelancer or web agency owner, you know how challenging it can be to get good 
client photos. Thankfully, artificial intelligence can turn you into a photo editing wizard while 
saving a ton of time. Whether you’re looking to upscale images, enhance them, or create 
new compositions with Photoshop, image editing tools help you finish the job quickly. E

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 14}
Content: Have In
-Depth Conversations
One of the most creative ways to use artificial intelligence is through chatbots like Writesonic. Built on OpenAI’s GPT-4, you can converse with it to get answers to questions, help you develop 
blog post ideas, and more. One of the coolest things about Writesonic is their Botsonic tool, 
which allows you to create your own chatbot in a few minutes. This is a great tool to create 
personalized chat experiences for your site’s visitors providing answers directly related to your 
products and services.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 14}
Content: Alternatively, you can incorporate Character.AI to gain a unique 
perspective by chatting with historical figures, celebrities or any personality you choose. IntroducingthesuperiorAlArticleWriter5.0
AIArticle
Craftfactual,document-awarearticlesinyourownbrandvoice
TryItOutNow
Writer5.0
inminutes. Writesonic
Features
Resources
Chatsonic
Botsonic
Pricing
Academy
Signin
Getstarted
Best Al Writer for Creating
Freelancer
Marketer
CreateSEO-optimized andplagiarism-freecontent
foryourblogs,ads,emails,andwebsitexfastr. Blogger
StartWriting ForFree
Nocreditcardrequired.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 15}
Content: Increase Productivity
No matter what you need AI for, whether it’s to get insights on how your website is 
performing or advanced SEO techniques, there are many AI productivity tools to help you 
get the job done fast. Need a good CRM? No problem. Freshworks Freddy AI can help with 
marketing automation, all while building a better system to interact with your customers. Alternatively, if you need a way to transcribe meeting notes, check out Otter AI. freshworks
Products
Platform
Resources
Demo
Pricing
FreeTrial
Freshsales
Features
Integrations
What'sNew
FREDDYAI
Redefinecustomerrelationshipswith
artificialintelligence
Getactionableinsightsacrossthecustomerjourneyanddeliver
highlypersonalized engagementwith anAl-powered CRM. SIGNUP

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 16}
Content: Develop AI Software
One of the most powerful uses of artificial intelligence is the ability to create your own. With AI development tools, you can propel your business into the future by incorporating 
systems to assist with time-consuming tasks, freeing you up to perform more important 
feats. For example, if you’re planning on starting your own international ecommerce 
marketplace, you could incorporate Google’s Translation AIto provide content based on a 
user’s location, then translating your text into the appropriate language. Google Cloud
Overview
SolutionsProductsPricing
Resources
ContactUs
α
Docs Support
English
Console
：
Startfree
Alandmachine learning products
InnovativeAl andmachine learningproducts,solutions,andservicespoweredby Google'sresearch
and technology.Newcustomersget$3oo infreecreditstorun,test,and deployworkloads. Getstartedforfree
Contact sales
GenerativeAl
Data Science
ResponsibleAl
BuildgenerativeAlapplicationsquickly,
Generateinsightsfromdatawithourcomplete
Discovertoolsandframeworkstounderstandand
efficientlyandresponsiblypoweredbyGoogles
suiteofdatamanagement,analytics,and
interpret your machine learning models.Learn
most advanced technology. machine learning tools.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 16}
Content: whyvalues-basedAlisnecessaryfordevelopers.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 17}
Content: Write Copy
One of the most notable uses for artificial intelligence is for writing copy. Using AI writing 
software such as Copy.AI, Writesonic, or ChatGPT, you can create new copy for social 
media posts, informational text, blogs, and more. Alternatively, if you need a little help making your copy more focused and concise, you can 
use a rewriter tool, such as Quillbot, to help you clarify your original content to flow better, 
all while retaining the authentic tone it was written in. QuillBot
Paraphraser
Upgrade to Premium
中
Modes:
StandardFluency
FormalSimpleCreativeExpand
Shorten
Synonyms:
目
QuillBotwill rewriteyour text.Startbywritingorpastingsomethinghereand
thenpresstheParaphrasebutton. 66
Try SampleText
PasteText
?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 17}
Content: UploadDoc
Paraphrase
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 18}
Content: Make Creative Assets
Imagery is one of the most important aspects of any website or marketing material. With 
the introduction of AI art generators in late 2022, anyone, regardless of the level of artistic 
ability, can create beautiful images for their projects. Between Adobe Firefly, Stable 
Diffusion, Midjourney, and others, there is no shortage of platforms. If you need more than 
images, these AI design tools can help you create brochures, social media templates, color
palettes, and more. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 19}
Content: Write Code
In the past, if you wanted to create a custom plugin or an entire static HTML, you’d need to 
prepare yourself for a long project. With the number of AI coding assistants hitting the 
market at the speed of light, you no longer have to spend countless hours coding on your 
own. Using software such as GitHub Copilot, you can code faster and more efficiently than 
ever before. The best part about these coding assistants is the majority of them will work 
alongside your favorite code editor, so you don’t have to spend time learning a new 
platform. Search or jump to...

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 19}
Content: Pullrequests
Issues
CodespacesMarketplaceExplore
Features
Actions
Packages
Security
Codespaces
Copilot
Codereview
Search
Issues
Discussions
YourAlpairprogrammer
GitHub Copilot uses the OpenAl Codex to suggest code and entire functions in
real-time,rightfrom your editor. GetCopilot>
Compare plans
sentiments.ts
oowrite_sql.go
parse_expenses.py
addresses.rb

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 20}
Content: Build Websites
Making a website is a lot easier now, thanks to AI website builders. Big players in the industry, 
such as Wix and Hostinger, have started offering services to streamline the process. You can 
answer a few questions through Wix and have a custom website in a few minutes. Other 
tools, like Framer, can generate a fully functional website with a text prompt. Alternatively, you can combine tools like Midjourney and ChatGPT to generate images and 
content for your new site. If you’re a WordPress user, there are even a few AI plugins to make 
working in WordPress a lot easier. VessTete
Conafi Frrunss
Det Our Reorn Wetout
Weo
Srenet Cortiee
BSOWGOSS
CAIHONSTS
FADOr
Cor
lon6m.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 20}
Content: sooeble
boliamie

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 21}
Content: Generate Video Assets
Another excellent use of artificial intelligence is AI video generators. You can create 
marketing videos from a blog post URL using Pictory. Alternatively, you can create realistic 
training videos with animated avatars using Synthesia. There are even programs, such as Runway, that will give you the tools to improve existing 
videos by adding slow-motion effects, making color enhancements, and removing artifacts. synthesia
Create a free AIvideo
Thisiswhatyourvideowilllooklike
Selectatemplate&edityourscript.Political,sexualanddiscriminatorycontent
will notbeapproved. 1
SELECTVIDEOTEMPLATE
SynthesiaDemo
SalesPitch
Learning&Development
ACMECOMPANY
Compliment
How-ToVideo
Evelyn Johnson
Jonathan
EDITYOURVIDEOSCRIPT
2
You canuse anypopularlanguage
Evelyn
HeyJohn,EvelynfromAcmehere！Aspromised,Iam
sendingyouabriefsummaryofthekeypointsofthe
Hey John,
proposalwediscussedtoday.Iwillputyouintouchwith
Evelyn fromAcmeherelAspromised,Iamsendingyou abrief
summary of thekeypointsof theproposal we discussed today. youronboardingpartnerrightaway.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 21}
Content: Iwill put you in touch withyouronboarding partner right away. -aura
Bridget
English(US)-Meticulous
Play script
3charactersleft
Kenneth
Continue

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 22}
Content: Improve SEO
The last AI tools on our list can help you get your site’s SEO up to par. With AI SEO tools, you 
can generate page titles and meta descriptions in bulk, rewrite copy, check for plagiarism, and 
develop SEO-friendly outlines, and more. Some tools, such as Alli AI, will help you quickly 
identify any SEO problem areas and provide steps to correct them. AlliAI
Features
Pricing
Company
Helpdesk
FAQ
Demo
Login
Starta Trial
Optimize, Automate,
Deploy and Scale SEO. AttentionAgenciesandSEO teams:NoCodingRequired.Workswithany
CMs.Makethousandstomillionsofcodeandcontentchangesinminutes.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 22}
Content: ManageallyourSEofromonedashboard.Installautomate,andscale. StartaFreeTrial

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 23}
Content: 
ousoNs? ?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 1.pdf', 'page': 24}
Content: 
We become what we behold. We shape our tools,
and thereafter our tools shape us. -Marshall McLuhan

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 3
1 Oct 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 1}
Content: AI: Representation and Problem Solving
Informed Search
Slide credits: CMU AI, http://ai.berkeley.edu
Mdrmil

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 2}
Content: Uninformed vs Informed Search
XD
裕

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 3}
Content: Today
•Informed Search
• Heuristics
• Greedy Search
• A* Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 4}
Content: Search Heuristics
A heuristic is:
 A function that estimates how close a state is to a goal
 Designed for a particular search problem
 Examples: Manhattan distance, Euclidean distance for 
pathing
10
5
11.2
52NOPE. GOAL! Heuristi-TronNoPE. GOAL! Heuristi-Tron

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 5}
Content: Example: Euclidean distance to Bucharest
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86
h(state)  value
Arad
366
Mehadia
241
Bucharest
0
Neamt
234
Craiova
160
Oradea
380
Drobeta
242
Pitesti
100
Eforie
161
Rimnicu Vilcea
193
Fagaras
176
Sibiu
253
Giurgiu
77
Timisoara
329
Hirsova
151
Urziceni
80
Iasi
226
Vaslui
199
Lugoj
244
Zerind
374

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 6}
Content: Effect of heuristics
Guide search towards the goal instead of all over the place
Start Goal Start Goal
Informed Uninformed


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 7}
Content: Greedy Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 8}
Content: Sibiu Fagaras
Pitesti
Rimnicu Vilcea
Bucharest
99
80
97
101
211
• Expand the node that seems closest…(order frontier by h)
• What can possibly go wrong? h=193
h= 253
h=100
h=0
h=176
Sibiu-Fagaras-Bucharest =
99+211 = 310
Sibiu-Rimnicu Vilcea-Pitesti-Bucharest =
80+97+101 = 278
1000000
Greedy Search
？!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 9}
Content: Greedy Search
• Strategy: expand a node that seems closest to 
a goal state, according to h
• Problem 1: it chooses a node even if it’s at the 
end of a very long and winding road
• Problem 2: it takes h literally even if it’s 
completely wrong
…
b


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 10}
Content: A* Search
D
裕

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 11}
Content: A* Search
UCS Greedy
A*


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 12}
Content: Combining UCS and Greedy
• Uniform-cost orders by path cost, or backward cost g(n)
• Greedy orders by goal proximity, or forward cost h(n)
• A* Search orders by the sum: f(n) = g(n) + h(n)
S a d
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0
c
h=7
3
e h=1
1
Example: Teg Grenager
S
a
b
c
d e
G d
G
g = 0 
h=6
g = 1 
h=5
g = 2 
h=6
g = 3 
h=7
g = 4 
h=2
g = 6 
h=0
g = 9 
h=1
g = 10 
h=2
g = 12 
h=0


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 13}
Content: S a d
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0
c
h=7
3
e h=1
1
Current Open list (Greedy) Close
S(6) A(5) -
A(5) E(1), D(2), B(6) S(6)
E(1) D(2), B(6) S(6), A(5)
D(2) G(0), B(6) S(6), A(5), E(1)
G(0) Goal S(6), A(5), E(1), D(2)
Path = S-A-E-D-G
Cost = 1+8+1+2 = 12
A* Search orders by the sum: f(n) = g(n) + h(n)
Current Open list (UCS) Close
S(0) A(1)
A(1) B(2), D(4), E(9) S(0)
B(2) C(3) , D(4), E(9) S(0), A(1)
C(3) D(4), E(9) S(0), A(1), B(2)
D(4) G(6), E(9) S(0), A(1), B(2), 
C(3)
G(6) = goal 
node
E(9) S(0), A(1), B(2), 
C(3)D(4)
Current Open list (A*) Close
S(0+6=6) A(1+5=6)
A(6) D(6), B(8), E(10) S(6)
D(6) G(6), B(8), E(10) S(6), A(6)
G(6) = goal node B(8), E(10) S(6), A(6), D(6)
Path = S-A-D-G
Cost = 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 14}
Content: A*
Sad
b
G
h=5
h=6
h=2
1
8
1
1
2
h=6 h=0 c
h=7
3
e h=1
1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 15}
Content: function UNIFORM-COST-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using g(n) as the priority
add initial state of problem to frontier with priority g(S) = 0
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher g(n) then
replace that frontier node with child


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 16}
Content: function A-STAR-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using f(n) = g(n) + h(n) as the priority
add initial state of problem to frontier with priority f(S) = 0 + h(S)
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher f(n) then
replace that frontier node with child


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 17}
Content: A* Search Algorithms 
• A* Tree Search 
• Same tree search algorithm but with a frontier that is a priority queue using 
priority f(n) = g(n) + h(n) 
• A* Graph Search 
• Same as UCS graph search algorithm but with a frontier that is a priority 
queue using priority f(n) = g(n) + h(n) 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 18}
Content: UCS vs A* Contours 
UCS vs A* Contours
A* expands mainly toward the 
goal, but does hedge its bets 
to ensure optimality 
Start Goal
Start Goal


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 19}
Content: Greedy Uniform Cost A*
Comparison
5
SCORE:55
C
SCORE:aiogW%
SCORE:

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 20}
Content: Is A* Optimal? • What went wrong? • Actual bad goal cost < estimated good goal cost
• We need estimates to be less than actual costs! A
G
S
1 3
h = 6
h = 0
5
h = 7


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 21}
Content: Admissible Heuristics
GETTING
YAY! CLASER... ->
Heuristi-Tron

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 22}
Content: Admissible Heuristics
• A heuristic h is admissible (optimistic) if:
0  h(n)  h*(n) 
where h*(n) is the true cost to a nearest goal
• Example:
• Coming up with admissible heuristics is most of what’s 
involved in using A* in practice. 15
52

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 23}
Content: Optimality of A* Tree Search
巧

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 24}
Content: A* Tree Search
S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
S (0+2)
A (1+4)
C (2+1)
G (5+0)
C (3+1)
G (6+0)
State space graph Search tree


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 25}
Content: Optimality of A* Tree Search
Assume:
• A is an optimal goal node
• B is a suboptimal goal node
• h is admissible
Claim:
• A will be chosen for exploration (popped off the frontier) before B
…
A
B


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 26}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, 
too (Maybe the start state; maybe A
itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
f(n) = g(n) + h(n) Definition of f-cost
f(n)  g(A) Admissibility of h
…
g(A) = f(A) h = 0 at a goal
A
B
n


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 27}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, 
too (Maybe the start state; maybe A
itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
2. f(A) is less than f(B)
…
A
B
n
g(A) < g(B) Suboptimality of B
f(A) < f(B) h = 0 at a goal


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 28}
Content: Optimality of A* Tree Search: Blocking
Proof:
• Imagine B is on the frontier
• Some ancestor n of A is on the frontier, too 
(Maybe the start state; maybe A itself!)
• Claim: n will be explored before B
1. f(n) is less than or equal to f(A)
2. f(A) is less than f(B)
3. n is explored before B
• All ancestors of A are explored before B
• A is explored before B
• A* search is optimal
…
A
B
n
f(n)  f(A) < f(B) 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 29}
Content: Optimality of A* Graph Search
巧

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 30}
Content: S
A
C
G
1
3
1
3
What paths does A* graph search consider during its search? A) S, S-A, S-C, S-C-G
C) S, S-A, S-A-C, S-A-C-G
D) S, S-A, S-C, S-A-C, S-A-C-G
B) S, S-A, S-C, S-A-C, S-C-G
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 31}
Content: S
A
C
G
1
3
1
3
What paths does A* graph search consider during its search? A) S, S-A, S-C, S-C-G
C) S, S-A, S-A-C, S-A-C-G
D) S, S-A, S-C, S-A-C, S-A-C-G
B) S, S-A, S-C, S-A-C, S-C-G
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 32}
Content: A* Graph Search
S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
What does the resulting graph tree look like? S
A
C
G
S
A
C
G
A)
B)
C & D)
S
A
C
G


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 33}
Content: A* Graph Search Gone Wrong? S
A
C
G
1
3
1
3
h=2
h=4
h=1
h=0
S (0+2)
A (1+4) C (3+1)
G (6+0)
State space graph Search tree
Simple check against explored set blocks C
Fancy check allows new C if cheaper than old
but requires recalculating C’s descendants


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 34}
Content: Admissibility of Heuristics
• Main idea: Estimated heuristic values ≤ actual costs
• Admissibility:
heuristic value ≤ actual cost to goal
h(A) ≤ actual cost from A to G
3
A
C
G
h=4
1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 35}
Content: Consistency of Heuristics
• Main idea: Estimated heuristic costs ≤ actual costs
• Admissibility:
heuristic cost ≤ actual cost to goal
h(A) ≤ actual cost from A to G
• Consistency:
“heuristic step cost” ≤ actual cost for each step
h(A) – h(C) ≤ cost(A to C)
triangle inequality
h(A) ≤ cost(A to C) + h(C)
• Consequences of consistency:
• The f value along a path never decreases
• A* graph search is optimal
A
C
G
h=4 h=1
1
h=2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 36}
Content: Optimality of A* Graph Search
• Sketch: consider what A* does with a 
consistent heuristic:
• Fact 1: In tree search, A* expands nodes 
in increasing total f value (f-contours)
• Fact 2: For every state s, nodes that 
reach s optimally are explored before 
nodes that reach s suboptimally
• Result: A* graph search is optimal
…
f  3
f  2
f  1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 37}
Content: Optimality
• Tree search:
• A* is optimal if heuristic is admissible
• UCS is a special case (h = 0)
• Graph search:
• A* optimal if heuristic is consistent
• UCS optimal (h = 0 is consistent)
• Consistency implies admissibility
• In general, most natural admissible heuristics tend to be 
consistent, especially if from relaxed problems
巧

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 38}
Content: Creating Heuristics
YOUGOT
HEURISTIC
UPGRRDE!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 39}
Content: Creating Admissible Heuristics
• Most of the work in solving hard search problems optimally is in 
coming up with admissible heuristics
• Often, admissible heuristics are solutions to relaxed problems, where 
new actions are available
15
366
52Straight-line distance
Oradea
to Bucharest
71
Neamt
Arad
366
口
87
Bucharest
0
Zerind
151
75
Craiova
160
lasi
Dobreta
242
Arad
140
Eforie
161
92
Fagaras
Sibiu
178
99
Fagaras
Giurgiu
118
77
口
Vaslui
Hirsova
80
151
Iasi
226
RimnicuVilcea
Timisoara
Lugoj
244
142
Mehadia
241
11↑
211
Pitesti
Neamt
97
234
Lugoj
Oradea
380
70
98
Pitesti
98
146
101
85
口
Hirsova
Mehadia
Urziceni
Rimnicu Vilcea
■
193
75
86
Sibiu
138
253
Bucharest
Timisoara
120
329
Dobreta
90
口
Urziceni
80
■
Craiova
Eforie
Vaslui
199
Giurgiu
Zerind
374

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 40}
Content: Example: 8 Puzzle
• What are the states? • How many states? • What are the actions? • How many actions from the start state? • What should the step costs be? Start State Actions Goal State
3
7
1
2
4
5
8
672
4
5
6
83
11
2
34
5
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 41}
Content: 8 Puzzle I
• Heuristic: Number of tiles misplaced
• Why is it admissible? • h(start) =
• This is a relaxed-problem heuristic
8
Average nodes expanded when 
the optimal path has…
…4 steps …8 steps …12 steps
UCS 112 6,300 3.6 x 106
A*TILES 13 39 227
Start State Goal State
Statistics from Andrew Moore
1
5
+
87
2
4
1
2
34
5
6
5
83
1
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 42}
Content: 8 Puzzle II
• What if we had an easier 8-puzzle 
where any tile could slide any 
direction at any time, ignoring 
other tiles? • Total Manhattan distance
• Why is it admissible? • h(start) = 3 + 1 + 2 + … = 18
Average nodes expanded when 
the optimal path has…
…4 steps …8 steps …12 steps
A*TILES 13 39 227
A*MANHATTAN 12 25 73
Start State Goal State
7
2
4
1
2
34
5
6
5
83
1
6
7
8

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 43}
Content: Combining heuristics
• Dominance: ha ≥ hcif 
n ha(n)  hc(n)
• Roughly speaking, larger is better as long as both are admissible
• The zero heuristic is pretty bad (what does A* do with h=0?)
• The exact heuristic is pretty good, but usually too expensive! • What if we have two heuristics, neither dominates the other?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 43}
Content: • Form a new heuristic by taking the max of both:
h(n) = max( ha(n), hb(n) )
• Max of admissible heuristics is admissible and dominates both! 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 44}
Content: A*: Summary


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 45}
Content: A*: Summary
• A* uses both backward costs and (estimates of) forward costs
• A* is optimal with admissible / consistent heuristics
• Heuristic design is key: often use relaxed problems
D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 46}
Content: In-Class Activity 
• Q1: Practice creating heuristics and running Greedy and A* search 
• Q2: Walk through Amazon Robot Example 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 47}
Content: • Notes added on LMS
• https://www.oreilly.com/library/view/graph-algorithms/9781492047674/ch04.html
Recommended
Reading

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 3.pdf', 'page': 48}
Content: 
ousoNs? ?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 6
21st Oct 2023
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 1}
Content: AI: Representation and 
Problem Solving
Local Search
Slide credits: Pat Virtue, http://ai.berkeley.edu


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 2}
Content: Outline
• Beam Search
• Local Beam Search & Variants of Beam Search
• Grid Search
• Simulated Annealing


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 3}
Content: Beam Search
• Search Algorithms like BFS, DFS and A* etc. are 
infeasible on large search spaces. • Beam Search was developed in an attempt to 
achieve the optimal(or sub-optimal) solution 
without consuming too much memory. • It is used in many machine translation systems. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 4}
Content: Where to use Beam Search? • In many problems path is irrelevant, we are only 
interested in a solution (e.g. 8-queens problem)
• This class of problems includes 
Integrated-circuit design
Factory-floor layout
Job scheduling
Network optimization
Vehicle routing
Traveling salesman problem
Machine translation


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 5}
Content: N-queens problem
• Put n queens on an n × n board with no two 
queens sharing a row, column, or diagonal
• move a queen to reduce number of conflicts. • Solves n-queens problem very quickly for very 
large n. 业
业
业
业
业
业
业
业
h=5
h=2
h=0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 6}
Content: Machine Translation
• To select the best translation, each part is 
processed. • Many different ways of translating the words 
appear. • The top best translations according to their 
sentence structures are kept. • The rest are discarded. • The translator then evaluates the translations 
according to a given criteria.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 6}
Content: • Choosing the translation which best keeps the 
goals. • The first use of a beam search was in the Harpy 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 7}
Content: Beam Search
• Is heuristic approach where only the most 
promising ß nodes (instead of all nodes) at 
each step of the search are retained for 
further branching. • ß is called Beam Width. • Beam search is an optimization of best-first 
search that reduces its memory requirements. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 8}
Content: Beam Search Algorithm
OPEN = {initial state} 
while OPEN is not empty do 
1. Remove the best node from OPEN, call it n. 2. If n is the goal state, backtrace path to n 
(through recorded parents) and return path. 3. Create n's successors. 4. Evaluate each successor, add it to OPEN, and 
record its parent. 5. If |OPEN| > ß , take the best ß nodes 
(according to heuristic) and remove the others 
from the OPEN. done


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 9}
Content: Example of Beam Search
• 4-queen puzzle
• Initially, randomly put queens in each column
• h = no. of conflicts
• Let ß = 1,and proceed as given below
业
业
业
业
业
业
业
业
h=5
h=2
h=0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 10}
Content: Beam Search vs. A*
• In 48-tiles Puzzle, A* may run 
out of memory since the space 
requirements can go up to order 
of 1061. • Experiment conducted shows that 
beam search with a beam width 
of 10,000 solves about 80% of 
random problem instances of the 
48-Puzzle (7x7 tile puzzle). 3
4
2
1
8
6
7
5
12
10
11
9
131415

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 11}
Content: Completeness of Beam 
Search
• In general, the Beam Search Algorithm is not 
complete. • Even given unlimited time and memory, it is 
possible for the Algorithm to miss the goal node 
when there is a path from the start node to the 
goal node (example in next slide). • A more accurate heuristic function and a larger 
beam width can improve Beam Search's chances of 
finding the goal.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 11}
Content: 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 12}
Content: Example with ß=2
Steps: 
1. OPEN= {A}
H=1 H= 3 2. OPEN= {B,C}
3. OPEN={D,E}
4. OPEN={E}
5. OPEN={}
H=2 H=2 H=3 H=0
Clearly, open set becomes empty without finding goal node . With ß = 3, the algorithm succeeds to find goal node. B
A
C
D E F G


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 13}
Content: Optimality
• Just as the Algorithm is not complete, it is 
also not guaranteed to be optimal. • This can happen because the beam width and an 
inaccurate heuristic function may cause the 
algorithm to miss expanding the shortest path. • A more precise heuristic function and a larger 
beam width can make Beam Search more likely to 
find the optimal path to the goal. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 14}
Content: Example with ß=2
Steps: 
2 1 2 1. OPEN= {A}
h=1 h=2 h=3 
2. OPEN= {B,C}
3. OPEN={C,E}
3 2 3
4. OPEN={F,E}
5. OPEN={G,E} 
h=3 h=1 
6. found goal node, stop. 4 3 
Path : A->C->F->G
B
A
C
E F
G
D


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 15}
Content: Time Complexity
• Depends on the accuracy of the heuristic 
function. • In the worst case, the heuristic function leads 
Beam Search all the way to the deepest level in 
the search tree. • The worst case time = O(B*m)
where B is the beam width and m is the 
maximum depth of any path in the search tree.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 15}
Content: 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 16}
Content: Space Complexity
• Beam Search's memory consumption is its most 
desirable trait. • Since the algorithm only stores B nodes at each 
level in the search tree,
the worst-case space complexity = 
O(B*m)
where B is the beam width, and m is the 
maximum depth of any path in the search tree. • This linear memory consumption allows Beam 
Search to probe very deeply into large search 
spaces and potentially find solutions that 
other algorithms cannot reach.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 16}
Content: 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 17}
Content: Applications of Beam Search 
• Job Scheduling - early/tardy scheduling 
problem 
• Phrase-Based Translation Model 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 18}
Content: Local Beam Search
• Local beam search is a cross between beam 
search and local search ( special case of beam 
search β =1). • Only the most promising ß nodes at each level 
of the search tree are selected for further 
branching. • remaining nodes are pruned off permanently. • only ß nodes are retained at each level, the 
running time is polynomial in the problem size. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 19}
Content: Variants in Beam Search
• Flexible Beam Search:
In case more than one child nodes have same heuristic 
value and one or more are included in the top B 
nodes, then all such nodes are included too. Increases the beam width temporarily. • Recovery Beam Search 
• Beam Stack Search
• BULB (Beam Search Using Limited Discrepancy 
Backtracking)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 6.pdf', 'page': 20}
Content: Conclusion
• A beam search is most often used to 
maintain tractability in large systems 
with insufficient amount of memory to 
store the entire search tree. • Used widely in machine translation 
systems. • Beam Search is neither complete nor 
optimal. • Despite these disadvantages, beam search 
has found success in the practical areas 
of speech recognition, vision, planning, 
and machine learning (Zhang, 1999). 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 7
24th Oct 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 1}
Content: AI: Representation and 
Problem Solving
Local Search
Slide credits: Pat Virtue, http://ai.berkeley.edu


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 2}
Content: Hill climbing 
Initialposition
SimulatedAnnealingexplores
of theball
more.Choosesthismovewitha
smallprobability(HillClimbing)
GreedyAlgorithm
gets stuckhere! Locally Optimum
Solution. Upon a large no.ofiterations,
SA converges to thissolution.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 3}
Content: Hill climbing 
Hillclimb
B
Fitness

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 4}
Content: Simulated Annealing 
Solutionspace
LocalSearch
Costfunction

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 5}
Content: Annealing
• Annealing is a thermal process for obtaining 
low energy states of a solid in a heat bath. • The process contains two steps: 
• Increase the temperature of the heat bath to a 
maximum value at which the solid melts. • Decrease carefully the temperature of the heat bath 
until the particles arrange themselves in the ground 
state of the solid. Ground state is a minimum energy 
state of the solid. • The ground state of the solid is obtained only 
if the maximum temperature is high enough and 
the cooling is done slowly. https://www.youtube.com/watch?v=9kWWKgiBh-Q


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 6}
Content: 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 7}
Content: Simulated Annealing 
• To apply simulated annealing with optimization 
purposes we require the following:
• A successor function that returns a “close” 
neighboring solution given the actual one. This will 
work as the “disturbance” for the particles of the 
system. • A target function to optimize that depends on the 
current state of the system. This function will work 
as the energy of the system. • The search is started with a randomized state. In a polling loop we will move to neighboring
states always accepting the moves that decrease 
the energy while only accepting bad moves 
accordingly to a probability distribution 
dependent on the “temperature” of the system. 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 8}
Content: Simulated Annealing 
• Decrease the temperature slowly, accepting 
less bad moves at each temperature level until 
at very low temperatures the algorithm 
becomes a greedy hill-climbing algorithm. • The distribution used to decide if we accept a 
bad movement is know as Boltzman
distribution. • This distribution is very well known is in solid 
physics and plays a central role in simulated 
annealing.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 8}
Content: Where γ is the current configuration 
of the system, E γ is the energy related with it, 
and Z is a normalization constant. L/g-a
-E/T
(L)Z
Q)
= (c)d

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 9}
Content: Annealing Process 
• Raising the temperature up to a very high level 
(melting temperature, for example), the atoms 
have a higher energy state and a high 
possibility to re-arrange the crystalline 
structure. • Cooling down slowly, the atoms have a lower and 
lower energy state and a smaller and smaller 
possibility to re-arrange the crystalline 
structure.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 9}
Content: 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 10}
Content: Annealing - Analogy
OptimizationProblem
Iterationimprovement
Simulatedannealing
Optimalsolution
Costfunction
Solution
PhysicalSystem
State(configuration)
RapidQuenching
CarefulAnnealing
GroundState
Energy

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 11}
Content: Simulated Annealing - Analogy
Global optimal solution can be achieved as long as the 
cooling process is slow enough. - A completely ordered crystalline structure
→the optimal solution for the problem
Temperature→ControlParameter
-Energy State→ Cost Function
Metal→Problem
一

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 12}
Content: Simulate
d 
Annealin
g Flow 
Chart
Input Function & Initial Solution
GenerateNewSolution
AcceptNew
No
Solution? Yes
Update Stored Values
AdjustTemperature
Stopping
No
Criterion
Satisfied? Yes
End

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 13}
Content: How SA explores the solution 
space
Generaterandomsolution
Fitness
FeasibleSolutions
MakeAGIF.com

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 14}
Content: Conclusion
• SA is a global optimization technique. • SA distinguishes between different local optima. • SA is a memory less algorithm, the algorithm does not use any 
information gathered during the search
• SA is motivated by an analogy to annealing in solids. • Simulated Annealing - an iterative improvement algorithm


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 15}
Content: Solving 
Sudku 
using SA 
3
8
1
2
2
1
3
6
4
2
4
8
9
1
6
6
5
7
2
4
9
5
9
9
4
8
7
5
6
1
7
3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 16}
Content: 
Steps for solving Sudoku
1- Generating random states
5
2
4
4
3
7
6
9
6
7
9
I
5
9
2
7
3
8
1
3
6
8
2
4
1
5
4
3
1
6
7
5
4
8
1
5
8
6
1
8
9
9
3
2
7
9
2
2
3
4
7
6
5
2
8
9
7
1
5
8
4
2
6
4
5
2
9
3
9
1
3
3
1
7
8
6
4
7
5
6

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 17}
Content: 
Steps for solving
Sudoku
SimulatedAnnealing Explained By Solving
1- Generating random
Sudoku-Artificial Intelligence
states
5
2
4
4
3
7
6
9
2- Writing a cost function
2
6
9
7
I
5
9
2
7
2
8
1
3
6
8
2
4
1
5
1
4
3
I
6
7
5
4
8
1
2
5
8
6
1
8
9
9
3
2
2
7
9
2
2
3
4
7
6
5
2
2
8
9
7
1
5
4
2
2
6
4
5
2
9
3
9
1
3
2
3
I
7
8
6
4
7
5
6
2
3132332
3
Total cost = 38

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 18}
Content: 
Steps for solving Sudoku
1- Generating random states
5
4
4
7
6
9
2- Writing a cost function
2
9
9
7
T
5
9
2
7
3
3- Selecting starting temperature
2
8
1
3
6
8
2
4
1
5
1
4
3
I
6
7
5
4
8
1
2
5
6
9
9
3
2
2
7
9
2
2
4
7
6
5
2
2
9
7
1
5
4
2
2
6
4
5
2
3
2
3
7
Simulated Annealing Explained BySolving
I
Sudoku-Artificial Intelligence
6
士
2
3132332
Total cost = 38

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 19}
Content: 
mulatedAnnealingExplainedBySolving
udoku-Artificial Intelligence
Steps for solving Sudoku
1- Generating random states
5
4
4
3
7
6
U
2- Writing a cost function
2
6
9
7
1
5
9
2
7
3
3- Selecting starting temperature
2
1
3
6
8
4
1
5
4- Calculating iterations per T
1
4
1
6
7
5
4
1
2
5
8
6
1
8
9
9
3
2
2
7
9
2
2
3
7
6
5
2
2
8
9
7
1
5
4
2
6
4
5
2
9
3
9
1
3
2
3
I
7
8
6
4
7
5
6
2
3
１3233２
3
Total cost = 38

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI - Lec 7.pdf', 'page': 20}
Content: 
Steps f
for solving
Sudoku
1- Generating random states
5
2
4
4
3
7
6
9
2- Writing a cost function
2
9
9
7
1
5
9
2
7
3
3- Selecting starting
temperature
2
8
1
3
6
8
2
4
1
5
Calculating iterations
T
4-
per
I
4
3
1
6
7
5
4
1
X
SimulatedAnnealingExplainedBy Solving
Sudoku-Artificial Intelligence
2
5- Choosing
5
6
8
1
3
g a cooling rate
U
2
7
9
2
2
3
7
6
5
4
2
2
8
9
7
1
5
Q
4
2
2
5
4
2
9
3
I
3
2
3
1
7
7
5
0
2
3132332
Total cost = 38

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 8
Markov Models
In previous notes, we talked about Bayes’ nets and how they are a wonderful structure used for compactly
representing relationships between random variables. We’ll now cover a very intrinsically related structure
called a Markov model, which for the purposes of this course can be thought of as analogous to a chainlike, infinite-length Bayes’ net. The running example we’ll be working with in this section is the day-to-day
fluctuations in weather patterns.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 0}
Content: Our weather model will be time-dependent (as are Markov models in
general), meaning we’ll have a separate random variable for the weather on each day. If we define Wi as the
random variable representing the weather on day i, the Markov model for our weather example would look
like this:
What information should we store about the random variables involved in our Markov model? To track
how our quantity under consideration (in this case, the weather) changes over time, we need to know both
it’s initial distribution at time t = 0 and some sort of transition model that characterizes the probability
of moving from one state to another between timesteps. The initial distribution of a Markov model is
enumerated by the probability table given by Pr(W0) and the transition model of transitioning from state i to
i+1 is given by Pr(Wi+1|Wi). Note that this transition model implies that the value of Wi+1 is conditionally
dependent only on the value of Wi. In other words, the weather at time t = i + 1 satisfies the Markov
property or memoryless property, and is independent of the weather at all other timesteps besides t = i. Using our Markov model for weather, if we wanted to reconstruct the joint between W0, W1, and W2 using
the chain rule, we would want:
Pr(W0,W1,W2) = Pr(W0)Pr(W1|W0)Pr(W2|W1,W0)
However, with our assumption that the Markov property holds true and W0 |=W2|W1, the joint simplifies to:
Pr(W0,W1,W2) = Pr(W0)Pr(W1|W0)Pr(W2|W1)
And we have everything we need to calculate this from the Markov model. More generally, Markov models
make the following independence assumption at each timestep: Wi+1 |={W0,...,Wi1}|Wi. This allows us to
reconstruct the joint distribution for the first n+1 variables via the chain rule as follows:
Pr(W0,W1,...,Wn) = Pr(W0)Pr(W1|W0)Pr(W2|W1)...Pr(Wn|Wn1) = Pr(W0)
n1
’
i=0
Pr(Wi+1|Wi)
A final assumption that’s typically made in Markov models is that the transition model is stationary. In
other words, for all values of i, Pr(Wi+1|Wi) is identical. This allows us to represent a Markov model with
only two tables: one for Pr(W0) and one for Pr(Wi+1|Wi). CS 188, Fall 2018, Note 8 1
M
W1
W2
W3
?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 1}
Content: The Mini-Forward Algorithm
We now know how to compute the joint distribution across timesteps of a Markov model. However, this
doesn’t explicitly help us answer the question of the distribution of the weather on some given day t. Naturally, we can compute the joint then marginalize (sum out) over all other variables, but this is typically
extremely inefficient, since if we have j variables each of which can take on d values, the size of the joint
distribution is O(d j). Instead, we’ll present a more efficient technique called the mini-forward algorithm. Here’s how it works. By properties of marginalization, we know that
Pr(Wi+1) = ÂwiPr(wi,Wi+1)
By the chain rule we can reexpress this as follows:
Pr(Wi+1) = ÂwiPr(Wi+1|wi)Pr(wi)
This equation should make some intuitive sense - to compute the distribution of the weather at timestep i+1,
we look at the probability distribution at timestep i given by Pr(Wi) and "advance" this model a timestep
with our transition model Pr(Wi+1|Wi). With this equation, we can iteratively compute the distribution of the
weather at any timestep of our choice by starting with our initial distribution Pr(W0) and using it to compute
Pr(W1), then in turn using Pr(W1) to compute Pr(W2), and so on. Let’s walk through an example, using the
following initial distribution and transition model:
W0 Pr(W0)
sun 0.8
rain 0.2
Wi+1 Wi Pr(Wi+1|Wi)
sun sun 0.6
rain sun 0.4
sun rain 0.1
rain rain 0.9
Using the mini-forward algorithm we can compute Pr(W1) as follows:
Pr(W1 = sun) = Âw0Pr(W1 = sun|w0)Pr(w0)
= Pr(W1 = sun|W0 = sun)Pr(W0 = sun) +Pr(W1 = sun|W0 = rain)Pr(W0 = rain)
= 0.6 · 0.8+0.1 · 0.2 = 0.5
Pr(W1 = rain) = Pr(W1 = rain|w0)Pr(w0)
= Pr(W1 = rain|W0 = sun)Pr(W0 = sun) +Pr(W1 = rain|W0 = rain)Pr(W0 = rain)
= 0.4 · 0.8+0.9 · 0.2 = 0.5
Hence our distribution for Pr(W1) is
W1 Pr(W1)
sun 0.5
rain 0.5
Notably, the probability that it will be sunny has decreased from 80% at time t = 0 to only 50% at time t = 1. This is a direct result of our transition model, which favors transitioning to rainy days over sunny days. This
gives rise to a natural follow-up question: does the probability of being in a state at a given timestep ever
converge? We’ll address the answer to this problem in the following section.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 1}
Content: CS 188, Fall 2018, Note 8 2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 2}
Content: Stationary Distribution
To solve the problem stated above, we must compute the stationary distribution of the weather. As the
name suggests, the stationary distribution is one that remains the same after the passage of time, i.e. Pr(Wt+1) = Pr(Wt)
We can compute these converged probabilities of being in a given state by combining the above equivalence
with the same equation used by the mini-forward algorithm:
Pr(Wt+1) = Pr(Wt) = ÂwtPr(Wt+1|wt)Pr(wt)
For our weather example, this gives us the following two equations:
Pr(Wt = sun) = Pr(Wt+1 = sun|Wt = sun)Pr(Wt = sun) +Pr(Wt+1 = sun|Wt = rain)Pr(Wt = rain)
= 0.6 ·Pr(Wt = sun) +0.1 ·Pr(Wt = rain)
Pr(Wt = rain) = Pr(Wt+1 = rain|Wt = sun)Pr(Wt = sun) +Pr(Wt+1 = rain|Wt = rain)Pr(Wt = rain)
= 0.4 ·Pr(Wt = sun) +0.9 ·Pr(Wt = rain)
We now have exactly what we need to solve for the stationary distribution, a system of 2 equations in 2
unknowns! We can get a third equation by using the fact that Pr(Wt) is a probability distribution and so
must sum to 1:
Pr(Wt = sun) = 0.6 ·Pr(Wt = sun) +0.1 ·Pr(Wt = rain)
Pr(Wt = rain) = 0.4 ·Pr(Wt = sun) +0.9 ·Pr(Wt = rain)
1 = Pr(Wt = sun) +Pr(Wt = rain)
Solving this system of equations yields Pr(Wt = sun) = 0.2 and Pr(Wt = rain) = 0.8. Hence the table for
our stationary distribution, which we’ll henceforth denote as Pr(W•), is the following:
W• Pr(W•)
sun 0.2
rain 0.8
To verify this result, let’s apply the transition model to the stationary distribution:
Pr(W•+1 = sun) = Pr(W•+1 = sun|W• = sun)Pr(W• = sun) +Pr(W•+1 = sun|W• = rain)Pr(W• = rain)
= 0.6 · 0.2+0.1 · 0.8 = 0.2
Pr(W•+1 = rain) = Pr(W•+1 = rain|W• = sun)Pr(W• = sun) +Pr(W•+1 = rain|W• = rain)Pr(W• = rain)
= 0.4 · 0.2+0.9 · 0.8 = 0.8
As expected, Pr(W•+1) = Pr(W•). In general, if Wt had a domain of size k, the equivalence
Pr(Wt) = ÂwtPr(Wt+1|wt)Pr(wt)
yields a system of k equations, which we can use to solve for the stationary distribution.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 2}
Content: CS 188, Fall 2018, Note 8 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 3}
Content: Hidden Markov Models
With Markov models, we saw how we could incorporate change over time through a chain of random variables. For example, if we want to know the weather on day 10 with our standard Markov model from above,
we can begin with the initial distribution Pr(W0) and use the mini-forward algorithm with our transition
model to compute Pr(W10). However, between time t = 0 and time t = 10, we may collect new meteorological evidence that might affect our belief of the probability distribution over the weather at any given
timestep. In simpler terms, if the weather forecasts an 80% chance of rain on day 10, but there are clear
skies on the night of day 9, that 80% probability might drop drastically. This is exactly what the Hidden
Markov Model helps us with - it allows us to observe some evidence at each timestep, which can potentially
affect the belief distribution at each of the states. The Hidden Markov Model for our weather model can be
described using a Bayes’ net structure that looks like the following:
Unlike vanilla Markov models, we now have two different types of nodes. To make this distinction, we’ll
call each Wi a state variable and each weather forecast Fi an evidence variable. Since Wi encodes our belief
of the probability distribution for the weather on day i, it should be a natural result that the weather forecast
for day i is conditionally dependent upon this belief. The model implies similar conditional indepencence
relationships as standard Markov models, with an additional set of relationships for the evidence variables:
F1 |=
W0|W1
8i = 2,...,n; Wi |={W0,...,Wi2,F1,...,Fi1}|Wi1
8i = 2,...,n; Fi |={W0,...,Wi1,F1,...,Fi1}|Wi
Just like Markov models, Hidden Markov Models make the assumption that the transition model Pr(Wi+1|Wi)
is stationary. Hidden Markov Models make the additional simplifying assumption that the sensor model
Pr(Fi|Wi) is stationary as well.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 3}
Content: Hence any Hidden Markov Model can be represented compactly with just
three probability tables: the initial distribution, the transition model, and the sensor model. As a final point on notation, we’ll define the belief distribution at time i with all evidence F1,...,Fi observed
up to date:
B(Wi) = Pr(Wi| f1,..., fi)
Similarly, we’ll define B0(Wi) as the belief distribution at time i with evidence f1,..., fi1 observed:
B0(Wi) = Pr(Wi| f1,..., fi1)
Defining ei as evidence observed at timestep i, you might sometimes see the aggregated evidence from
timesteps 1  i  t reexpressed in the following form:
e1:t = e1,..., et
CS 188, Fall 2018, Note 8 4
Wo
W1
W2
W3
F1
F2
F3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 4}
Content: Under this notation, Pr(Wi| f1,..., fi1) can be written as Pr(Wi| f1:(i1)). This notation will become relevant
in the upcoming sections, where we’ll discuss time elapse updates that iteratively incorporate new evidence
into our weather model. The Forward Algorithm
Using the conditional probability assumptions stated above and marginalization properties of conditional
probability tables, we can derive a relationship between B(Wi) and B0(Wi+1) that’s of the same form as the
update rule for the mini-forward algorithm. We begin by using marginalization:
B0(Wi+1) = Pr(Wi+1| f1,..., fi) = ÂwiPr(Wi+1,wi| f1,..., fi)
This can be reexpressed then with the chain rule as follows:
B0(Wi+1) = Pr(Wi+1| f1,..., fi) = ÂwiPr(Wi+1|wi, f1,..., fi)Pr(wi| f1,..., fi)
Noting that Pr(wi| f1,..., fi) is simply B(wi) and that Wi+1 |={ f1,...

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 4}
Content: fi}|Wi, this simplies to our final relationship between B(Wi) to B0
(Wi+1):
B0(Wi+1) = ÂwiPr(Wi+1|wi)B(wi)
Now let’s consider how we can derive a relationship between B0(Wi+1) and B(Wi+1). By simple application
of Bayes’ rule, we can see that
B(Wi+1) = Pr(Wi+1| f1,..., fi+1) = Pr(Wi+1, fi+1| f1,..., fi)
Pr(fi+1| f1,..., fi)
When dealing with conditional probabilities a commonly used trick is to delay normalization until we require
the normalized probabilities, a trick we’ll now employ. More specifically, since the denominator in the
above expansion of B(Wi+1) is common to every term in the probability table represented by B(Wi+1), we
can omit actually dividing by Pr(fi+1| f1,..., fi). Instead, we can simply note that B(Wi+1) is proportional
to Pr(Wi+1, fi+1| f1,..., fi):
B(Wi+1) µ Pr(Wi+1, fi+1| f1,..., fi)
with a constant of proportionality equal to Pr(fi+1| f1,..., fi). Whenever we decide we want to recover the
belief distribution B(Wi+1), we can divide each computed value by this constant of proportionality. Now,
using the chain rule we can observe the following:
B(Wi+1) µ Pr(Wi+1, fi+1| f1,..., fi) = Pr(fi+1|Wi+1, f1,..., fi)Pr(Wi+1| f1,..., fi)
By the conditional independence assumptions associated with Hidden Markov Models stated previously,
Pr(fi+1|Wi+1, f1,..., fi)is equivalent to simply Pr(fi+1|Wi+1) and by definition Pr(Wi+1| f1,..., fi) = B0(Wi+1). This allows us to express the relationship between B0(Wi+1) and B(Wi+1) in it’s final form:
B(Wi+1) µ Pr(fi+1|Wi+1)B0(Wi+1)
Combining the two relationships we’ve just derived yields an iterative algorithm known as the forward
algorithm, the Hidden Markov Model analog of the mini-forward algorithm from earlier:
B(Wi+1) µ Pr(fi+1|Wi+1)ÂwiPr(Wi+1|wi)B(wi)
CS 188, Fall 2018, Note 8 5


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 5}
Content: The forward algorithm can be thought of as consisting of two distinctive steps: the time elapse update
which corresponds to determining B0(Wi+1) from B(Wi) and the observation update which corresponds to
determining B(Wi+1) from B0(Wi+1). Hence, in order to advance our belief distribution by one timestep
(i.e.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 5}
Content: compute B(Wi+1) from B(Wi)), we must first advance our model’s state by one timestep with the time
elapse update, then incorporate new evidence from that timestep with the observation update. Consider the
following initial distribution, transition model, and sensor model:
W0 B(W0)
sun 0.8
rain 0.2
Wi+1 Wi Pr(Wi+1|Wi)
sun sun 0.6
rain sun 0.4
sun rain 0.1
rain rain 0.9
Fi Wi Pr(Fi|Wi)
good sun 0.8
bad sun 0.2
good rain 0.3
bad rain 0.7
To compute B(W1), we begin by performing a time update to get B0(W1):
B0(W1 = sun) = Âw0Pr(W1 = sun|w0)B(w0)
= Pr(W1 = sun|W0 = sun)B(W0 = sun) +Pr(W1 = sun|W0 = rain)B(W0 = rain)
= 0.6 · 0.8+0.1 · 0.2 = 0.5
B0(W1 = rain) = Âw0Pr(W1 = rain|w0)B(w0)
= Pr(W1 = rain|W0 = sun)B(W0 = sun) +Pr(W1 = rain|W0 = rain)B(W0 = rain)
= 0.4 · 0.8+0.9 · 0.2 = 0.5
Hence:
W1 B0(W1)
sun 0.5
rain 0.5
Next, we’ll assume that the weather forecast for day 1 was good (i.e. F1 = good), and perform an observation
update to get B(W1):
B(W1 = sun) µ Pr(F1 = good|W1 = sun)B0(W1 = sun) = 0.8 · 0.5 = 0.4
B(W1 = rain) µ Pr(F1 = good|W1 = rain)B0(W1 = rain) = 0.3 · 0.5 = 0.15
The last step is to normalize B(W1), noting that the entries in table for B(W1) sum to 0.4+0.15 = 0.55:
B(W1 = sun) = 0.4/0.55 = 8
11
B(W1 = rain) = 0.15/0.55 = 3
11
Our final table for B(W1) is thus the following:
W1 B0(W1)
sun 8/11
rain 3/11
CS 188, Fall 2018, Note 8 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 6}
Content: Note the result of observing the weather forecast. Because the weatherman predicted good weather, our
belief that it would be sunny increased from 1
2 after the time update to 811 after the observation update. As a parting note, the normalization trick discussed above can actually simplify computation significantly
when working with Hidden Markov Models. If we began with some initial distribution and were interested
in computing the belief distribution at time t, we could use the forward algorithm to iteratively compute
B(W1),...,B(Wt) and normalize only once at the end by dividing each entry in the table for B(Wt) by the
sum of it’s entries. Viterbi Algorithm
In the Forward Algorithm, we used recursion to solve for P(XN|e1:N), the probability distribution over
states the system could inhabit given the evidence variables observed so far. Another important question related to Hidden Markov Models is: What is the most likely sequence of hidden states the system followed given the observed evidence variables so far? In other words, we would like to solve for
argmaxx1:N P(x1:N|e1:N) = argmaxx1:N P(x1:N, e1:N). This trajectory can also be solved for using dynamic
programming with the Viterbi algorithm. The algorithm consists of two passes: the first runs forward in time and computes the probability of the
best path to that (state, time) tuple given the evidence observed so far.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 6}
Content: The second pass runs backwards
in time: first it finds the terminal state that lies on the path with the highest probability, and then traverses
backward through time along the path that leads into this state (which must be the best path). To visualize the algorithm, consider the following state trellis, a graph of states and transitions over time:
In this HMM with two possible hidden states, sun or rain, we would like to compute the highest probability
path from X1 to XN. The weights on the edges are equal to P(Xt|Xt1)P(Et|Xt) and the probability of a path
is computed by taking the product of its edge weights. The first term in the weight estimates how likely a
particular transition is and the second term weights how well the observed evidence fits the resulting state. Recall that:
P(X1:N, e1:N) = P(X1)P(e1|X1)
N
’t=2
P(Xt|Xt1)P(et|Xt)
The Forward Algorithm computes (up to normalization)
P(XN, e1:N) = Âx1,..,xN1P(XN, x1:N1, e1:N)
CS 188, Fall 2018, Note 8 7
sun
sun
sun
sun
rain
rain
rain
rain
X1
X2
XN

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 7}
Content: In the Viberbi Algorithm, we want to compute
arg max x1,..,xNP(x1:N, e1:N)
to find the maximum likelihood estimate of the sequence of hidden states. Notice that each term in the product is exactly the expression for the edge weight between layer t 1 to layer t. So, the product of weights
along the path on the trellis gives us the probability of the path given the evidence. We could solve for a joint probability table over all of the possible hidden states, but this results in an
exponential space cost. Given such a table, we could use dynamic programming to compute the best path in
polynomial time. However, because we can use dynamic programming to compute the best path, we don’t
necessarily need the whole table at any given time. Define mt[xt] = maxx1:t1 P(x1:t, e1:t), or the maximum probability of a path starting at any x0 and the evidence seen so far to a given xt at time t. This is the same as the highest weight path through the trellis from
step 1 to t. Also note that
mt[xt] = max
x1:t1
P(et|xt)P(xt|xt1)P(x1:t1, e1:t1) (1)
= P(et|xt)max
xt1
P(xt|xt1)max
x1:t2
P(x1:t1, e1:t1) (2)
= P(et|xt)max
xt1
P(xt|xt1)mt1[xt1]. (3)
This suggests that we can compute mt for all t recursively via dynamic programming. This makes it possible
to determine the last state xN for the most likely path, but we still need a way to backtrack to reconstruct the
entire path. Let’s define at[xt] = P(et|xt) argmaxxt1 P(xt|xt1)mt1[xt1] = argmaxxt1 P(xt|xt1)mt1[xt1]
to keep track of the last transition along the best path to xt. We can now outline the algorithm. Result: Most likely sequence of hidden states x⇤
1:N
/* Forward pass */
for t = 1 to N do
for xt 2 X do
if t = 1 then
mt[xt] = P(xt)P(e0|xt)
else
at[xt] = argmaxxt1 P(xt|xt1)mt1[xt1];
mt[xt] = P(et|xt)P(xt|at[xt])mt1[at[xt]];
end
end
end
/* Find the most likely path’s ending point */
x⇤
N = argmaxxN mN[xN];
/* Work backwards through our most likely path and find the hidden
states */
for t = N to 2 do
x⇤
t1 = at[x⇤
t ];
end
Notice that our a arrays define a set of N sequences, each of which is the most likely sequence to a particular
end state xN. Once we finish the forward pass, we look at the likelihood of the N sequences, pick the best
CS 188, Fall 2018, Note 8 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 8}
Content: one, and reconstruct it in the backwards pass. We have thus computed the most likely explanation for our
evidence in polynomial space and time. Particle Filtering
Recall that with Bayes’ nets, when running exact inference was too computationally expensive, using one of
the sampling techniques we discussed was a viable alternative to efficiently approximate the desired probability distribution(s) we wanted. Hidden Markov Models have the same drawback - the time it takes to run
exact inference with the forward algorithm scales with the number of values in the domains of the random
variables. This was acceptable in our current weather problem formulation where the weather can only take
on 2 values, Wi 2 {sun,rain}, but say instead we wanted to run inference to compute the distribution of
the actual temperature on a given day to the nearest tenth of a degree. The Hidden Markov Model analog
to Bayes’ net sampling is called particle filtering, and involves simulating the motion of a set of particles
through a state graph to approximate the probability (belief) distribution of the random variable in question. Instead of storing a full probability table mapping each state to its belief probability, we’ll instead store a
list of n particles, where each particle is in one of the d possible states in the domain of our time-dependent
random variable. Typically, n is significantly smaller than d (denoted symbolically as n << d) but still large
enough to yield meaningful approximations; otherwise the performance advantage of particle filtering becomes negligible. Our belief that a particle is in any given state at any given timestep is dependent entirely
on the number of particles in that state at that timestep in our simulation. For example, say we indeed wanted
to simulate the belief distribution of the temperature T on some day i and assume for simplicity that this
temperature can only take on integer values in the range [10,20] (d = 11 possible states). Assume further
that we have n = 10 particles, which take on the following values at timestep i of our simulation:
[15,12,12,10,18,14,12,11,11,10]
By taking counts of each temperature that appears in our particle list and diving by the total number of
particles, we can generate our desired empirical distribution for the temperature at time i:
Ti 10 11 12 13 14 15 16 17 18 19 20
B(Ti) 0.2 0.2 0.3 0 0.1 0.1 0 0 0.1 0 0
Now that we’ve seen how to recover a belief distribution from a particle list, all that remains to be discussed
is how to generate such a list for a timestep of our choosing. Particle Filtering Simulation
Particle filtering simulation begins with particle initialization, which can be done quite flexibly - we can
sample particles randomly, uniformly, or from some initial distribution. Once we’ve sampled an initial list
of particles, the simulation takes on a similar form to the forward algorithm, with a time elapse update
followed by an observation update at each timestep:
• Time Elapse Update - Update the value of each particle according to the transition model. For a
particle in state ti, sample the updated value from the probability distribution given by Pr(Ti+1|ti). Note the similarity of the time elapse update to prior sampling with Bayes’ nets, since the frequency
of particles in any given state reflects the transition probabilities. CS 188, Fall 2018, Note 8 9


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 9}
Content: • Observation Update - During the observation update for particle filtering, we use the sensor model
Pr(Fi|Ti) to weight each particle according to the probability dictated by the observed evidence and
the particle’s state. Specifically, for a particle in state ti with sensor reading fi, assign a weight of
Pr(fi|ti). The algorithm for the observation update is as follows:
1. Calculate the weights of all particles as described above. 2. Calculate the total weight for each state. 3. If the sum of all weights across all states is 0, reinitialize all particles. 4. Else, normalize the distribution of total weights over states and resample your list of particles
from this distribution. Note the similarity of the observation update to likelihood weighting, where we again downweight
samples based on our evidence. Let’s see if we can understand this process slightly better by example. Define a transition model for our
weather scenario using temperature as the time-dependent random variable as follows: for a particular temperature state, you can either stay in the same state or transition to a state one degree away, within the range
[10,20]. Out of the possible resultant states, the probability of transitioning to the one closest to 15 is 80%
and the remaining resultant states uniformly split the remaining 20% probability amongst themselves. Our temperature particle list was as follows:
[15,12,12,10,18,14,12,11,11,10]
To perform a time elapse update for the first particle in this particle list, which is in state Ti = 15, we need
the corresponding transition model:
Ti+1 14 15 16
Pr(Ti+1|Ti = 15) 0.1 0.8 0.1
In practice, we allocate a different range of values for each value in the domain of Ti+1 such that together
the ranges entirely span the interval [0,1) without overlap. For the above transition model, the ranges are as
follows:
1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 9}
Content: The range for Ti+1 = 14 is 0  r < 0.1. 2. The range for Ti+1 = 15 is 0.1  r < 0.9. 3. The range for Ti+1 = 16 is 0.9  r < 1. In order to resample our particle in state Ti = 15, we simply generate a random number in the range [0,1) and
see which range it falls in. Hence if our random number is r = 0.467, then the particle at Ti = 15 remains in
Ti+1 = 15 since 0.1  r < 0.9. Now consider the following list of 10 random numbers in the interval [0,1):
[0.467,0.452,0.583,0.604,0.748,0.932,0.609,0.372,0.402,0.026]
If we use these 10 values as the random value for resampling our 10 particles, our new particle list after the
full time elapse update should look like this:
[15,13,13,11,17,15,13,12,12,10]
Verify this for yourself! The updated particle list gives rise to the corresponding updated belief distribution
B(Ti+1):
CS 188, Fall 2018, Note 8 10


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 10}
Content: Ti 10 11 12 13 14 15 16 17 18 19 20
B(Ti+1) 0.1 0.1 0.2 0.3 0 0.2 0 0.1 0 0 0
Comparing our updated belief distribution B(Ti+1) to our initial belief distribution B(Ti), we can see that as
a general trend the particles tend to converge towards a temperature of T = 15. Next, let’s perform the observation update, assuming that our sensor model Pr(Fi|Ti) states that the probability of a correct forecast fi = ti is 80%, with a uniform 2% chance of the forecast predicting any of the
other 10 states. Assuming a forecast of Fi+1 = 13, the weights of our 10 particles are as follows:
Particle p1 p2 p3 p4 p5 p6 p7 p8 p9 p10
State 15 13 13 11 17 15 13 12 12 10
Weight 0.02 0.8 0.8 0.02 0.02 0.02 0.8 0.02 0.02 0.02
Then we aggregate weights by state:
State 10 11 12 13 15 17
Weight 0.02 0.02 0.04 2.4 0.04 0.02
Summing the values of all weights yields a sum of 2.54, and we can normalize our table of weights to
generate a probability distribution by dividing each entry by this sum:
State 10 11 12 13 15 17
Weight 0.02 0.02 0.04 2.4 0.04 0.02
Normalized Weight 0.0079 0.0079 0.0157 0.9449 0.0157 0.0079
The final step is to resample from this probability distribution, using the same technique we used to resample during the time elapse update. Let’s say we generate 10 random numbers in the range [0,1) with the
following values:
[0.315,0.829,0.304,0.368,0.459,0.891,0.282,0.980,0.898,0.341]
This yields a resampled particle list as follows:
[13,13,13,13,13,13,13,15,13,13]
With the corresponding final new belief distribution:
Ti 10 11 12 13 14 15 16 17 18 19 20
B(Ti+1) 0 0 0 0.9 0 0.1 0 0 0 0 0
Observe that our sensor model encodes that our weather prediction is very accurate with probability 80%,
and that our new particles list is consisistent with this since most particles are resampled to be Ti+1 = 13. Summary
In this note, we covered two new types of models:
CS 188, Fall 2018, Note 8 11


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 11}
Content: • Markov models, which encode time-dependent random variables that possess the Markov property. We can compute a belief distribution at any timestep of our choice for a Markov model using probabilistic inference with the mini-forward algorithm. • Hidden Markov Models, which are Markov models with the additional property that new evidence
which can affect our belief distribution can be observed at each timestep. To compute the belief
distribution at any given timestep with Hidden Markov Models, we use the forward algorithm. Sometimes, running exact inference on these models can be too computationally expensive, in which case
we can use particle filtering as a method of approximate inference.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Notes.pdf', 'page': 11}
Content: CS 188, Fall 2018, Note 8 12


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 0}
Content: Artificial Intelligence
Markov Models
[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.]
E
X3
X4
X
X2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 1}
Content: Uncertainty and Time
 Often, we want to reason about a sequence of observations 
where the state of the underlying system is changing
 Speech recognition
 Robot localization
 User attention
 Medical monitoring
 Global climate 
 Need to introduce time into our models


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 2}
Content: Markov Models (aka Markov chain/process)
 Value of X at a given time is called the state
P(X0) P(Xt| Xt-1) 
 The transition model P(Xt | Xt-1) specifies how the state evolves over time
 Stationarity assumption: transition probabilities are the same at all times
 Markov assumption: “future is independent of the past given the present” 
 Xt+1 is independent of X0,..., Xt-1given Xt
 This is a first-order Markov model (a kth-order model allows dependencies on k earlier steps) 
 Joint distribution P(X0,..., XT) = P(X0) ∏t P(Xt | Xt-1) 
X0 X1 X2 X3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 3}
Content: 4
Example: n-gram models
We call ourselvesHomosapiens—man thewise—becauseourintelligenceis soimportant to us. For thousandsof years,wehave tried to understand howwe think;that is,how a merehandful ofmatter can
perceive,understand,predict,and manipulate a world far larger and more complicated than itself... State:wordatpositiontintext(canalsobuildlettern-grams)
Transitionmodel(probabilitiescomefromempiricalfrequencies):
Unigram(zero-order):P(Wordt=i)
“logical are as are confusion a may right tries agent goal the was..."
Bigram(first-order):P(Wordt=i|Wordt-1=j)
“planning and schedulingare integrated the success of naivebayesmodelis..."
Applications:textclassification,spamdetection,authoridentification,
languageclassification,speechrecognition

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 4}
Content: 5
Example: Webbrowsing
State:URLvisited atstep t
Transitionmodel:
·Withprobabilityp,chooseanoutgoinglinkatrandom
·With probability (1-p), choose an arbitrary new page
Question:Whatis thestationarydistributionoverpages? I.e.,if theprocessrunsforever,whatfractionoftime doesitspend in
anygivenpage? Application:Googlepagerank

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 5}
Content: Joint Distribution of a Markov Model
 Joint distribution:
 More generally:
 Questions to be resolved:
 Does this indeed define a joint distribution?  Can every joint distribution be factored this way, or are we making some assumptions 
about the joint distribution by using this factorization?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 5}
Content: X1 X2 X3 X4
D
It-
1
1D
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 6}
Content: Example Markov Chain: Weather
rain sun
0.9
0.7
0.3
0.1
Two new ways of representing the same CPT
sun
rain
sun
rain
0.1
0.9
0.7
0.3
Mor
Wed
Thu
FriStates{rain,sun}
Initial distributionP(Xo)
P(X)
sun
rain
0.5
0.5
Transition model P(Xt | Xt-1)
X-1
P(X1X-1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 7}
Content: 
Weatherprediction
Time0:<0.5,0.5>
Xr-1
P(X1X-1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime1? P(X1) =∑x P(X,Xo=xo)
= Zx。 P(X=xo) P(X1 X=xo)
=0.5<0.9,0.1>+0.5<0.3,0.7>=<0.6,0.4>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 8}
Content: 
Weather prediction, contd. Time1:<0.6,0.4>
X-1
P(X1X1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime2? P(X2)=∑xP(X2,X=x1)
=∑x P(X=x1) P(X2↓ X=x1)
0.6<0.9,0.1>+0.4<0.3,0.7>=<0.66，0.34>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 9}
Content: 
Weather prediction, contd. Time2:<0.66,0.34>
Xt-1
P(X|X1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime3? P(X3)=∑xP(XX2=x2)
=∑x2P(X2=x2) P(X31 X2=X2)
=0.66<0.9,0.1>+0.34<0.3，0.7>=<0.696,0.304>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 10}
Content: Mini-Forward Algorithm
 Question: What’s P(X) on some day t? Forward simulation
X1 X2 X3 X4
D
C
kn
nown
1?? days later

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 11}
Content: 
Forward algorithm (simple form)
P(Xo)
P(Xt I Xt-1)
Probabilityfrom
previous iteration
Whatisthestateattimet? Transitionmodel
(T-4x=1-4xx)d rx7 =(x)d
=Zxt1
(1-4x=1-x1²x)d(1-x=1-x)d
Iteratethisupdatestartingatt=o
This is called a recursiveupdate:Pt=g(Pt-1)=g(g(g(g(..P))))

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 12}
Content: 
And the same thing in linear algebra
Whatistheweatherlikeattime2? P(X2)=0.6<0.9,0.1>+0.4<0.3,0.7>=<0.66,0.34>
Inmatrix-vectorform:
Xr-1
P(X1X-1)
(4) =(
sun
rain
P(X2)=
0.90.3
0.66
0.10.7
0.34
sun
0.9
0.1
rain
0.3
0.7
1.e.,multiplybyT,transposeoftransitionmatrix

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 13}
Content:  Stationary distribution:
 The distribution we end up with is called 
the stationary distribution of the 
chain
 It satisfies
Stationary Distributions
 For most chains:
 Influence of the initial distribution 
gets less and less over time.  The distribution we end up in is 
independent of the initial distribution


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 14}
Content: Example: Stationary Distributions
 Question: What’s P(X) at time t = infinity? X1 X2 X3 X4
Xt-1 Xt P(Xt|Xt-1)
sun sun 0.9
sun rain 0.1
rain sun 0.3
rain rain 0.7
Also:
?? days later(%9) (c) =(c.)
0.10.7
0.9p+0.3(1-p)=p
p=0.75

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 15}
Content: Application of Stationary Distribution: Web Link Analysis
 PageRank over a web graph
 Each web page is a state
 Initial distribution: uniform over pages
 Transitions:
 With prob. c, uniform jump to a
random page (dotted lines, not all shown)
 With prob. 1-c, follow a random
outlink (solid lines)
 Stationary distribution
 Will spend more time on highly reachable pages
 E.g. many ways to get to the Acrobat Reader download page
 Somewhat robust to link spam
 Google 1.0 returned the set of pages containing all your 
keywords in decreasing rank, now all search engines use link 
analysis along with many other factors (rank actually getting 
less important over time)
9
M
S

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 16}
Content: Hidden Markov Models
00
BM

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 17}
Content: Hidden Markov Models
X2 X5
E1
X1 X3 X4
E2E3E4E5
00
BMUsuallythetruestateisnotobserveddirectly
HiddenMarkovmodels(HMMs)
UnderlyingMarkovchainoverstatesX
YouobserveevidenceEateachtimestep
Xisa singlediscretevariable;Etmaybe continuous
andmayconsistofseveralvariables

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 18}
Content: 
Example:Weather HMM
AnHMM is defined by:
W1
Initialdistribution:
P(X)
P(W,IW1)
sun
rain
Transitionmodel:
P(Xt| Xt-1)
sun
0.9
0.1
Sensormodel:
P(EtI Xt)
rain
0.3
0.7
Weathert-1
Weathert
Weathert+1
W:
P(U,|W.)
true
false
sun
0.2
0.8
rain
0.9
0.1
Umbrellat-1
Umbrellat
Umbrellat1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 19}
Content: 20
HMM as probability model
 Joint distribution for Markov model: P(Xo, Xt) = P(Xo) IIt=1:T P(Xt I Xt-1)
JointdistributionforhiddenMarkovmodel:
P(Xo,Eo,X1,E1.,XT,ET) = P(Xo) IIt=1:T P(Xt I Xt-1) P(Et I Xt)
Futurestatesareindependentofthepastgiventhepresent
Currentevidenceisindependentofeverythingelsegiventhecurrentstate
Areevidencevariablesindependentofeachother? Usefulnotation:
Xa:b=Xa,Xa+1,.,Xb
E3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 20}
Content: 
RealHMMExamples
SpeechrecognitionHMMs:
Observations are acoustic signals(continuous valued)
States are specific positions in specific words (so,tens of thousands)
MachinetranslationHMMs:
·Observations are words(tens of thousands)
States are translation options
Robottracking:
Observations arerangereadings(continuous)
States arepositionsonamap(continuous)
Molecularbiology:
·ObservationsarenucleotidesACGT
Statesarecoding/non-coding/start/stop/splice-siteetc.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 21}
Content: 
Inferencetasks
Filtering:P(Xtle1:t)
beliefstate-inputtothedecisionprocessofarationalagent
Prediction:P(Xt+kle1:t)fork>0
evaluationofpossibleactionsequences;likefilteringwithouttheevidence
Smoothing:P(Xkle1:t)for0≤k<t
betterestimateofpaststates,essentialforlearning
speechrecognition,decodingwithanoisychannel

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 22}
Content: 23
Inferencetasks
Filtering:P(Xtle1:t)
Prediction:P(X+k|e1:t)
Xa
Xa
Smoothing:P(Xle1:t),k<t
Explanation:P(X1:tle1:t)
X3
e

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 23}
Content: 24
Filtering/Monitoring
Filtering,ormonitoring,orstateestimation,isthetaskof
maintaining thedistributionf1:t=P(Xtle1:t)overtime
Westartwithfoinaninitialsetting,usuallyuniform
Filteringisafundamentaltaskinengineeringandscience
TheKalmanfilter(continuousvariables,lineardynamics,
Gaussiannoise)wasinvented in1960 andusedfortrajectory
estimationintheApolloprogram;coreideasusedbyGaussfor
planetaryobservations;>1,o00,000papersonGoogleScholar

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 24}
Content: Example: Robot Localization
t=0
Sensor model: can read in which directions there is a wall, 
never more than 1 mistake
Motion model: may not execute action with small prob. Prob 0 1
Example from 
Michael Pfeiffer
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 25}
Content: Example: Robot Localization
t=1
Lighter grey: was possible to get the reading, but less likely b/c 
required 1 mistake
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 26}
Content: Example: Robot Localization
t=2
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 27}
Content: Example: Robot Localization
t=3
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 28}
Content: Example: Robot Localization
t=4
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 29}
Content: Example: Robot Localization
t=5
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 30}
Content: 
Filtering algorithm
Aim:devisearecursivefilteringalgorithmoftheform
P(Xt+1le1:t+1)=g(et+1,P(Xtle1:t))
X3
Xa
P(Xt+1le1:t+1) = P(Xt+1le1:t
et+1)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 31}
Content: Filtering algorithm
32
Aim: devise a recursive filtering algorithm of the form
P(Xt+1le1:t+1) = g(et+1; P(Xtle1:t) )
X
ApplyBayes'rule
e
e2
e3
e4
P(Xt+1e1:t+1) = P(Xt+11e1:t
et+1)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 32}
Content: Filtering algorithm
33
Aim:devise a recursivefiltering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(X±le1:t) )
X
X
ApplyBayes'rule
P(Xt+1le1:t+1) =P(Xt+1le1:t
e
e
e
e
et+1
4
= α P(et+11Xt+1 e1:t)
)P(Xt+11e
e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 33}
Content: Filtering algorithm
34
Aim: devise a recursive filtering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
P(Xt+1le1:t+1)
)= P(Xt+1le1:t
Applyconditional independence
e
4
et+1l
= 0uP(et+11Xt+1e1:t)Pt+1
e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 34}
Content: Filtering algorithm
Aim:devisearecursivefiltering algorithm oftheform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
P(Xt+11e1:t+1)=P(Xt+1le1:t
Applyconditionalindependence
et+1l
=αP(et+11Xt+1e1:t)
PVt+1
e1:t)
=αP(et+11Xt+1) P(Xt+11
I e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 35}
Content: Filtering algorithm
Aim:devise a recursivefilteringalgorithm oftheform
P(Xt+1le1:t+1) =g(et+1, P(Xtle1:t) )
X1
ApplyBayes'rule
e3
e4
P(X+1le1:t+1) = P(X+1le1:t
Applyconditionalindependence
et+1)
=a P(et+11Xt+1 e1:t)
P^t+1
e1:t)
= α P(et+1Xt+1) P(X+11 e1:t)
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 36}
Content: Filtering algorithm
Aim:devise a recursivefiltering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
ea
P(Xt+1e1:t+1) = P(Xt+1le1:t
,et+1)
Applyconditional independence
=a P(et+11Xt+1 e1:t)
e1:t)
= α P(et+1Xt+1) P(Xt+11 e1:t)
Update
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 37}
Content: Filtering algorithm
38
Aim:devise arecursivefilteringalgorithmoftheform
P(Xt+1le1:t+1) = g(et+1, P(Xle1:t) )
X1
ApplyBayes'rule
e3
P(Xt+1le1:t+1) =P(Xt+1le1:t
Applyconditionalindependence
et+1)
=αP(et+1X+1e1:t)
)P^t+1
e1:t)
=α P(et+1Xt+1) P(Xt+11 e1:t)
Normalize
Update
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 38}
Content: Filtering algorithm
39
Aim:devise arecursivefilteringalgorithm of theform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
P(Xt+1le1:t+1) =P(Xt+1e1:t
Applyconditionalindependence
e
et+1)
=αP(et+1Xt+1e1:t)Pt+1
ConditiononX
e
1
= α P(et+1X+1) P(Xt+1 e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 39}
Content: Filtering algorithm
40
Aim:devise arecursivefiltering algorithm oftheform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t))
X
XA
ApplyBayes'rule
e3
e4
P(Xt+1e1:t+1) = P(Xt+1le1:t
Applyconditionalindependence
et+1)
(+x)=
e1:
ConditiononX
=αP(et+11Xt+1)P(Xt+1 e1:t)
P(xt1 e1:t) P(Xt+11 Xt e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 40}
Content: Filtering algorithm
Aim:devise arecursivefiltering algorithm of theform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
ea
P(Xt+1le1:t+1) = P(Xt+1le1:t
Applyconditional independence
et+1)
=α P(e+11Xt+1 e1:t)
e1:l
Condition onX
^t+1
=α P(et+11Xt+1) P(Xt+11 e1:t)
Applyconditional
independence
= α P(et+1 lXt+1)
P(xt I e1:t) P(Xt+11 Xt e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 41}
Content: Filtering algorithm
Aim:devise arecursivefilteringalgorithm oftheform
P(Xt+1 le1:t+1) = g(et+1, P(Xtle1:t) )
X1
XA
ApplyBayes'rule
e3
e4
P(Xt+1le1:t+1) = P(X+1le1:t
Applyconditionalindependence
et+1)
= α P(et+11Xt+1 e1:t)t+1l
ConditiononX
= αP(et+11Xt+1) P(Xt+1 e1:t)
Applyconditional
independence
= α P(et+1IXt+1)
P(xt 1 e1:t) P(Xt+1xt e1:t)
=α P(et+1IXt+1)
P(xtI e1:t) P(Xt+1
xt)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 42}
Content: 
Filtering algorithm
Aim:devisearecursivefiltering algorithm of theform
■P(Xt+1le1:t+1)= g(et+1, P(Xle1:t) )
P(Xt+1le1:t+1) = P(Xt+1le1:t E+1)
= α P(et+1l Xt+1, e1:t) P(Xt+1 e1:t)
=αP(et+1lX+1)P(Xt+1| e1:t)
= α P(et+1IXx+1) ∑x P(xt I e1:t) P(Xt+1l Xt, e1:t)
=α P(et+11Xt+1) ∑xt P(xt 1 e1:t) P(Xt+1 xt)
Givenby HMM
Pre-computed
Givenby HMM

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 43}
Content: 
Filtering algorithm
Aim:devise arecursivefiltering algorithm of theform
■ P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
P(Xt+1le1:t+1) = P(Xt+1le1:t Et+1)
= α P(et+1lX+1, e1:t) P(Xt+1 e1:t)
LHS: P(X++1,e1:t, Et+1)/P(e1:t Et+1)
RHS: α P(et+1, X+1, e1:t)/P(X+1, e1:t) * P(X+1, e1:t)/ P(e1:t)
RHS: α P(et+1, Xt+1, e1:t) / P(e1:t)
α = P(e1:t) / P(e1:t et+1) which is the same for all xt+1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 44}
Content: 
Filtering algorithm
Aim:devisearecursivefilteringalgorithmof theform
■P(Xt+1le1:t+1) = g(et+1, P(Xle1:t) )
P(Xt+1 le1:t+1) = P(Xt+1le1:to et+1)
= α P(et+1IXt+1, e1:t) P(Xt+1 e1:t)
= α P(et+1IXt+1) P(Xt+1l e1:t)
Why does P(et+1IXt+1, e1:t) = P(et+1 IXt+1) ? Variablesareindependentofnon-descendantsgivenparents
If I know X4, nothing else will help be better predict e4

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 45}
Content: 
Filtering algorithm
Aim:devise arecursivefiltering algorithmof theform
■P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t))
P(Xt+1le1:t+1) = P(X++1le1:to Et+1)
= α P(et+1 / Xt+1, e1:t) P(Xt+1 e1:t)
=αP(et+1|Xt+1)P(Xt+1l e1:t)
= α P(et+11Xt+1) ∑x P(xt I e1:t) P(X+11 Xt, e1:t)
P(A|B)P(B) = P(A,B)
Marginalizationoverxt
∑x P(X+1l Xt e1:t) P(xt I e1:t)= ∑x P(Xt+1 Xt I e1:t) =P(Xt+1l e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 46}
Content: 
Filtering algorithm
Aim:devisearecursivefiltering algorithm of theform
■P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
P(X+1 le1:t+1) = P(Xt+1le1:t Et+1)
= α P(et+1l Xt+1, e1:t) P(Xt+1 e1:t)
=αP(et+1lX+1)P(Xt+1| e1:t)
= α P(et+1IXx+1) ∑x P(xt I e1:t) P(Xt+1l Xt, e1:t)
=αx P(et+11Xt+1)xP(xt1 e1:t) P(Xt+11 xt)
Variablesare
independentof non-
descendants given
Given by HMM
Pre-computed
Given by HMM
parents

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 47}
Content: 
"Forward"algorithm
Givenby HMM(vector)
Pre-computed (scalar
GivenbyHMM(vectorfor
foreachterminsum)
eachterminsum)
P(X+1le1:t+1) =α P(et+1lXt+1) ∑x P(xt I e1:t) P(Xt+1l xt)
Normalize
Update
Predict
■ f1:t+1 = FoRWARD(f1:t , Ee+1) ; f1:t is P(Xtle1:t) *for t=0, note e1:o is empty
Costpertimestep:O(|x|²)where|X|is thenumberofstates
Time and space costs are constant, independent of t
Aw
O(|xl²)isinfeasibleformodelswithmanystatevariables
Wegettoinventreallycool approximatefiltering algorithms
Awesome

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 48}
Content: 
And the same thing in linear algebra
Transition matrixT, observation matrixO
·Observation matrix has state likelihoods for Et along diagonal
~ E.g,for U =true, O =(° :9)
Xe-1
P(X1X-1)
sun
rain
Filteringalgorithmbecomes
sun
0.9
0.1
f1:t+1=α Ot+1Tf1:t
rain
0.3
0.7
W,
P(U,IW.)
true
false
sun
0.2
0.8
rain
0.9
0.1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 49}
Content: 
Example: Weather HMM
0.6
0.45
predict
0.4
predict
0.55
update
update
W:
P(W,|W1)
f(sun) = 0.5
f(sun) = 0.25
f(sun) = 0.154
sun
rain
f(rain) = 0.5
f(rain) = 0.75
f(rain) = 0.846
sun
0.9
0.1
rain
E'0
0.7
Weathero
Weather
Weather2
W
P(U;1W.)
true
false
P(W。)
sun
rain
sun
0.2
0.8
Umbrella,
Umbrella2
rain
0.5
0.5
60
0.1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 50}
Content: 
Most Likely Explanation

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 51}
Content: 
Inferencetasks
Filtering:P(Xtle1:t)
·beliefstate-inputtothedecisionprocessofarationalagent
Prediction:P(Xt+kle1:t)fork>0
evaluationofpossibleactionsequences;likefilteringwithouttheevidence
Smoothing:P(Xkle1:t)for0≤k<t
·betterestimateofpaststates,essentialforlearning
Most likely explanation: arg maxx.t P(x1:t I e1:t)
■speechrecognition,decodingwith a noisy channel

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 52}
Content: 
Most likely explanation = most probable path
Statetrellis:graphofstatesandtransitionsovertime
sun
sun
sun
sun
rain
rain
rain
rain
Xo
X1
XT
arg max1.tP(x1:t I e1:t)
=arg maxx1:tα P(x1:t, e1:t)
AllgivenbyHMM
=argmaxx1.t P(x1:t,e1:t)
= arg maxx.: P(xo) IIt P(xt I xt-1) P(et I xt)
Alternative form
=arg maxxt log [P(xo) IIt P(xt | x-1) P(et| x)]
= arg minx:t-log P(xo) +∑t-log P(xt I xt-1) + -log P(et | xt)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 53}
Content: 
Most likelyexplanation=most probable path
Statetrellis:graphofstatesandtransitionsovertime
sun
sun
sun
sun
rain
rain
rain
rain
Xo
X
XT
·Each arc represents some transition Xt-1 →Xt
■Each arc has weight P(xt I Xt-1) P(et I xt) (arcs to initial states have weight P(xo))
·Theproductof weightsonapathisproportionaltothatstatesequence'sprobability
·Forward algorithm computes sums ofpaths,Viterbialgorithmcomputesbestpaths

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 54}
Content: 
Forward/Viterbialgorithms
sun
sun
sun
sun
rain
rain
rain
rain
Xo
X1
XT
ForwardAlgorithm(sum)
ViterbiAlgorithm(max)
Foreachstateattimet,keeptrackof
Foreach state at time t,keep track of
thetotalprobabilityofallpathstoit
themaximumprobabilityofanypathtoit
f1:t+1=FORWARD(f1:t,et+1)
m1:t+1=VITERBI(m1:t，et+1)
=α P(et+1X+1)∑xtP(Xt+1 xt) f1:t
=P(et+1X+1) maxxt P(Xt+1 xt) m1:t

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 55}
Content: 56
Wt-1
P(W,/W.)
0.18
0.72
0.18
sun
rain
0.5
sun
sun
sun
sun
0.09
0:01
0.09
sun
0.9
0.1
0.06
0,24
0,06
rain
0.3
0.7
0.5
rain
rain
rain
rain
0.63
0.07
0.63
w,
P(U,IW,)
Xo
X1
XT
true
false
X2
U=true
U3=true
sun
0.2
0.8
U2=false
rain
0.9
0.1Viterbi algorithm contd. We.1
P(W,IW3)
0.5
0.5
0.18
0.09
0.72
0.076
0.18
0.0136080
sun
rain
0.09
0.01
660
sun
0.9
0.1
0.06
9.06
rain
0.3
0.5
0.7
0.5
0.63
0.315
0.07
0.022
0.63
0.0138495
W:
P(U,|W.)
Xo
X1
X
XT
true
false
U=true
U2=false
U3=true
sun
0.2
0.8
rain
60
0.1
Time complexity? Space complexity?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 55}
Content: Number of paths? (1zlxl)o
(1 Ixl)o
(1x1)o

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 56}
Content: 
Viterbi algorithm contd. We.1
P(W,IW3)
0.5
0.5
0.18
0.09
0.72
0.076
0.18
0.0136080
sun
rain
0.09
0.01
660
sun
0.9
0.1
0.06
9.06
rain
0.3
0.5
0.7
0.5
0.63
0.315
0.07
0.022
0.63
0.0138495
W:
P(U,|W.)
Xo
X1
X
XT
true
false
U=true
U2=false
U3=true
sun
0.2
0.8
rain
60
0.1
Time complexity? Space complexity?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 56}
Content: Number of paths? (1zlxl)o
(1 Ixl)o
(1x1)o

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 1.pdf', 'page': 57}
Content: 
Viterbi in negativelog space
We.1
P(W,IW)
1.0
2.47
0.72
2.47
sun
sun
rain
sun
3.47
sun
6.64
sun
3.47
sun
0.9
0.1
4.06
2.06
4.06
rain
0.3
0.7
1.0
rain
0.67
rain
3.84
rain
0.67
rain
W
P(U,|W.)
true
false
G
sun
0.2
0.8
argmaxofproductofprobabilities
rain
0.9
0.1
=argminofsumofnegativelogprobabilities
=minimum-costpath
Viterbi is essentially breadth-first graph search
WhataboutA*?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 0}
Content: Artificial Intelligence
Markov Models
[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.]
E
X3
X4
X
X2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 1}
Content: Uncertainty and Time
 Often, we want to reason about a sequence of observations 
where the state of the underlying system is changing
 Speech recognition
 Robot localization
 User attention
 Medical monitoring
 Global climate 
 Need to introduce time into our models


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 2}
Content: Markov Models (aka Markov chain/process)
 Value of X at a given time is called the state
P(X0) P(Xt| Xt-1) 
 The transition model P(Xt | Xt-1) specifies how the state evolves over time
 Stationarity assumption: transition probabilities are the same at all times
 Markov assumption: “future is independent of the past given the present” 
 Xt+1 is independent of X0,..., Xt-1given Xt
 This is a first-order Markov model (a kth-order model allows dependencies on k earlier steps) 
 Joint distribution P(X0,..., XT) = P(X0) ∏t P(Xt | Xt-1) 
X0 X1 X2 X3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 3}
Content: 5
Example: n-gram models
We call ourselvesHomosapiens—man thewise—becauseourintelligenceis soimportant to us. For thousandsof years,wehave tried to understand howwe think;that is,how a merehandful ofmatter can
perceive,understand,predict,and manipulate a world far larger and more complicated than itself... State:wordatpositiontintext(canalsobuildlettern-grams)
Transitionmodel(probabilitiescomefromempiricalfrequencies):
Unigram(zero-order):P(Wordt=i)
“logical are as are confusion a may right tries agent goal the was..."
Bigram(first-order):P(Wordt=i|Wordt-1=j)
“planning and schedulingare integrated the success of naivebayesmodelis..."
Applications:textclassification,spamdetection,authoridentification,
languageclassification,speechrecognition

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 4}
Content: 6
Example: Webbrowsing
State:URLvisited atstep t
Transitionmodel:
·Withprobabilityp,chooseanoutgoinglinkatrandom
·With probability (1-p), choose an arbitrary new page
Question:Whatis thestationarydistributionoverpages? I.e.,if theprocessrunsforever,whatfractionoftime doesitspend in
anygivenpage? Application:Googlepagerank

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 5}
Content: Joint Distribution of a Markov Model
 Joint distribution:
 More generally:
 Questions to be resolved:
 Does this indeed define a joint distribution?  Can every joint distribution be factored this way, or are we making some assumptions 
about the joint distribution by using this factorization?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 5}
Content: X1 X2 X3 X4
D
It-
1
1D
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 6}
Content: Example Markov Chain: Weather
rain sun
0.9
0.7
0.3
0.1
Two new ways of representing the same CPT
sun
rain
sun
rain
0.1
0.9
0.7
0.3
Mor
Wed
Thu
FriStates{rain,sun}
Initial distributionP(Xo)
P(X)
sun
rain
0.5
0.5
Transition model P(Xt | Xt-1)
X-1
P(X1X-1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 7}
Content: 
Weatherprediction
Time0:<0.5,0.5>
Xr-1
P(X1X-1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime1? P(X1) =∑x P(X,Xo=xo)
= Zx。 P(X=xo) P(X1 X=xo)
=0.5<0.9,0.1>+0.5<0.3,0.7>=<0.6,0.4>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 8}
Content: 
Weather prediction, contd. Time1:<0.6,0.4>
X-1
P(X1X1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime2? P(X2)=∑xP(X2,X=x1)
=∑x P(X=x1) P(X2↓ X=x1)
0.6<0.9,0.1>+0.4<0.3,0.7>=<0.66，0.34>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 9}
Content: 
Weather prediction, contd. Time2:<0.66,0.34>
Xt-1
P(X|X1)
sun
rain
sun
0.9
0.1
rain
0.3
0.7
Whatistheweatherlikeattime3? P(X3)=∑xP(XX2=x2)
=∑x2P(X2=x2) P(X31 X2=X2)
=0.66<0.9,0.1>+0.34<0.3，0.7>=<0.696,0.304>

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 10}
Content: Mini-Forward Algorithm
 Question: What’s P(X) on some day t? Forward simulation
X1 X2 X3 X4
D
C
kn
nown
1?? days later

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 11}
Content: 
Forward algorithm (simple form)
P(Xo)
P(Xt I Xt-1)
Probabilityfrom
previous iteration
Whatisthestateattimet? Transitionmodel
(T-4x=1-4xx)d rx7 =(x)d
=Zxt1
(1-4x=1-x1²x)d(1-x=1-x)d
Iteratethisupdatestartingatt=o
This is called a recursiveupdate:Pt=g(Pt-1)=g(g(g(g(..P))))

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 12}
Content: 
And the same thing in linear algebra
Whatistheweatherlikeattime2? P(X2)=0.6<0.9,0.1>+0.4<0.3,0.7>=<0.66,0.34>
Inmatrix-vectorform:
Xr-1
P(X1X-1)
(4) =(
sun
rain
P(X2)=
0.90.3
0.66
0.10.7
0.34
sun
0.9
0.1
rain
0.3
0.7
1.e.,multiplybyT,transposeoftransitionmatrix

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 13}
Content:  Stationary distribution:
 The distribution we end up with is called 
the stationary distribution of the 
chain
 It satisfies
Stationary Distributions
 For most chains:
 Influence of the initial distribution 
gets less and less over time.  The distribution we end up in is 
independent of the initial distribution


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 14}
Content: Example: Stationary Distributions
 Question: What’s P(X) at time t = infinity? X1 X2 X3 X4
Xt-1 Xt P(Xt|Xt-1)
sun sun 0.9
sun rain 0.1
rain sun 0.3
rain rain 0.7
Also:
?? days later(%9) (c) =(c.)
0.10.7
0.9p+0.3(1-p)=p
p=0.75

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 15}
Content: Application of Stationary Distribution: Web Link Analysis
 PageRank over a web graph
 Each web page is a state
 Initial distribution: uniform over pages
 Transitions:
 With prob. c, uniform jump to a
random page (dotted lines, not all shown)
 With prob. 1-c, follow a random
outlink (solid lines)
 Stationary distribution
 Will spend more time on highly reachable pages
 E.g. many ways to get to the Acrobat Reader download page
 Somewhat robust to link spam
 Google 1.0 returned the set of pages containing all your 
keywords in decreasing rank, now all search engines use link 
analysis along with many other factors (rank actually getting 
less important over time)
9
M
S

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 16}
Content: Hidden Markov Models
00
BM

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 17}
Content: Hidden Markov Models
X2 X5
E1
X1 X3 X4
E2E3E4E5
00
BMUsuallythetruestateisnotobserveddirectly
HiddenMarkovmodels(HMMs)
UnderlyingMarkovchainoverstatesX
YouobserveevidenceEateachtimestep
Xisa singlediscretevariable;Etmaybe continuous
andmayconsistofseveralvariables

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 18}
Content: 
Example:Weather HMM
AnHMM is defined by:
W1
Initialdistribution:
P(X)
P(W,IW1)
sun
rain
Transitionmodel:
P(Xt| Xt-1)
sun
0.9
0.1
Sensormodel:
P(EtI Xt)
rain
0.3
0.7
Weathert-1
Weathert
Weathert+1
W:
P(U,|W.)
true
false
sun
0.2
0.8
rain
0.9
0.1
Umbrellat-1
Umbrellat
Umbrellat1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 19}
Content: 32
HMM as probability model
 Joint distribution for Markov model: P(Xo, Xt) = P(Xo) IIt=1:T P(Xt I Xt-1)
JointdistributionforhiddenMarkovmodel:
P(Xo,Eo,X1,E1.,XT,ET) = P(Xo) IIt=1:T P(Xt I Xt-1) P(Et I Xt)
Futurestatesareindependentofthepastgiventhepresent
Currentevidenceisindependentofeverythingelsegiventhecurrentstate
Areevidencevariablesindependentofeachother? Usefulnotation:
Xa:b=Xa,Xa+1,.,Xb
E3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 20}
Content: 
RealHMMExamples
SpeechrecognitionHMMs:
Observations are acoustic signals(continuous valued)
States are specific positions in specific words (so,tens of thousands)
MachinetranslationHMMs:
·Observations are words(tens of thousands)
States are translation options
Robottracking:
Observations arerangereadings(continuous)
States arepositionsonamap(continuous)
Molecularbiology:
·ObservationsarenucleotidesACGT
Statesarecoding/non-coding/start/stop/splice-siteetc.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 21}
Content: 
Inferencetasks
Filtering:P(Xtle1:t)
beliefstate-inputtothedecisionprocessofarationalagent
Prediction:P(Xt+kle1:t)fork>0
evaluationofpossibleactionsequences;likefilteringwithouttheevidence
Smoothing:P(Xkle1:t)for0≤k<t
betterestimateofpaststates,essentialforlearning
speechrecognition,decodingwithanoisychannel

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 22}
Content: 35
Inferencetasks
Filtering:P(Xtle1:t)
Prediction:P(X+k|e1:t)
Xa
Xa
Smoothing:P(Xle1:t),k<t
Explanation:P(X1:tle1:t)
X3
e

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 23}
Content: 36
Filtering/Monitoring
Filtering,ormonitoring,orstateestimation,isthetaskof
maintaining thedistributionf1:t=P(Xtle1:t)overtime
Westartwithfoinaninitialsetting,usuallyuniform
Filteringisafundamentaltaskinengineeringandscience
TheKalmanfilter(continuousvariables,lineardynamics,
Gaussiannoise)wasinvented in1960 andusedfortrajectory
estimationintheApolloprogram;coreideasusedbyGaussfor
planetaryobservations;>1,o00,000papersonGoogleScholar

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 24}
Content: Example: Robot Localization
t=0
Sensor model: can read in which directions there is a wall, 
never more than 1 mistake
Motion model: may not execute action with small prob. Prob 0 1
Example from 
Michael Pfeiffer
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 25}
Content: Example: Robot Localization
t=1
Lighter grey: was possible to get the reading, but less likely b/c 
required 1 mistake
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 26}
Content: Example: Robot Localization
t=2
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 27}
Content: Example: Robot Localization
t=3
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 28}
Content: Example: Robot Localization
t=4
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 29}
Content: Example: Robot Localization
t=5
Prob 0 1
口

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 30}
Content: 
Filtering algorithm
Aim:devisearecursivefilteringalgorithmoftheform
P(Xt+1le1:t+1)=g(et+1,P(Xtle1:t))
X3
Xa
P(Xt+1le1:t+1) = P(Xt+1le1:t
et+1)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 31}
Content: Filtering algorithm
44
Aim: devise a recursive filtering algorithm of the form
P(Xt+1le1:t+1) = g(et+1; P(Xtle1:t) )
X
ApplyBayes'rule
e
e2
e3
e4
P(Xt+1e1:t+1) = P(Xt+11e1:t
et+1)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 32}
Content: Filtering algorithm
45
Aim:devise a recursivefiltering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(X±le1:t) )
X
X
ApplyBayes'rule
P(Xt+1le1:t+1) =P(Xt+1le1:t
e
e
e
e
et+1
4
= α P(et+11Xt+1 e1:t)
)P(Xt+11e
e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 33}
Content: Filtering algorithm
46
Aim: devise a recursive filtering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
P(Xt+1le1:t+1)
)= P(Xt+1le1:t
Applyconditional independence
e
4
et+1l
= 0uP(et+11Xt+1e1:t)Pt+1
e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 34}
Content: Filtering algorithm
Aim:devisearecursivefiltering algorithm oftheform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
P(Xt+11e1:t+1)=P(Xt+1le1:t
Applyconditionalindependence
et+1l
=αP(et+11Xt+1e1:t)
PVt+1
e1:t)
=αP(et+11Xt+1) P(Xt+11
I e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 35}
Content: Filtering algorithm
Aim:devise a recursivefilteringalgorithm oftheform
P(Xt+1le1:t+1) =g(et+1, P(Xtle1:t) )
X1
ApplyBayes'rule
e3
e4
P(X+1le1:t+1) = P(X+1le1:t
Applyconditionalindependence
et+1)
=a P(et+11Xt+1 e1:t)
P^t+1
e1:t)
= α P(et+1Xt+1) P(X+11 e1:t)
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 36}
Content: Filtering algorithm
Aim:devise a recursivefiltering algorithm of the form
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
ea
P(Xt+1e1:t+1) = P(Xt+1le1:t
,et+1)
Applyconditional independence
=a P(et+11Xt+1 e1:t)
e1:t)
= α P(et+1Xt+1) P(Xt+11 e1:t)
Update
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 37}
Content: Filtering algorithm
50
Aim:devise arecursivefilteringalgorithmoftheform
P(Xt+1le1:t+1) = g(et+1, P(Xle1:t) )
X1
ApplyBayes'rule
e3
P(Xt+1le1:t+1) =P(Xt+1le1:t
Applyconditionalindependence
et+1)
=αP(et+1X+1e1:t)
)P^t+1
e1:t)
=α P(et+1Xt+1) P(Xt+11 e1:t)
Normalize
Update
Predict

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 38}
Content: Filtering algorithm
51
Aim:devise arecursivefilteringalgorithm of theform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
P(Xt+1le1:t+1) =P(Xt+1e1:t
Applyconditionalindependence
e
et+1)
=αP(et+1Xt+1e1:t)Pt+1
ConditiononX
e
1
= α P(et+1X+1) P(Xt+1 e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 39}
Content: Filtering algorithm
52
Aim:devise arecursivefiltering algorithm oftheform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t))
X
XA
ApplyBayes'rule
e3
e4
P(Xt+1e1:t+1) = P(Xt+1le1:t
Applyconditionalindependence
et+1)
(+x)=
e1:
ConditiononX
=αP(et+11Xt+1)P(Xt+1 e1:t)
P(xt1 e1:t) P(Xt+11 Xt e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 40}
Content: Filtering algorithm
Aim:devise arecursivefiltering algorithm of theform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
ApplyBayes'rule
e3
ea
P(Xt+1le1:t+1) = P(Xt+1le1:t
Applyconditional independence
et+1)
=α P(e+11Xt+1 e1:t)
e1:l
Condition onX
^t+1
=α P(et+11Xt+1) P(Xt+11 e1:t)
Applyconditional
independence
= α P(et+1 lXt+1)
P(xt I e1:t) P(Xt+11 Xt e1:t)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 41}
Content: Filtering algorithm
Aim:devise arecursivefilteringalgorithm oftheform
P(Xt+1 le1:t+1) = g(et+1, P(Xtle1:t) )
X1
XA
ApplyBayes'rule
e3
e4
P(Xt+1le1:t+1) = P(X+1le1:t
Applyconditionalindependence
et+1)
= α P(et+11Xt+1 e1:t)t+1l
ConditiononX
= αP(et+11Xt+1) P(Xt+1 e1:t)
Applyconditional
independence
= α P(et+1IXt+1)
P(xt 1 e1:t) P(Xt+1xt e1:t)
=α P(et+1IXt+1)
P(xtI e1:t) P(Xt+1
xt)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 10 Part 2.pdf', 'page': 42}
Content: Filtering algorithm
55
Aim:devisearecursivefilteringalgorithmoftheform
P(Xt+1le1:t+1) = g(et+1, P(Xtle1:t) )
XA
e3
e4
P(Xt+1le1:t+1)=P(X+11e1:t et+1)
e
(2 +x) (+x+) =
= α P(et+1X+1) P(Xt+1 e1:t)
P(xt 1 e1:t) P(Xt+11 xt e1:t)
P(xt 1 e1:t) P(Xt+11 xt)
GivenbyHMM
Pre-computed
GivenbyHMM

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 4
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Non-Deterministic Search
Picture a runner, coming to the end of his first ever marathon. Though it seems likely he will complete
the race and claim the accompanying everlasting glory, it’s by no means guaranteed.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 0}
Content: He may pass out
from exhaustion or misstep and slip and fall, tragically breaking both of his legs. Even more unlikely, a
literally earth-shattering earthquake may spontaneously occur, swallowing up the runner mere inches before
he crosses the finish line. Such possibilities add a degree of uncertainty to the runner’s actions, and it’s this
uncertainty that will be the subject of the following discussion. In the first note, we talked about traditional
search problems and how to solve them; then, in the third note, we changed our model to account for
adversaries and other agents in the world that influenced our path to goal states. Now, we’ll change our
model again to account for another influencing factor – the dynamics of world itself. The environment in
which an agent is placed may subject the agent’s actions to being nondeterministic, which means that there
are multiple possible successor states that can result from an action taken in some state. This is, in fact, the
case in many card games such as poker or blackjack, where there exists an inherent uncertainty from the
randomness of card dealing. Such problems where the world poses a degree of uncertainty are known as
nondeterministic search problems, and can be solved with models known as Markov decision processes,
or MDPs. Markov Decision Processes
A Markov Decision Process is defined by several properties:
• A set of states S. States in MDPs are represented in the same way as states in traditional search
problems. • A set of actions A. Actions in MDPs are also represented in the same way as in traditional search
problems. • A start state.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 0}
Content: • Possibly one or more terminal states. • Possibly a discount factor g. We’ll cover discount factors shortly. • A transition function T(s,a,s0). Since we have introduced the possibility of nondeterministic actions,
we need a way to delineate the likelihood of the possible outcomes after taking any given action from
any given state. The transition function for a MDP does exactly this - it’s a probability function which
represents the probability that an agent taking an action a 2 A from a state s 2 S ends up in a state
s0 2 S. CS 188, Fall 2018, Note 4 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 1}
Content: • A reward function R(s,a,s0
). Typically, MDPs are modeled with small "living" rewards at each step
to reward an agent’s survival, along with large rewards for arriving at a terminal state. Rewards may
be positive or negative depending on whether or not they benefit the agent in question, and the agent’s
objective is naturally to acquire the maximum reward possible before arriving at some terminal state. Constructing a MDP for a situation is quite similar to constructing a state-space graph for a search problem,
with a couple additional caveats. Consider the motivating example of a racecar:
There are three possible states, S = {cool,warm,overheated}, and two possible actions A = {slow, f ast}. Just like in a state-space graph, each of the three states is represented by a node, with edges representing
actions. Overheated is a terminal state, since once a racecar agent arrives at this state, it can no longer
perform any actions for further rewards (it’s a sink state in the MDP and has no outgoing edges). Notably,
for nondeterministic actions, there are multiple edges representing the same action from the same state with
differing successor states. Each edge is annotated not only with the action it represents, but also a transition
probability and corresponding reward. These are summarized below:
• Transition Function: T(s,a,s0)
– T(cool,slow, cool) = 1
– T(warm,slow, cool) = 0.5
– T(warm,slow,warm) = 0.5
– T(cool, f ast, cool) = 0.5
– T(cool, f ast,warm) = 0.5
– T(warm, f ast,overheated) = 1
• Reward Function: R(s,a,s0)
– R(cool,slow, cool) = 1
– R(warm,slow, cool) = 1
– R(warm,slow,warm) = 1
– R(cool, f ast, cool) = 2
– R(cool, f ast,warm) = 2
– R(warm, f ast,overheated) = 10
We represent the movement of an agent through different MDP states over time with discrete timesteps,
defining st 2 S and at 2 A as the state in which an agent exists and the action which an agent takes at
timestep t respectively. An agent starts in state s0 at timestep 0, and takes an action at every timestep. The
movement of an agent through a MDP can thus be modeled as follows:
s0
a0 ! s1
a1 ! s2a2 ! s3
a3 !

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 1}
Content: ... Additionally, knowing that an agent’s goal is to maximize it’s reward across all timesteps, we can correspondingly express this mathematically as a maximization of the following utility function:
U([s0,a0,s1,a1,s2,...]) = R(s0,a0,s1) +R(s1,a1,s2) +R(s2,a2,s3) +... CS 188, Fall 2018, Note 4 2
0.5
+1
1.0
Fast
Slow
-10
+1
0.5
Warm
Slow
Fast
0.5
+2
Cool
0.5
+1
Overheated
1.0
+2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 2}
Content: Markov decision processes, like state-space graphs, can be unraveled into search trees. Uncertainty is modeled in these search trees with q-states, also known as action states, essentially identical to expectimax
chance nodes. This is a fitting choice, as q-states use probabilities to model the uncertainty that the environment will land an agent in a given state just as expectimax chance nodes use probabilities to model the
uncertainty that adversarial agents will land our agent in a given state through the move these agents select. The q-state represented by having taken action a from state s is notated as the tuple (s,a). Observe the unraveled search tree for our racecar, truncated to depth-2:
The green nodes represent q-states, where an action has been taken from a state but has yet to be resolved
into a successor state. It’s important to understand that agents spend zero timesteps in q-states, and that they
are simply a construct created for ease of representation and development of MDP algorithms. Finite Horizons and Discounting
There is an inherent problem with our racecar MDP - we haven’t placed any time constraints on the number
of timesteps for which a racecar can take actions and collect rewards. With our current formulation, it could
routinely choose a = slow at every timestep forever, safely and effectively obtaining infinite reward without
any risk of overheating. This is prevented by the introduction of finite horizons and/or discount factors. An MDP enforcing a finite horizon is simple - it essentially defines a "lifetime" for agents, which gives them
some set number of timesteps n to accrue as much reward as they can before being automatically terminated. We’ll return to this concept shortly.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 2}
Content: Discount factors are slightly more complicated, and are introduced to model an exponential decay in the
value of rewards over time. Concretely, with a discount factor of g, taking action at from state st at timestep
t and ending up in state st+1 results in a reward of gtR(st,at,st+1) instead of just R(st,at,st+1). Now, instead
of maximizing the additive utility
U([s0,a0,s1,a1,s2,...]) = R(s0,a0,s1) +R(s1,a1,s2) +R(s2,a2,s3) +... we attempt to maximize discounted utility
U([s0,a0,s1,a1,s2,...]) = R(s0,a0,s1) +gR(s1,a1,s2) +g2R(s2,a2,s3) +... Noting that the above definition of a discounted utility function looks dangerously close to a geometric
series with ratio g, we can prove that it’s guaranteed to be finite-valued as long as the constraint |g| < 1
CS 188, Fall 2018, Note 4 3
0.5
0.5
1.0
1.0
0.5
0.5
1.0
0.5
0.5
0.5
0.5
1.0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 3}
Content: (where |n| denotes the absolute value operator) is met through the following logic:
U([s0,s1,s2,...]) = R(s0,a0,s1) +gR(s1,a1,s2) +g2R(s2,a2,s3) +... =
•
Â
t=0
gtR(st,at,st+1) 
•
Â
t=0
gtRmax = Rmax
1g
where Rmax is the maximum possible reward attainable at any given timestep in the MDP. Typically, g is
selected strictly from the range 0 < g < 1 since values values in the range 1 < g  0 are simply not
meaningful in most real-world situations - a negative value for g means the reward for a state s would
flip-flop between positive and negative values at alternating timesteps. Markovianess
Markov decision processes are "markovian" in the sense that they satisfy the Markov property, or memoryless property, which states that the future and the past are conditionally independent, given the present. Intuitively, this means that, if we know the present state, knowing the past doesn’t give us any more information about the future.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 3}
Content: To express this mathematically, consider an agent that has visited states s0,s1,...,st
after taking actions a0,a1,...,at1 in some MDP, and has just taken action at. The probability that this agent
then arrives at state st+1 given their history of previous states visited and actions taken can be written as
follows:
P(St+1 = st+1|St = st,At = at,St1 = st1,At1 = at1,...,S0 = s0)
where each St denotes the random variable representing our agent’s state and At denotes the random variable
representing the action our agent takes at time t. The Markov property states that the above probability can
be simplified as follows:
P(St+1 = st+1|St = st,At = at,St1 = st1,At1 = at1,...,S0 = s0) = P(St+1 = st+1|St = st,At = at)
which is "memoryless" in the sense that the probability of arriving in a state s0 at time t +1 depends only on
the state s and action a taken at time t, not on any earlier states or actions. In fact, it is these memoryless
probabilities which are encoded by the transition function: T(s,a,s
0
) = P(s
0
|s,a) . Solving Markov Decision Processes
Recall that in deterministic, non-adversarial search, solving a search problem means finding an optimal plan
to arrive at a goal state. Solving a Markov decision process, on the other hand, means finding an optimal
policy p⇤ : S ! A, a function mapping each state s 2 S to an action a 2 A. An explicit policy p defines a
reflex agent - given a state s, an agent at s implementing p will select a = p(s) as the appropriate action to
make without considering future consequences of its actions. An optimal policy is one that if followed by
the implementing agent, will yield the maximum expected total reward or utility. Consider the following MDP with S = {a,b, c,d, e}, A = {East,West,Exit} (with Exit being a valid
action only in states a and e and yielding rewards of 10 and 1 respectively), a discount factor g = 0.1, and
deterministic transitions:
CS 188, Fall 2018, Note 4 4
10
1
b
d
a
C
e

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 4}
Content: Two potential policies for this MDP are as follows:
(a) Policy 1 (b) Policy 2
With some investigation, it’s not hard to determine that Policy 2 is optimal. Following the policy until
making action a = Exit yields the following rewards for each start state:
Start State Reward
a 10
b 1
c 0.1
d 0.1
e 1
We’ll now learn how to solve such MDPs (and much more complex ones!) algorithmically using the
Bellman equation for Markov decision processes. The Bellman Equation
In order to talk about the Bellman equation for MDPs, we must first introduce two new mathematical quantities:
• The optimal value of a state s, V⇤(s) – the optimal value of s is the expected value of the utility an
optimally-behaving agent that starts in s will receive, over the rest of the agent’s lifetime. • The optimal value of a q-state (s,a), Q⇤(s,a) - the optimal value of (s,a) is the expected value of the
utility an agent receives after starting in s, taking a, and acting optimally henceforth. Using these two new quantities and the other MDP quantities discussed earlier, the Bellman equation is
defined as follows:
V⇤(s) = maxa Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
Before we begin interpreting what this means, let’s also define the equation for the optimal value of a q-state
(more commonly known as an optimal q-value):
Q⇤(s,a) = Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
Note that this second definition allows us to reexpress the Bellman equation as
V⇤(s) = maxa Q⇤(s,a)
which is a dramatically simpler quantity. The Bellman equation is an example of a dynamic programming equation, an equation that decomposes a problem into smaller subproblems via an inherent recursive structure.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 4}
Content: We can see this inherent recursion in the equation for the q-value of a state, in the term
CS 188, Fall 2018, Note 4 5
EXIT
EXIT
b
d
a
C
eEXIT
EXIT
b
d
a
C
e

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 5}
Content: [R(s,a,s0
) +gV⇤(s0)]. This term represents the total utility an agent receives by first taking a from s and arriving at s0 and then acting optimally henceforth. The immediate reward from the action a taken, R(s,a,s0
),
is added to the optimal reward attainable from s0, V⇤(s0), which is discounted by g to account for the passage
of the timestep in taking a. Though in most cases there exists a vast number of possible sequences of states
and actions from s0 to some terminal state, all this detail is abstracted away and encapsulated in a single
recursive value, V⇤(s0). We can now take another step outwards and consider the full equation for q-value. Knowing [R(s,a,s0) +
gV⇤(s0)] represents the utility attained by acting optimally after arriving in state s0 from q-state (s,a), it
becomes evident that the quantity
Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
is simply a weighted sum of utilities, with each utility weighted by its probability of occurrence. This is definitionally the expected utility of acting optimally from q-state (s,a) onwards! This completes our analysis
and gives us enough insight to interpret the full Bellman equation - the optimal value of a state, V⇤(s), is
simply the maximum expected utility over all possible actions from s. Computing maximum expected utility
for a state s is essentially the same as running expectimax - we first compute the expected utility from each
q-state (s,a) (equivalent to computing the value of chance nodes), then compute the maximum over these
nodes to compute the maximum expected utility (equivalent to computing the value of a maximizer node). One final note on the Bellman equation – its usage is as a condition for optimality.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 5}
Content: In other words, if we can
somehow determine a value V(s) for every state s 2 S such that the Bellman equation holds true for each
of these states, we can conclude that these values are the optimal values for their respective states. Indeed,
satisfying this condition implies 8s 2 S, V(s) = V⇤(s). Value Iteration
Now that we have a framework to test for optimality of the values of states in a MDP, the natural follow-up
question to ask is how to actually compute these optimal values. To answer this question, we need timelimited values (the natural result of enforcing finite horizons). The time-limited value for a state s with a
time-limit of k timesteps is denoted Vk(s), and represents the maximum expected utility attainable from s
given that the Markov decision process under consideration terminates in k timesteps. Equivalently, this is
what a depth-k expectimax run on the search tree for a MDP returns. Value iteration is a dynamic programming algorithm that uses an iteratively longer time limit to compute
time-limited values until convergence (that is, until the V values are the same for each state as they were in
the past iteration: 8s,Vk+1(s) = Vk(s)). It operates as follows:
1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 5}
Content: 8s 2 S, initialize V0(s) = 0. This should be intuitive, since setting a time limit of 0 timesteps means
no actions can be taken before termination, and so no rewards can be acquired. 2. Repeat the following update rule until convergence:
8s 2 S, Vk+1(s) maxa Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gVk(s
0
)]
At iteration k of value iteration, we use the time-limited values for with limit k for each state to generate the time-limited values with limit (k +1). In essence, we use computed solutions to subproblems
(all the Vk(s)) to iteratively build up solutions to larger subproblems (all the Vk+1(s)); this is what
makes value iteration a dynamic programming algorithm. CS 188, Fall 2018, Note 4 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 6}
Content: Note that though the Bellman equation looks essentially identical in construction to the update rule above,
they are not the same. The Bellman equation gives a condition for optimality, while the update rule gives a
method to iteratively update values until convergence. When convergence is reached, the Bellman equation
will hold for every state: 8s 2 S, Vk(s) = Vk+1(s) = V⇤(s). Let’s see a few updates of value iteration in practice by revisiting our racecar MDP from earlier, introducing
a discount factor of g = 0.5:
We begin value iteration by initialization of all V0(s) = 0:
cool warm overheated
V0 0 0 0
In our first round of updates, we can compute 8s 2 S, V1(s) as follows:
V1(cool) = max{1 · [1+0.5 · 0], 0.5 · [2+0.5 · 0] +0.5 · [2+0.5 · 0]}
= max{1,2}
= 2
V1(warm) = max{0.5 · [1+0.5 · 0] +0.5 · [1+0.5 · 0], 1 · [10+0.5 · 0]}
= max{1,10}
= 1
V1(overheated) = max{}
= 0
cool warm overheated
V0 0 0 0
V1 2 1 0
Similarly, we can repeat the procedure to compute a second round of updates with our newfound values for
CS 188, Fall 2018, Note 4 7
0.5
+1
1.0
Fast
Slow
-10
+1
0.5
Warm
Slow
Fast
0.5
+2
Cool
0.5
+1
Overheated
1.0
+2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 7}
Content: V1(s) to compute V2(s). V2(cool) = max{1 · [1+0.5 · 2], 0.5 · [2+0.5 · 2] +0.5 · [2+0.5 · 1]}
= max{2,2.75}
= 2.75
V2(warm) = max{0.5 · [1+0.5 · 2] +0.5 · [1+0.5 · 1], 1 · [10+0.5 · 0]}
= max{1.75,10}
= 1.75
V2(overheated) = max{}
= 0
cool warm overheated
V0 0 0 0
V1 2 1 0
V2 2.75 1.75 0
It’s worthwhile to observe that V⇤(s) for any terminal state must be 0, since no actions can ever be taken
from any terminal state to reap any rewards. Policy Extraction
Recall that our ultimate goal in solving a MDP is to determine an optimal policy.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 7}
Content: This can be done once
all optimal values for states are determined using a method called policy extraction. The intuition behind
policy extraction is very simple: if you’re in a state s, you should take the action a which yields the maximum
expected utility. Not surprisingly, a is the action which takes us to the q-state with maximum q-value,
allowing for a formal definition of the optimal policy:
8s 2 S, p⇤(s) = argmax aQ⇤(s,a) = argmax a Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
It’s useful to keep in mind for performance reasons that it’s better for policy extraction to have the optimal
q-values of states, in which case a single argmax operation is all that is required to determine the optimal
action from a state. Storing only each V⇤(s) means that we must recompute all necessary q-values with the
Bellman equation before applying argmax, equivalent to performing a depth-1 expectimax. Policy Iteration
Value iteration can be quite slow. At each iteration, we must update the values of all |S| states (where |n|
refers to the cardinality operator), each of which requires iteration over all |A| actions as we compute the
q-value for each action. The computation of each of these q-values, in turn, requires iteration over each of
the |S| states again, leading to a poor runtime of O(|S|
2|A|). Additionally, when all we want to determine
is the optimal policy for the MDP, value iteration tends to do a lot of overcomputation since the policy as
computed by policy extraction generally converges significantly faster than the values themselves. The fix
for these flaws is to use policy iteration as an alternative, an algorithm that maintains the optimality of value
iteration while providing significant performance gains. Policy iteration operates as follows:
1. Define an initial policy. This can be arbitrary, but policy iteration will converge faster the closer the
initial policy is to the eventual optimal policy. CS 188, Fall 2018, Note 4 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 8}
Content: 2. Repeat the following until convergence:
• Evaluate the current policy with policy evaluation. For a policy p, policy evaluation means
computing Vp (s) for all states s, where Vp (s) is expected utility of starting in state s when
following p:
Vp (s) = Â
s0
T(s,p(s),s
0
)[R(s,p(s),s
0
) +gVp (s
0
)]
Define the policy at iteration i of policy iteration as pi. Since we are fixing a single action for
each state, we no longer need the max operator which effectively leaves us with a system of |S|
equations generated by the above rule. Each Vpi(s) can then be computed by simply solving
this system. Alternatively, we can also compute Vpi(s) by using the following update rule until
convergence, just like in value iteration:
Vpi
k+1(s) Â
s0
T(s,pi(s),s
0
)[R(s,pi(s),s
0
) +gVpi
k (s
0
)]
However, this second method is typically slower in practice. • Once we’ve evaluated the current policy, use policy improvement to generate a better policy. Policy improvement uses policy extraction on the values of states generated by policy evaluation
to generate this new and improved policy:
pi+1(s) = argmax a Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gVpi(s
0
)]
If pi+1 = pi, the algorithm has converged, and we can conclude that pi+1 = pi = p⇤. Let’s run through our racecar example one last time (getting tired of it yet?) to see if we get the same policy
using policy iteration as we did with value iteration. Recall that we were using a discount factor of g = 0.5. We start with an initial policy of Always go slow:
cool warm overheated
p0 slow slow 
Because terminal states have no outgoing actions, no policy can assign a value to one. Hence, it’s reasonable
to disregard the state overheated from consideration as we have done, and simply assign 8i, Vpi(s) = 0 for
CS 188, Fall 2018, Note 4 9
0.5
+1
1.0
Fast
Slow
-10
+1
0.5
Warm
Slow
Fast
0.5
+2
Cool
0.5
+1
Overheated
1.0
+2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 9}
Content: any terminal state s. The next step is to run a round of policy evaluation on p0:
Vp0 (cool) = 1 · [1+0.5 ·Vp0 (cool)]
Vp0 (warm) = 0.5 · [1+0.5 ·Vp0 (cool)] +0.5 · [1+0.5 ·Vp0 (warm)]
Solving this system of equations for Vp0 (cool) and Vp0 (warm) yields:
cool warm overheated
Vp0 2 2 0
We can now run policy extraction with these values:
p1(cool) = argmax{slow : 1 · [1+0.5 · 2], f ast : 0.5 · [2+0.5 · 2] +0.5 · [2+0.5 · 2]}
= argmax{slow : 2, f ast : 3}
= f ast
p1(warm) = argmax{slow : 0.5 · [1+0.5 · 2] +0.5 · [1+0.5 · 2], f ast : 1 · [10+0.5 · 0]}
= argmax{slow : 3, f ast : 10}
= slow
Running policy iteration for a second round yields p2(cool) = f ast and p2(warm) = slow. Since this is the
same policy as p1, we can conclude that p1 = p2 = p⇤.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 9}
Content: Verify this for practice! cool warm
p0 slow slow
p1 f ast slow
p2 f ast slow
This example shows the true power of policy iteration: with only two iterations, we’ve already arrived at the
optimal policy for our racecar MDP! This is more than we can say for when we ran value iteration on the
same MDP, which was still several iterations from convergence after the two updates we performed. Summary
The material presented above has much opportunity for confusion. We covered value iteration, policy iteration, policy extraction, and policy evaluation, all of which look similar, using the Bellman equation with
subtle variation. Below is a summary of when to use each algorithm:
• Value iteration: Used for computing the optimal values of states, by iterative updates until convergence. • Policy evaluation: Used for computing the values of states under a specific policy. • Policy extraction: Used for determining a policy given some state value function. If the state values
are optimal, this policy will be optimal. This method is used after running value iteration, to compute
an optimal policy from the optimal state values; or as a subroutine in policy iteration, to compute the
best policy for the currently estimated state values. CS 188, Fall 2018, Note 4 10


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11 Notes.pdf', 'page': 10}
Content: • Policy iteration: A technique that encapsulates both policy evaluation and policy extraction and is
used for iterative convergence to an optimal policy. It tends to outperform value iteration, by virtue of
the fact that policies usually converge much faster than the values of states. CS 188, Fall 2018, Note 4 11


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 0}
Content: Artificial Intelligence
Markov Decision Processes
[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.]
l

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 1}
Content: Non-Deterministic Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 2}
Content: Example: Grid World
 A maze-like problem
 The agent lives in a grid
 Walls block the agent’s path
 Noisy movement: actions do not always go as planned
 80% of the time, the action North takes the agent North 
(if there is no wall there)
 10% of the time, North takes the agent West; 10% East
 If there is a wall in the direction the agent would have 
been taken, the agent stays put
 The agent receives rewards each time step
 Small “living” reward each step (can be negative)
 Big rewards come at the end (good or bad)
 Goal: maximize sum of rewards
3
+1
2
-1
1
START
1
2
3
4+1
-1
STARTT

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 3}
Content: Grid World Actions
Deterministic Grid World Stochastic Grid World


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 4}
Content: Markov Decision Processes
 An MDP is defined by:
 A set of states s  S
 A set of actions a  A
 A transition function T(s, a, s’)
 Probability that a from s leads to s’, i.e., P(s’| s, a)
 Also called the model or the dynamics
 A reward function R(s, a, s’) 
 Sometimes just R(s) or R(s’)
 A start state
 Maybe a terminal state
 MDPs are non-deterministic search problems
 One way to solve them is with expectimax search
 We’ll have a new tool soon
3
+1
2
-1
1
START
1
2
3
4+1
-1
STARTT

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 5}
Content: Video of Demo Gridworld Manual Intro


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 6}
Content: What is Markov about MDPs?  “Markov” generally means that given the present state, the 
future and the past are independent
 For Markov decision processes, “Markov” means action 
outcomes depend only on the current state
 This is just like search, where the successor function could only 
depend on the current state (not the history)
Andrey Markov 
(1856-1922)
P(St
At-1,...So
= St, At
= αt,
= S0
t+1
S
t-1
川
St-1,P(St+1
St,At
at
川
川

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 7}
Content: 
"Markov" as in Markov Chains? HMMs? Markov Chain
E
E
3
HiddenMarkovModel
??? Ro
PartiallyObservable
MarkovDecisionProcess
MarkovDecisionProcess

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 8}
Content: Policies
Optimal policy when R(s, a, s’) = -0.03 
for all non-terminals s
 In deterministic single-agent search problems, 
we wanted an optimal plan, or sequence of 
actions, from start to a goal
 For MDPs, we want an optimal policy *: S → A
 A policy  gives an action for each state
 An optimal policy is one that maximizes 
expected utility if followed
 An explicit policy defines a reflex agent
 Expectimax didn’t compute entire policies
 It computed the action for a single state only
3
+1
2
-1
1
1
2
3
41
1
1
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 9}
Content: Optimal Policies
R(s) = -0.4 R(s) = -2.0
R(s) = -0.01 R(s) = -0.03
+1
-1+1
-1+1
-1+1
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 10}
Content: Example: Racing


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 11}
Content: Example: Racing
 A robot car wants to travel far, quickly
 Three states: Cool, Warm, Overheated
 Two actions: Slow, Fast
 Going faster gets double reward
Cool
Warm
Overheated
Fast
Fast
Slow
Slow
0.5 
0.5 
0.5 
0.5 
1.0 
1.0 
+1 
+1 
+1 
+2 
+2 
-10
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 12}
Content: 
Example: Racing
s'
T(s,a,s')
a
R(s,a,s')
S
Slow
1.0
+1
Fast
0.5
+2
Fast
0.5
+2
Slow
0.5
+1
0.5
1.0
Fast
Slow
0.5
+1
Slow
-10
0.5
Fast
1.0
-10
Warm
Slow
Fast
0.5
+2
(end)
1.0
0
Cool
0.5
Overheated
1.0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 13}
Content: Racing Search Tree


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 14}
Content: MDP Search Trees
 Each MDP state projects an expectimax-like search tree
a
s
s’
s, a
(s,a,s’) called a transition
T(s,a,s’) = P(s’|s,a)
R(s,a,s’)
s,a,s’
s is a state
(s, a) is a qstate


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 15}
Content: Utilities of Sequences


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 16}
Content: Utilities of Sequences
 What preferences should an agent have over reward sequences?  More or less?  Now or later? [1, 2, 2] or [2, 3, 4]
[0, 0, 1] or [1, 0, 0]


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 17}
Content: Discounting
 It’s reasonable to maximize the sum of rewards
 It’s also reasonable to prefer rewards now to rewards later
 One solution: values of rewards decay exponentially
Worth Now Worth Next Step Worth In Two Steps
D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 18}
Content: Discounting
 How to discount?  Each time we descend a level, we 
multiply in the discount once
 Why discount?  Sooner rewards probably do have 
higher utility than later rewards
 Also helps our algorithms converge
 Example: discount of 0.5
 U([1,2,3]) = 1*1 + 0.5*2 + 0.25*3
 U([1,2,3]) < U([3,2,1])
D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 19}
Content: Stationary Preferences
 Theorem: if we assume stationary preferences:
 Then: there are only two ways to define utilities
 Additive utility:
 Discounted utility:
>
Y
7U([ro, r1, r2, ...])
ro + r1
+ r2 ++ 2r2
U([ro, ri, r2,...])
ro +r1
三1[bi,6
b2,. a1,(
a2,:
人[r, b
b1,b
b2,·
[r, a1, a2,. . 人

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 20}
Content: Quiz: Discounting
 Given:
 Actions: East, West, and Exit (only available in exit states a, e)
 Transitions: deterministic
 Quiz 1: For  = 1, what is the optimal policy?  Quiz 2: For  = 0.1, what is the optimal policy?  Quiz 3: For which  are West and East equally good when in state d? 10
C
L
a
b
C
d
e10
L10
L

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 21}
Content: Infinite Utilities?!  Problem: What if the game lasts forever? Do we get infinite rewards?  Solutions:
 Finite horizon: (similar to depth-limited search)
 Terminate episodes after a fixed T steps (e.g. life)
 Gives nonstationary policies ( depends on time left)
 Discounting: use 0 <  < 1
 Smaller  means smaller “horizon” – shorter term focus
 Absorbing state: guarantee that for every policy, a terminal state will eventually 
be reached (like “overheated” for racing)
8
U([ro, 
roo])
M
Rmax/(1
一
0=#

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 22}
Content: Recap: Defining MDPs
 Markov decision processes:
 Set of states S
 Start state s0
 Set of actions A
 Transitions P(s’|s,a) (or T(s,a,s’))
 Rewards R(s,a,s’) (and discount )
 MDP quantities so far:
 Policy = Choice of action for each state
 Utility = sum of (discounted) rewards
a
s
s, a
s,a,s’
s’


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 23}
Content: Solving MDPs
1
1
1
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 24}
Content: Optimal Quantities
 The value (utility) of a state s:
V
*
(s) = expected utility starting in s and 
acting optimally
 The value (utility) of a q-state (s,a):
Q*(s,a) = expected utility starting out 
having taken action a from state s and 
(thereafter) acting optimally
 The optimal policy:

*
(s) = optimal action from state s
a
s
s’
s, a
(s,a,s’) is a 
transition
s,a,s’
s is a 
state
(s, a) is a 
q-state
[Demo – gridworld values (L8D4)]


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 25}
Content: Snapshot of Demo – Gridworld V Values
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64
0.74
0.85
1.00
0.57
0.57
-1.00
0.49
< 0.43
0.48
1 0.28
VALUES
AFTER 10O ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 26}
Content: Snapshot of Demo – Gridworld Q Values
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.59
0.67
0.77
0.57
0.640.60
0.740.66
0.85
1.00
0.53
0.67
0.57
0.57
0.57
0.51
0.51
0.53
-0.60
-1.00
0.46
0.30
0.49
0.40
0.48
-0.65
0.45
0.410.43
0.420.40
0.290.28
0.13
0.44
0.40
0.41
0.27
Q-VALUES
AFTER
100
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 27}
Content: Values of States
 Fundamental operation: compute the (expectimax) value of a state
 Expected utility under optimal action
 Average sum of (discounted) rewards
 This is just what expectimax computed!  Recursive definition of value:
a
s
s, a
s,a,s’
s’
max
Q
*
S
S
a
？
aT(s,a, s')
R(s,a,s') +V*(s')
*/
(s)
max
a
s'Q*(s,a)=∑
T(s,a, s')
R(s,a, s') +V*(s)
s1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 28}
Content: Racing Search Tree


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 29}
Content: Racing Search Tree


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 30}
Content: Racing Search Tree
 We’re doing way too much 
work with expectimax!  Problem: States are repeated 
 Idea: Only compute needed 
quantities once
 Problem: Tree goes on forever
 Idea: Do a depth-limited 
computation, but with increasing 
depths until change is small
 Note: deep parts of the tree 
eventually don’t matter if γ < 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 31}
Content: Time-Limited Values
 Key idea: time-limited values
 Define Vk(s) to be the optimal value of s if the game ends 
in k more time steps
 Equivalently, it’s what a depth-k expectimax would give from s
[Demo – time-limited values (L8D6)]
k2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 32}
Content: k=0
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
VALUES AFTER
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 33}
Content: k=1
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.00
0.00
0.00
1.00
0.00
0.00
-1.00
0.00
0.00
0.00
0.00
VALUES AFTER 1
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 34}
Content: k=2
Noise = 0.2
Discount = 0.9
Living reward = 0
Gridworld Display
0.00
0.00
0.72
1.00
0.00
0.00
-1.00
0.00
0.00
0.00
0.00
VALUES AFTER
2
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 35}
Content: k=3
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.00 1
0.52 
0.78
1.00
0.00
0.43
-1.00
0.00
0.00
0.00
0.00
VALUES
AFTER 3
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 36}
Content: k=4
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.37
0.66 1
0.831
1.00
0.00
0.51
-1.00
0.00
0.00 1
0.31
0.00
VALUES
AFTER
4
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 37}
Content: k=5
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.51 
0.72 
0.84
1.00
0.27
0.55
-1.00
0.00
0.22 
0.37
0.13
VALUES AFTER
5
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 38}
Content: k=6
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.59
0.73 
0.85
1.00
0.41
0.57
-1.00
0.21
0.31 
0.43
10.19
VALUES AFTER
6
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 39}
Content: k=7
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.62 
0.74 
0.85
1.00
0.50
0.57
-1.00
0.34
0.36 
0.45
0.24
VALUES
AFTER
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 40}
Content: k=8
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.63 
0.74 
0.85
1.00
0.53
0.57
-1.00
0.42
0.39 
0.46
1 0.26
VALUES
AFTER
8
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 41}
Content: k=9
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64 1
0.74 
0.85
1.00
0.55
0.57
-1.00
0.46
0.40 ~
0.47
1 0.27
VALUES AFTER
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 42}
Content: k=10
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64 
0.74 1
0.85
1.00
0.56
0.57
-1.00
0.48
< 0.41
0.47
1 0.27
VALUES
AFTER
 10
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 43}
Content: k=11
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64 
0.74 
0.85
1.00
0.56
0.57
-1.00
0.48
< 0.42
0.47
1 0.27
VALUES
AFTER
11
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 44}
Content: k=12
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64 1
0.74 
0.85 
1.00
0.57
0.57
-1.00
0.49
< 0.42
0.47
< 0.28
VALUES
AFTER
 12
ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 45}
Content: k=100
Noise = 0.2
Discount = 0.9
Living reward = 0
000
Gridworld Display
0.64 
0.74 
0.85
1.00
0.57
0.57
-1.00
0.49
< 0.43
0.48
1 0.28
VALUES
AFTER 1OO ITERATIONS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 46}
Content: Computing Time-Limited Values
L
T
0L
T
0L
T
0111222333444

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 47}
Content: The Bellman Equations
How to be optimal:
Step 1: Take correct first action
Step 2: Keep being optimal


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 48}
Content: The Bellman Equations
 Definition of “optimal utility” via expectimax
recurrence gives a simple one-step lookahead
relationship amongst optimal utility values
 These are the Bellman equations, and they characterize 
optimal values in a way we’ll use over and over
a
s
s, a
s,a,s’
s’
max
Q
*
S
I
S
a
aV*(s) =
∑T(s,a, s')
R(s,α, s') +V*(s')
max
a
S1Q*(s,a)=∑
T(s,a, s')
R(s,a, s') +V*(s)
s1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 49}
Content: Time-Limited Values
 Key idea: time-limited values
 Define Vk(s) to be the optimal value of s if the game ends 
in k more time steps
 Equivalently, it’s what a depth-k expectimax would give from s
[Demo – time-limited values (L8D6)]
k2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 50}
Content: 
Example: Value Iteration
s'
T(s,a,s')
R(s,a,s')
S
a
Slow
1.0
+1
Fast
0.5
+2
Fast
0.5
+2
Slow
0.5
+1
Slow
0.5
+1
Fast
1.0
-10
(end)
1.0
0
Vk+1(s) ← maxT(s,a,s) [R(s,a,s') +Vk(s)
Assumenodiscount!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 51}
Content: Example: Value Iteration
0 0 0
2 1 0
3.5 2.5 0
Assume no discount! T
0T
1一
20.5
1.0
Fast
Slow
-10
0.5
Warm
Slow
Fast
0.5
+2
Cool
0.5
1.0
+2∑T(s,a, s′)
R(s,α, s') + Vk(s)
Vk+1(s) ←
max
a
s'

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 52}
Content: Value Iteration
 Bellman equations characterize the optimal values:
 Value iteration computes them:
 Value iteration is just a fixed point solution method
 … though the Vkvectors are also interpretable as time-limited values
a
V(s)
s, a
s,a,s’
V(s’)
∑T(s,a, s)
R(s,a, s') +Vk(s')
V+1(s) ←
max
aV*(s) =
∑T(s,a, s')
R(s,α, s') +V*(s')
max
a
S1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 53}
Content: Value Iteration
N:2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 11.pdf', 'page': 54}
Content: Value Iteration
 Start with V0(s) = 0: no time steps left means an expected reward sum of zero
 Given vector of Vk(s) values, do one ply of expectimax from each state:
 Repeat until convergence
 Complexity of each iteration: O(S2A)
 Theorem: will converge to unique optimal values
 Basic idea: approximations get refined towards optimal values
 Policy may converge long before values do
a
Vk+1(s)
s, a
s,a,s’
Vk(s’)
∑T(s,a, s)
R(s,a, s') +Vk(s')
V+1(s) ←
max
a

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 1
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Agents
In artificial intelligence, the central problem at hand is that of the creation of a rational agent, an entity that
has goals or preferences and tries to perform a series of actions that yield the best/optimal expected outcome
given these goals. Rational agents exist in an environment, which is specific to the given instantiation of
the agent. As a very simple example, the environment for a checkers agent is the virtual checkers board on
which it plays against opponents, where piece moves are actions. Together, an environment and the agents
that reside within it create a world. A reflex agent is one that doesn’t think about the consequences of its actions, but rather selects an action
based solely on the current state of the world. These agents are typically outperformed by planning agents,
which maintain a model of the world and use this model to simulate performing various actions.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 0}
Content: Then, the
agent can determine hypothesized consequences of the actions and can select the best one. This is simulated
"intelligence" in the sense that it’s exactly what humans do when trying to determine the best possible move
in any situation - thinking ahead. State Spaces and Search Problems
In order to create a rational planning agent, we need a way to mathematically express the given environment
in which the agent will exist. To do this, we must formally express a search problem - given our agent’s
current state (its configuration within its environment), how can we arrive at a new state that satisfies its
goals in the best possible way? Formulating such a problem requires four things:
• A state space - The set of all possible states that are possible in your given world
• A successor function - A function that takes in a state and an action and computes the cost of performing that action as well as the successor state, the state the world would be in if the given agent
performed that action
• A start state - The state in which an agent exists initially
• A goal test - A function that takes a state as input, and determines whether it is a goal state
Fundamentally, a search problem is solved by first considering the start state, then exploring the state space
using the successor function, iteratively computing successors of various states until we arrive at a goal state,
at which point we will have determined a path from the start state to the goal state (typically called a plan). The order in which states are considered is determined using a predetermined strategy. We’ll cover types
of strategies and their usefulness shortly. Before we continue with how to solve search problems, it’s important to note the difference between a world
state, and a search state. A world state contains all information about a given state, whereas a search state
CS 188, Fall 2018, Note 1 1
CS-370 Artificial Intelligence
Fall 2023
CS-370, Fall 2023, Note 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: contains only the information about the world that’s necessary for planning (primarily for space effiency
reasons). To illustrate these concepts, we’ll introduce the hallmark motivating example of this course -
Pacman. The game of Pacman is simple: Pacman must navigate a maze and eat all the (small) food pellets
in the maze without being eaten by the malicious patrolling ghosts. If Pacman eats one of the (large) power
pellets, he becomes ghost-immune for a set period of time and gains the ability to eat ghosts for points. Let’s consider a variation of the game in which the maze contains only Pacman and food pellets. We can
pose two distinct search problems in this scenario: pathing and eat-all-dots. Pathing attempts to solve
the problem of getting from position (x1, y1) to position (x2, y2) in the maze optimally, while eat all dots
attempts to solve the problem of consuming all food pellets in the maze in the shortest time possible. Below,
the states, actions, successor function, and goal test for both problems are listed:
• Pathing
– States: (x,y) locations
– Actions: North, South, East, West
– Successor: Update location only
– Goal test: Is (x,y)=END? • Eat-all-dots
– States: (x,y) location, dot booleans
– Actions: North, South, East, West
– Successor: Update location and booleans
– Goal test: Are all dot booleans false? Note that for pathing, states contain less information than states for eat-all-dots, because for eat-all-dots we
must maintain an array of booleans corresponding to each food pellet and whether or not it’s been eaten in
the given state. A world state may contain more information still, potentially encoding information about
things like total distance traveled by Pacman or all positions visited by Pacman on top of its current (x,y)
location and dot booleans. State Space Size
An important question that often comes up while estimating the computational runtime of solving a search
problem is the size of the state space. This is done almost exclusively with the fundamental counting
principle, which states that if there are n variable objects in a given world which can take on x1, x2, ..., xn
different values respectively, then the total number of states is x1 · x2 · ...

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 1}
Content: · xn. Let’s use Pacman to show this
concept by example:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 2
84.3.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: Let’s say that the variable objects and their corresponding number of possiblilites are as follows:
• Pacman positions - Pacman can be in 120 distinct (x, y) positions, and there is only one Pacman
• Pacman Direction - this can be North, South, East, or West, for a total of 4 possibilities
• Ghost positions - There are two ghosts, each of which can be in 12 distinct (x, y) positions
• Food pellet configurations - There are 30 food pellets, each of which can be eaten or not eaten
Using the fundamental counting principle, we have 120 positions for Pacman, 4 directions Pacman can be
facing, 12·12 ghost configurations (12 for each ghost), and 2·2·...·2 = 230 food pellet configurations (each
of 30 food pellets has two possible values - eaten or not eaten). This gives us a total state space size of
120 · 4 · 122 · 230 .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 2}
Content: State Space Graphs and Search Trees
Now that we’ve established the idea of a state space and the four components necessary to completely define
one, we’re almost ready to begin solving search problems. The final piece of the puzzle is that of state space
graphs and search trees. Recall that a graph is defined by a set of nodes and a set of edges connecting various pairs of nodes. These
edges may also have weights associated with them. A state space graph is constructed with states representing nodes, with directed edges existing from a state to its successors. These edges represent actions,
and any associated weights represent the cost of performing the corresponding action. Typically, state space
graphs are much too large to store in memory (even our simple Pacman example from above has ⇡ 1013
possible states, yikes!), but they’re good to keep in mind conceptually while solving problems. It’s also
important to note that in a state space graph, each state is represented exactly once - there’s simply no need
to represent a state multiple times, and knowing this helps quite a bit when trying to reason about search
problems. Unlike state space graphs, our next structure of interest, search trees, have no such restriction on the number
of times a state can appear. This is because though search trees are also a class of graph with states as nodes
and actions as edges between states, each state/node encodes not just the state itself, but the entire path
(or plan) from the start state to the given state in the state space graph. Observe the state space graph and
corresponding search tree below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: The highlighted path (S ! d ! e ! r ! f ! G) in the given state space graph is represented in the
corresponding search tree by following the path in the tree from the start state S to the highlighted goal state
G. Similarly, each and every path from the start node to any other node is represented in the search tree by a
path from the root S to some descendant of the root corresponding to the other node. Since there often exist
multiple ways to get from one state to another, states tend to show up multiple times in search trees. As a
result, search trees are greater than or equal to their corresponding state space graph in size. We’ve already determined that state space graphs themselves can be enormous in size even for simple problems, and so the question arises - how can we perform useful computation on these structures if they’re too
big to represent in memory? The answer lies in successor functions - we only store states we’re immediately
working with, and compute new ones on-demand using the corresponding successor function. Typically,
search problems are solved using search trees, where we very carefully store a select few nodes to observe
at a time, iteratively replacing nodes with their successors until we arrive at a goal state. There exist various
methods by which to decide the order in which to conduct this iterative replacement of search tree nodes,
and we’ll present these methods now. Uninformed Search
The standard protocol for finding a plan to get from the start state to a goal state is to maintain an outer
fringe of partial plans derived from the search tree. We continually expand our fringe by removing a node
(which is selected using our given strategy) corresponding to a partial plan from the fringe, and replacing
it on the fringe with all its children. Removing and replacing an element on the fringe with its children
corresponds to discarding a single length n plan and bringing all length (n+1) plans that stem from it into
consideration. We continue this until eventually removing a goal state off the fringe, at which point we
conclude the partial plan corresponding to the removed goal state is in fact a path to get from the start state
to the goal state. Practically, most implementations of such algorithms will encode information about the
parent node, distance to node, and the state inside the node object. This procedure we have just outlined is
known as tree search, and the pseudocode for it is presented below:
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 4
Each NODE in in
State Space Graph
the search tree is
Search Tree
an entire PATH in
the state space
S
graph. n
6
C
a
a
S
We construct both
on demand - and
q
C
G
we construct as
q
C
a
G
little as possible.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 3}
Content: a

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: When we have no knowledge of the location of goal states in our search tree, we are forced to select our
strategy for tree search from one of the techniques that falls under the umbrella of uninformed search. We’ll now cover three such strategies in succession: depth-first search, breadth-first search, and uniform
cost search. Along with each strategy, some rudimentary properties of the strategy are presented as well, in
terms of the following:
• The completeness of each search strategy - if there exists a solution to the search problem, is the
strategy guaranteed to find it given infinite computational resources?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 4}
Content: • The optimality of each search strategy - is the strategy guaranteed to find the lowest cost path to a
goal state? • The branching factor b - The increase in the number of nodes on the fringe each time a fringe node
is dequeued and replaced with its children is O(b). At depth k in the search tree, there exists O(bk)
nodes. • The maximum depth m. • The depth of the shallowest solution s. Depth-First Search
• Description - Depth-first search (DFS) is a strategy for exploration that always selects the deepest
fringe node from the start node for expansion. • Fringe representation - Removing the deepest node and replacing it on the fringe with its children
necessarily means the children are now the new deepest nodes - their depth is one greater than the
depth of the previous deepest node. This implies that to implement DFS, we require a structure that
always gives the most recently added objects highest priority. A last-in, first-out (LIFO) stack does
exactly this, and is what is traditionally used to represent the fringe when implementing DFS. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 5
function TREE-SEARCH(problem, fringe)return a solution,or failure
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loop
do
iffringeisemptythenreturnfailure
node←REMOVE-FRONT(fringe)
if GOAL-TEsT(problem,sTATE[node])then return node
for child-nodeinEXPAND(sTATE[node],problem)do
fringe←INsERT(child-node,fringe)
end
end1 node
b nodes
b2 nodes
m tiers
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: • Completeness - Depth-first search is not complete. If there exist cycles in the state space graph, this
inevitably means that the corresponding search tree will be infinite in depth. Hence, there exists the
possibility that DFS will faithfully yet tragically get "stuck" searching for the deepest node in an
infinite-sized search tree, doomed to never find a solution. • Optimality - Depth-first search simply finds the "leftmost" solution in the search tree without regard
for path costs, and so is not optimal. • Time Complexity - In the worst case, depth first search may end up exploring the entire search tree. Hence, given a tree with maximum depth m, the runtime of DFS is O(bm). • Space Complexity - In the worst case, DFS maintains b nodes at each of m depth levels on the fringe. This is a simple consequence of the fact that once b children of some parent are enqueued, the nature
of DFS allows only one of the subtrees of any of these children to be explored at any given point in
time. Hence, the space complexity of BFS is O(bm). Breadth-First Search
• Description - Breadth-first search is a strategy for exploration that always selects the shallowest fringe
node from the start node for expansion. • Fringe representation - If we want to visit shallower nodes before deeper nodes, we must visit nodes
in their order of insertion. Hence, we desire a structure that outputs the oldest enqueued object to
represent our fringe. For this, BFS uses a first-in, first-out (FIFO) queue, which does exactly this. • Completeness - If a solution exists, then the depth of the shallowest node s must be finite, so BFS
must eventually search this depth. Hence, it’s complete. • Optimality - BFS is generally not optimal because it simply does not take costs into consideration
when determining which node to replace on the fringe. The special case where BFS is guaranteed to
be optimal is if all edge costs are equivalent, because this reduces BFS to a special case of uniform
cost search, which is discussed below. • Time Complexity - We must search 1+b+b2 +...+bs nodes in the worst case, since we go through
all nodes at every depth from 1 to s.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 5}
Content: Hence, the time complexity is O(bs). • Space Complexity - The fringe, in the worst case, contains all the nodes in the level corresponding to
the shallowest solution. Since the shallowest solution is located at depth s, there are O(bs) nodes at
this depth. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 6
1 node
b nodes
s tiers
b2 nodes
bs nodes
bm nodes

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: Uniform Cost Search
• Description - Uniform cost search (UCS), our last strategy, is a strategy for exploration that always
selects the lowest cost fringe node from the start node for expansion. • Fringe representation - To represent the fringe for UCS, the choice is usually a heap-based priority
queue, where the weight for a given enqueued node v is the path cost from the start node to v, or the
backward cost of v. Intuitively, a priority queue constructed in this manner simply reshuffles itself to
maintain the desired ordering by path cost as we remove the current minimum cost path and replace
it with its children. • Completeness - Uniform cost search is complete. If a goal state exists, it must have some finite length
shortest path; hence, UCS must eventually find this shortest length path. • Optimality - UCS is also optimal if we assume all edge costs are nonnegative. By construction, since
we explore nodes in order of increasing path cost, we’re guaranteed to find the lowest-cost path to a
goal state. The strategy employed in Uniform Cost Search is identical to that of Dijkstra’s algorithm,
and the chief difference is that UCS terminates upon finding a solution state instead of finding the
shortest path to all states. Note that having negative edge costs in our graph can make nodes on a
path have decreasing length, ruining our guarantee of optimality.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 6}
Content: (See Bellman-Ford algorithm for a
slower algorithm that handles this possibility)
• Time Complexity - Let us define the optimal path cost as C⇤ and the minimal cost between two nodes
in the state space graph as e. Then, we must roughly explore all nodes at depths ranging from 1 to
C⇤/e, leading to an runtime of O(bC⇤/e ). • Space Complexity - Roughly, the fringe will contain all nodes at the level of the cheapest solution, so
the space complexity of UCS is estimated as O(bC⇤/e ). As a parting note about uninformed search, it’s critical to note that the three strategies outlined above are
fundamentally the same - differing only in expansion strategy, with their similarities being captured by the
tree search pseudocode presented above. Informed Search
Uniform cost search is good because it’s both complete and optimal, but it can be fairly slow because it
expands in every direction from the start state while searching for a goal. If we have some notion of the
direction in which we should focus our search, we can significantly improve performance and "hone in" on
a goal much more quickly. This is exactly the focus of informed search. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 7
b
c≤1
c≤2
C*/e "tiers'
C≤3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: Heuristics
Heuristics are the driving force that allow estimation of distance to goal states - they’re functions that take
in a state as input and output a corresponding estimate. The computation performed by such a function is
specific to the search problem being solved. For reasons that we’ll see in A* search, below, we usually want
heuristic functions to be a lower bound on this remaining distance to the goal, and so heuristics are typically
solutions to relaxed problems (where some of the constraints of the original problem have been removed). Turning to our Pacman example, let’s consider the pathing problem described earlier. A common heuristic
that’s used to solve this problem is the Manhattan distance, which for two points (x1, y1) and (x2, y2) is
defined as follows:
Manhattan(x1, y1, x2, y2) = |x1 x2|+|y1 y2|
The above visualization shows the relaxed problem that the Manhattan distance helps solve - assuming
Pacman desires to get to the bottom left corner of the maze, it computes the distance from Pacman’s current
location to Pacman’s desired location assuming a lack of walls in the maze. This distance is the exact goal
distance in the relaxed search problem, and correspondingly is the estimated goal distance in the actual
search problem.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 7}
Content: With heuristics, it becomes very easy to implement logic in our agent that enables them
to "prefer" expanding states that are estimated to be closer to goal states when deciding which action to
perform. This concept of preference is very powerful, and is utilized by the following two search algorithms
that implement heuristic functions: greedy search and A*. Greedy Search
• Description - Greedy search is a strategy for exploration that always selects the fringe node with the
lowest heuristic value for expansion, which corresponds to the state it believes is nearest to a goal. • Fringe representation - Greedy search operates identically to UCS, with a priority queue fringe representation. The difference is that instead of using computed backward cost (the sum of edge weights
in the path to the state) to assign priority, greedy search uses estimated forward cost in the form of
heuristic values. • Completeness and Optimality - Greedy search is not guaranteed to find a goal state if one exists, nor is
it optimal, particularly in cases where a very bad heuristic function is selected. It generally acts fairly
unpredictably from scenario to scenario, and can range from going straight to a goal state to acting
like a badly-guided DFS and exploring all the wrong areas. A* Search
• Description - A* search is a strategy for exploration that always selects the fringe node with the lowest
estimated total cost for expansion, where total cost is the entire cost from the start node to the goal
node. CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 8
15

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: (a) Greedy search on a good day :) (b) Greedy search on a bad day :(
• Fringe representation - Just like greedy search and UCS, A* search also uses a priority queue to
represent its fringe. Again, the only difference is the method of priority selection. A* combines the
total backward cost (sum of edge weights in the path to the state) used by UCS with the estimated
forward cost (heuristic value) used by greedy search by adding these two values, effectively yielding
an estimated total cost from start to goal. Given that we want to minimize the total cost from start to
goal, this is an excellent choice. • Completeness and Optimality - A* search is both complete and optimal, given an appropriate heuristic
(which we’ll cover in a minute). It’s a combination of the good from all the other search strategies
we’ve covered so far, incorporating the generally high speed of greedy search with the optimality and
completeness of UCS! Admissibility and Consistency
Now that we’ve discussed heuristics and how they are applied in both greedy and A* search, let’s spend
some time discussing what constitutes a good heuristic. To do so, let’s first reformulate the methods used
for determining priority queue ordering in UCS, greedy search, and A* with the following definitions:
• g(n) - The function representing total backwards cost computed by UCS.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 8}
Content: • h(n) - The heuristic value function, or estimated forward cost, used by greedy search. • f(n) - The function representing estimated total cost, used by A* search. f(n) = g(n) +h(n). Before attacking the question of what constitutes a "good" heuristic, we must first answer the question of
whether A* maintains its properties of completeness and optimality regardless of the heuristic function we
use. Indeed, it’s very easy to find heuristics that break these two coveted properties. As an example, consider
the heuristic function h(n) = 1g(n). Regardless of the search problem, using this heuristic yields
f(n) = g(n) +h(n)
= g(n)+(1g(n))
= 1
Hence, such a heuristic reduces A* search to BFS, where all edge costs are equivalent. As we’ve already
shown, BFS is not guaranteed to be optimal in the general case where edge weights are not constant. The condition required for optimality when using A* tree search is known as admissibility. The admissibility constraint states that the value estimated by an admissible heuristic is neither negative nor an overestimate. Defining h⇤(n) as the true optimal forward cost to reach a goal state from a given node n, we can
formulate the admissibility constraint mathematically as follows:
8n, 0  h(n)  h⇤(n)
CS-370, Fall 2023, Note 1 CS 188, Fall 2018, Note 1 9
bb

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: Theorem. For a given search problem, if the admissibility constraint is satisfied by a heuristic function h,
using A* tree search with h on that search problem will yield an optimal solution. Proof. Assume two reachable goal states are located in the search tree for a given search problem, an optimal
goal A and a suboptimal goal B. Some ancestor n of A (including perhaps A itself) must currently be on the
fringe, since A is reachable from the start state. We claim n will be selected for expansion before B, using
the following three statements:
1. g(A) < g(B). Because A is given to be optimal and B is given to be suboptimal, we can conclude that
A has a lower backwards cost to the start state than B. 2. h(A) = h(B) = 0, because we are given that our heuristic satisfies the admissibility constraint. Since
both A and B are both goal states, the true optimal cost to a goal state from A or B is simply h⇤(n) = 0;
hence 0  h(n)  0. 3. f(n)  f(A), because, through admissibility of h, f(n) = g(n) +h(n)  g(n) +h⇤(n) = g(A) = f(A). The total cost through node n is at most the true backward cost of A, which is also the total cost of A. We can combine statements 1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: and 2. to conclude that f(A) < f(B) as follows:
f(A) = g(A) +h(A) = g(A) < g(B) = g(B) +h(B) = f(B)
A simple consequence of combining the above derived inequality with statement 3. is the following:
f(n)  f(A)^ f(A) < f(B) =) f(n) < f(B)
Hence, we can conclude that n is expanded before B. Because we have proven this for arbitrary n, we can
conclude that all ancestors of A (including A itself) expand before B. 2
One problem we found above with tree search was that in some cases it could fail to ever find a solution,
getting stuck searching the same cycle in the state space graph infinitely. Even in situations where our
search technique doesn’t involve such an infinite loop, it’s often the case that we revisit the same node
multiple times because there’s multiple ways to get to that same node. This leads to exponentially more
work, and the natural solution is to simply keep track of which states you’ve already expanded, and never
expand them again. More explicitly, maintain a "closed" set of expanded nodes while utilizing your search
method of choice.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 9}
Content: Then, ensure that each node isn’t already in the set before expansion and add it to the
set after expansion if it’s not. Tree search with this added optimization is known as graph search, and the
pseudocode for it is presented below:
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 10
function GRAPH-SEARCH(problem, fringe)return a solution,or failure
closed←an empty set
fringe←INSERT(MAKE-NODE(INITIAL-STATE[problem]),fringe)
loopdo
iffringeis empty thenreturnfailure
mode
e←REMOVE-FRONT(fringe)
if GOAL-TEsT(problem,STATE[node])thenreturn node
if sTATE[node]is not in closed then
addsTATE[node]toclosed
for child-node in EXPAND(sTATE[node],problem)do
fringe←INsERT(child-node,fringe)
end
end

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: Note that in implementation, it’s critically important to store the closed set as a disjoint set and not a list. Storing it as a list requires costs O(n) operations to check for membership, which eliminates the performance
improvement graph search is intended to provide. An additional caveat of graph search is that it tends to
ruin the optimality of A*, even under admissible heuristics. Consider the following simple state space graph
and corresponding search tree, annotated with weights and heuristic values:
In the above example, it’s clear that the optimal route is to follow S ! A !C !

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 10}
Content: G, yielding a total path cost
of 1+1+3 = 5. The only other path to the goal, S ! B !C ! G has a path cost of 1+2+3 = 6. However,
because the heuristic value of node A is so much larger than the heuristic value of node B, node C is first
expanded along the second, suboptimal path as a child of node B. It’s then placed into the "closed" set, and
so A* graph search fails to reexpand it when it visits it as a child of A, so it never finds the optimal solution. Hence, to maintain completeness and optimality under A* graph search, we need an even stronger property
than admissibility, consistency. The central idea of consistency is that we enforce not only that a heuristic
underestimates the total distance to a goal from any given node, but also the cost/weight of each edge in the
graph. The cost of an edge as measured by the heuristic function is simply the difference in heuristic values
for two connected nodes. Mathematically, the consistency constraint can be expressed as follows:
8A,C h(A)h(C)  cost(A,C)
Theorem. For a given search problem, if the consistency constraint is satisfied by a heuristic function h,
using A* graph search with h on that search problem will yield an optimal solution. Proof. In order to prove the above theorem, we first prove that when running A* graph search with a
consistent heuristic, whenever we remove a node for expansion, we’ve found the optimal path to that node. Using the consistency constraint, we can show that the values of f(n) for nodes along any plan are nondecreasing. Define two nodes, n and n0
, where n0 is a successor of n. Then:
f(n0) = g(n0) +h(n0)
= g(n) +cost(n,n0) +h(n0)
 g(n) +h(n)
= f(n)
If for every parent-child pair (n,n0) along a path, f(n0)  f(n), then it must be the case that the values of
f(n) are nondecreasing along that path. We can check that the above graph violates this rule between f(A)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 11
A
S (0+2)
1
1
S
h=4
c
h=1
A (1+4)
B (1+1)
h=2
1
2
3
C (2+1)
C (3+1)
B
h=1
G (5+0)
G (6+0)
G
h=0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: and f(C). With this information, we can now show that whenever a node n is removed for expansion, its
optimal path has been found. Assume towards a contradiction that this is false - that when n is removed
from the fringe, the path found to n is suboptimal. This means that there must be some ancestor of n, n00, on
the fringe that was never expanded but is on the optimal path to n. Contradiction! We’ve already shown that
values of f along a path are nondecreasing, and so n00 would have been removed for expansion before n. All we have left to show to complete our proof is that an optimal goal A will always be removed for expansion
and returned before any suboptimal goal B. This is trivial, since h(A) = h(B) = 0, so
f(A) = g(A) < g(B) = f(B)
just as in our proof of optimality of A* tree search under the admissibility constraint. Hence, we can
conclude that A* graph search is optimal under a consistent heuristic. 2
A couple of important highlights from the discussion above before we proceed: for heuristics that are either
admissible/consistent to be valid, it must by definition be the case that h(G) = 0 for any goal state G. Additionally, consistency is not just a stronger constraint than admissibility, consistency implies admissibility. This stems simply from the fact that if no edge costs are overestimates (as guaranteed by consistency), the
total estimated cost from any node to a goal will also fail to be an overestimate. Consider the following three-node network for an example of an admissible but inconsistent heuristic:
The red dotted line corresponds to the total estimated goal distance. If h(A) = 4, then the heuristic is
admissible, as the distance from A to the goal is 4  h(A), and same for h(C) = 1  3. However, the
heuristic cost from A to C is h(A)h(C) = 41 = 3. Our heuristic estimates the cost of the edge between
A and C to be 3 while the true value is cost(A,C) = 1, a smaller value. Since h(A)  h(C) ⇥ cost(A,C),
this heuristic is not consistent. Running the same computation for h(A) = 2, however, yields h(A)h(C) =
21 = 1  cost(A,C).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: Thus, using h(A) = 2 makes our heuristic consistent. Dominance
Now that we’ve established the properties of admissibility and consistency and their roles in maintaining
the optimality of A* search, we can return to our original problem of creating "good" heuristics, and how to
tell if one heuristic is better than another. The standard metric for this is that of dominance.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 11}
Content: If heuristic a is
dominant over heuristic b, then the estimated goal distance for a is greater than the estimated goal distance
for b for every node in the state space graph. Mathematically,
8n : ha(n)  hb(n)
CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 12
A
1
h-4
C
h=1
h=2
3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: Dominance very intuitively captures the idea of one heuristic being better than another - if one admissible/consistent heuristic is dominant over another, it must be better because it will always more closely estimate the distance to a goal from any given state. Additionally, the trivial heuristic is defined as h(n) = 0,
and using it reduces A* search to UCS. All admissible heuristics dominate the trivial heuristic. The trivial
heuristic is often incorporated at the base of a semi-lattice for a search problem, a dominance hierarchy of
which it is located at the bottom. Below is an example of a semi-lattice that incorporates various heuristics
ha,hb, and hc ranging from the trivial heuristic at the bottom to the exact goal distance at the top:
As a general rule, the max function applied to multiple admissible heuristics will also always be admissible. This is simply a consequence of all values output by the heuristics for any given state being constrained by
the admissibility condition, 0  h(n)  h⇤(n). The maximum of numbers in this range must also fall in the
same range. The same can be shown easily for multiple consistent heuristics as well.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2 Notes.pdf', 'page': 12}
Content: It’s common practice
to generate multiple admissible/consistent heuristics for any given search problem and compute the max
over the values output by them to generate a heuristic that dominates (and hence is better than) all of them
individually. Summary
In this note, we discussed search problems, which we characterize formally with four components: a state
space, a successor function, a start state, and a goal state. Search problems can be solved using a variety of
search techniques, including but not limited to the five we study in CS 188:
• Breadth-first Search
• Depth-first Search
• Uniform Cost Search
• Greedy Search
• A* Search
The first three search techniques listed above are examples of uninformed search, while the latter two are
examples of informed search which use heuristics to estimate goal distance and optimize performance. CS 188, Fall 2018, Note 1 CS-370, Fall 2023, Note 1 13
ecact
max(ha,hb)
ha
hb
hc
zero

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 2
17th Sept 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 1}
Content: Today
• Agents and Environment (Recap)
• Search Problems
• Uninformed Search Methods
• Depth-First Search
• Breadth-First Search
• Uniform-Cost Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 2}
Content: Search Problems
王

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 3}
Content: Search Problems
• A search problem consists of:
• A state space
• For each state, a set Actions(s) of allowable actions
• A transition model Result(s,a)
• A step cost function c(s,a,s’)
• A start state and a goal test
• A solution is a sequence of actions (a plan) which transforms 
the start state to a goal state
N
E
{N, E}
1
1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 4}
Content: Search Problems Are Models


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 5}
Content: Example: Travelling in Romania
• State space:
• Cities
• Actions:
• Go to adjacent city
• Transition model
• Result(A, Go(B)) = B
• Step cost
• Distance along road link 
• Start state:
• Arad
• Goal test:
• Is state == Bucharest? • Solution? Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 6}
Content: What’s in a State Space? • Problem: Pathing
• State representation: (x,y) location
• Actions: NSEW
• Transition model: update location
• Goal test: is (x,y)=END
• Problem: Eat-All-Dots
• State representation: {(x,y), dot booleans}
• Actions: NSEW
• Transition model: update location and 
possibly a dot boolean
• Goal test: dots all false
The real-world state includes every detail of the environment
A search state abstracts away details not needed to solve the problem
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 7}
Content: State Space Sizes? • World state:
• Agent positions: 120
• Food count: 30
• Ghost positions: 12
• Agent facing: NSEW
• How many
• World states? 120x(230)x(122)x4
• States for pathing? 120
• States for eat-all-dots? 120x(230)
00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 8}
Content: Safe Passage
• Problem: eat all dots while keeping the ghosts scared
• What does the state representation have to specify? • (agent position, dot booleans, power pellet booleans, remaining scared time)
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 9}
Content: State Space Graphs and Search Trees


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 10}
Content: State Space Graphs
• State space graph: A mathematical 
representation of a search problem
• Nodes are (abstracted) world configurations
• Arcs represent transitions resulting from actions
• The goal test is a set of goal nodes (maybe only one)
• In a state space graph, each state occurs only 
once! • We can rarely build this full graph in memory 
(it’s too big), but it’s a useful idea


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 11}
Content: More Examples
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 12}
Content: More Examples
R
L
S S
S S
R
L
R
L
R
L
S
S S
S
L
L
L L R
R
R
R


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 13}
Content: State Space Graphs vs. Search Trees
S G
b
a
Consider this 4-state graph: 
Important: Lots of repeated structure in the search tree! How big is its search tree (from S)? S
a b
b G G a
G a b G
∞


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 14}
Content: Tree Search vs Graph Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 15}
Content: function TREE_SEARCH(problem) returns a solution, or failure
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
for each resulting child from node
add child to the frontier


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 16}
Content: function GRAPH_SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 17}
Content: • What is the relationship between these sets of states after each loop 
iteration in GRAPH_SEARCH? • (Loop invariants!!!)
A
Explored Never Seen
Frontier
B
Explored Never Seen
Frontier
C
Explored Never Seen
Frontier
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 18}
Content: • What is the relationship between these sets of states after each loop iteration in GRAPH_SEARCH? • (Loop invariants!!!)
A
Explored Never Seen
Frontier
B
Explored Never Seen
Frontier
C
Explored Never Seen
Frontier
• The frontier states separate the explored states from never seen states
• Frontier is sub-set of Explored, as loop progresses, number of explored states will be 
more than the number of states infrontier
• Nodes that are not explored (Never Seen) are distinct from the other two 
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 19}
Content: Graph Search
• This graph search algorithm overlays a tree on a graph
• The frontier states separate the explored states from never seen 
states
Images: AIMA, Figure 3.8, 3.9


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 20}
Content: BFS vs DFS


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 21}
Content: • Is the following demo Part 1 using BFS or DFS
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 22}
Content: Video of Demo Maze Water DFS/BFS (part 1)
Search Strategies Demo

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 23}
Content: Video of Demo Maze Water DFS/BFS (part 2)
Search Strategies Demo

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 24}
Content: A Note on Implementation
Nodes have
state, parent, action, path-cost
1
3 2
5 4
6
7
1 8
3 2
5 4
6
7
8
Node
STATE
PARENT
ACTION = Right
PATH-COST = 6
A child of node by action a has
state = result(node.state,a)
parent = node
action = a
path-cost = node.path_cost +
step_cost(node.state, a, self.state)
Extract solution by tracing back parent pointers, collecting actions


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 25}
Content: Walk-through DFS Graph Search
S
G
d
b
p
q
c
e
h
a
f
r
Q,Q,R,E,P
Q,R,E,P
R,E,P
F,E,P
G,E,P
P
Q
R
F
G=Goal
S,D,B,A,C,E,H
S,D,B,A,C,E,H,P,
S,D,B,A,C,E,H,P,Q
S,D,B,A,C,E,H,P,Q,
F
S-D-B-A-C-E-H-P
S-D-B-A-C-E-H-PQ
S,D,B,A,C,E,H,P,Q,
R
S,D,B,A,C,E,H,P,Q,
R,F


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 26}
Content: BFS vs DFS
• When will BFS outperform DFS? • When will DFS outperform BFS? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 27}
Content: Search Algorithm Properties


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 28}
Content: Search Algorithm Properties
• Complete: Guaranteed to find a solution if one exists? • Optimal: Guaranteed to find the least cost path? • Time complexity? • Space complexity? • Cartoon of search tree:
• b is the branching factor
• m is the maximum depth
• solutions at various depths
• Number of nodes in entire tree? • 1 + b + b2 + …. b
m = O(bm)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 29}
Content: Search Algorithm Properties
• Complete: Guaranteed to find a solution if one exists? • Optimal: Guaranteed to find the least cost path? • Time complexity? • Space complexity? • Cartoon of search tree:
• b is the branching factor
• m is the maximum depth
• solutions at various depths
• Number of nodes in entire tree? • 1 + b + b2 + …. b
m = O(bm)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 30}
Content: • Are these the properties for BFS or DFS? • Takes O(b
m) time
• Uses O(bm) space on frontier
• Complete with graph search
• Not optimal unless all goals are in the same level 
(and the same step cost everywhere)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 31}
Content: Depth-First Search (DFS) Properties
…
b
1 node
b nodes
b
2 nodes
b
m nodes
m tiers
• What nodes does DFS expand? • Some left prefix of the tree. • Could process the whole tree! • If m is finite, takes time O(b
m)
• How much space does the frontier take? • Only has siblings on path to root, so O(bm)
• Is it complete? • m could be infinite, so only if we prevent 
cycles (graph search)
• Is it optimal? • No, it finds the “leftmost” solution, 
regardless of depth or cost


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 32}
Content: Breadth-First Search (BFS) Properties
• What nodes does BFS expand? • Processes all nodes above shallowest solution
• Let depth of shallowest solution be s
• Search takes time O(b
s
)
• How much space does the frontier take? • Has roughly the last tier, so O(b
s
)
• Is it complete? • s must be finite if a solution exists, so yes!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 32}
Content: • Is it optimal? • Only if costs are all the same (more on costs 
later)
…
b
1 node
b nodes
b
2 nodes
b
m nodes
s tiers
b
s nodes


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 33}
Content: Iterative Deepening
…
b
• Idea: get DFS’s space advantage with BFS’s 
time / shallow-solution advantages
• Run a DFS with depth limit 1. If no solution…
• Run a DFS with depth limit 2. If no solution…
• Run a DFS with depth limit 3. ….. • Isn’t that wastefully redundant? • Generally most work happens in the lowest level 
searched, so not so bad! 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 34}
Content: Finding a Least-Cost Path
START
GOAL
d
b
p
q
c
e
h
a
f
r
2
9 2
1 8
8
2
3
2
4
4
15
1
3
2
2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 35}
Content: Depth-First (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
S
G
d
b
p
q
c
e
h
a
f
r
q
p
h
d
b
a
c
e
r
Strategy: expand a 
deepest node first
Implementation: 
Frontier is a LIFO stack


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 36}
Content: Breadth-First (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
S
G
d
b
p
q
c
e
h
a
f
r
Search
Tiers
Strategy: expand a 
shallowest node first
Implementation: 
Frontier is a FIFO queue


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 37}
Content: Uniform Cost (Tree) Search
S
a
b
d
p
a
c
e
p
h
f
r
q
q c G
a
e q
p
h
f
r
q
q c G
a
Strategy: expand a cheapest 
node first:
Frontier is a priority queue 
(priority: cumulative cost)
S
G
d
b
p
q
c
e
h
a
f
r
3
9 1
4 16
11
5
13 7
8
11 10
17 11
0
6
3
9
1
1
2
8
8
2
15
1
2
Cost 
contours
2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 38}
Content: Uniform Cost Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 39}
Content: function GRAPH_SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a specific work list (stack, queue, priority queue)
add initial state of problem to frontier
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 40}
Content: function UNIFORM-COST-SEARCH(problem) returns a solution, or failure
initialize the explored set to be empty
initialize the frontier as a priority queue using node path_cost as the priority
add initial state of problem to frontier with path_cost = 0
loop do 
if the frontier is empty then
return failure
choose a node and remove it from the frontier
if the node contains a goal state then
return the corresponding solution
add the node state to the explored set
for each resulting child from node
if the child state is not already in the frontier or explored set then
add child to the frontier
else if the child is already in the frontier with higher path_cost then
replace that frontier node with child
S
A
B
C
D
G
1
4
2
4
1
3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 41}
Content: Walk-through UCS
S
A
B
C
D
G
1
4
2
4
1
3
Priotity Queue Current Node Explored\Close List Path followed
A(1),B(4) S S
B(4), C(3) A(1) S S-A
B(4), D(7) C(3) S,A S-A-C
D(7),D(5) B(4) S,A,C S-A-C-B
D(7), G(8) D(5) S,A,C,B S-A-C-B-D
G(8) S,A,C,B,D
G(8) = goal S,A,C,B,D,G S-A-C-B-D-G
Path = S-B-D-G
Cost = 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 42}
Content: Walk
-through UCS
START
GOAL
d
b
p
q
c
e
h
a
f
r
2
9
2
18
8
2
3
2
4
4
15
1
3
2
2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 43}
Content: …
Uniform Cost Search (UCS) Properties
• What nodes does UCS expand? • Processes all nodes with cost less than cheapest solution! • If that solution costs C* and arcs cost at least  , then the 
“effective depth” is roughly C*/
• Takes time O(b
C*/
) (exponential in effective depth)
• How much space does the frontier take? • Has roughly the last tier, so O(b
C*/
)
• Is it complete? • Assuming best solution has a finite cost and minimum arc cost 
is positive, yes! • Is it optimal?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 43}
Content: • Yes! (Proof next lecture via A*)
b
C*/ “tiers”
c  3
c  2
c  1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 44}
Content: Uniform Cost Issues
• Remember:
 UCS explores increasing cost contours
• The good:
 UCS is complete and optimal! • The bad:
• Explores options in every “direction”
• No information about goal location
• We’ll fix that soon! Start Goal
…
c  3
c  2
c  1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 45}
Content: • Notes added on LMS
• https://www.oreilly.com/library/view/graph-algorithms/9781492047674/ch04.html
Recommended
Reading

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 46}
Content: 
ousoNs? ?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 2.pdf', 'page': 47}
Content: 
We become what we behold. We shape our tools,
and thereafter our tools shape us. -Marshall McLuhan

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 3
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Games
In the first note, we talked about search problems and how to solve them efficiently and optimally - using
powerful generalized search algorithms, our agents could determine the best possible plan and then simply
execute it to arrive at a goal. Now, let’s shift gears and consider scenarios where our agents have one or
more adversaries who attempt to keep them from reaching their goal(s). Our agents can no longer run the
search algorithms we’ve already learned to formulate a plan as we typically don’t deterministically know
how our adversaries will plan against us and respond to our actions. Instead, we’ll need to run a new class
of algorithms that yield solutions to adversarial search problems, more commonly known as games. There are many different types of games. Games can have actions with either deterministic or stochastic
(probabilistic) outcomes, can have any variable number of players, and may or may not be zero-sum. The
first class of games we’ll cover are deterministic zero-sum games, games where actions are deterministic
and our gain is directly equivalent to our opponent’s loss and vice versa. The easiest way to think about
such games is as being defined by a single variable value, which one team or agent tries to maximize and
the opposing team or agent tries to minimize, effectively putting them in direct competition.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 0}
Content: In Pacman, this
variable is your score, which you try to maximize by eating pellets quickly and efficiently while ghosts try
to minimize by eating you first. Many common household games also fall under this class of games:
• Checkers - The first checkers computer player was created in 1950. Since then, checkers has become a
solved game, which means that any position can be evaluated as a win, loss, or draw deterministically
for either side given both players act optimally. • Chess - In 1997, Deep Blue became the first computer agent to defeat human chess champion Gary
Kasparov in a six-game match. Deep Blue was constructed to use extremely sophisticated methods to
evaluate over 200 million positions per second. Current programs are even better, though less historic. • Go - The search space for Go is much larger than for chess, and so most didn’t believe Go computer
agents would ever defeat human world champions for several years to come. However, AlphaGo,
developed by Google, historically defeated Go champion Lee Sodol 4 games to 1 in March 2016. CS 188, Fall 2018, Note 3 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 1}
Content: All of the world champion agents above use, at least to some degree, the adversarial search techniques that
we’re about to cover. As opposed to normal search, which returned a comprehensive plan, adversarial search
returns a strategy, or policy, which simply recommends the best possible move given some configuration of
our agent(s) and their adversaries. We’ll soon see that such algorithms have the beautiful property of giving
rise to behavior through computation - the computation we run is relatively simple in concept and widely
generalizable, yet innately generates cooperation between agents on the same team as well as "outthinking"
of adversarial agents. Minimax
The first zero-sum-game algorithm we will consider is minimax, which runs under the motivating assumption that the opponent we face behaves optimally, and will always perform the move that is worst for us. To
introduce this algorithm, we must first formalize the notion of terminal utilities and state value.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 1}
Content: The value
of a state is the optimal score attainable by the agent which controls that state. In order to get a sense of
what this means, observe the following trivially simple Pacman game board:
Assume that Pacman starts with 10 points and loses 1 point per move until he eats the pellet, at which point
the game arrives at a terminal state and ends. We can start building a game tree for this board as follows,
where children of a state are successor states just as in search trees for normal search problems:
CS 188, Fall 2018, Note 3 2
SOLVED! EXPERT
HUMAN
ABRICK
Checkers
Chess
Go
Pacman8
2
0
2
6
4
6

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 2}
Content: It’s evident from this tree that if Pacman goes straight to the pellet, he ends the game with a score of 8 points,
whereas if he backtracks at any point, he ends up with some lower valued score. Now that we’ve generated
a game tree with several terminal and intermediary states, we’re ready to formalize the meaning of the value
of any of these states. A state’s value is defined as the best possible outcome (utility) an agent can achieve from that state. We’ll
formalize the concept of utility more concretely later, but for now it’s enough to simply think of an agent’s
utility as its score or number of points it attains. The value of a terminal state, called a terminal utility, is
always some deterministic known value and an inherent game property. In our Pacman example, the value
of the rightmost terminal state is simply 8, the score Pacman gets by going straight to the pellet.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 2}
Content: Also, in this
example, the value of a non-terminal state is defined as the maximum of the values of its children. Defining
V(s) as the function defining the value of a state s, we can summarize the above discussion:
8 non-terminal states, V(s) = max s0
2successors(s)
V(s
0
)
8 terminal states, V(s) = known
This sets up a very simple recursive rule, from which it should make sense that the value of the root node’s
direct right child will be 8, and the root node’s direct left child will be 6, since these are the maximum
possible scores the agent can obtain if it moves right or left, respectively, from the start state. It follows that
by running such computation, an agent can determine that it’s optimal to move right, since the right child
has a greater value than the left child of the start state. Let’s now introduce a new game board with an adversarial ghost that wants to keep Pacman from eating the
pellet. The rules of the game dictate that the two agents take turns making moves, leading to a game tree where
the two agents switch off on layers of the tree that they "control". An agent having control over a node
simply means that node corresponds to a state where it is that agent’s turn, and so it’s their opportunity to
decide upon an action and change the game state accordingly. Here’s the game tree that arises from the new
two-agent game board above:
CS 188, Fall 2018, Note 3 3
800
00
00
00
00
00
-20
-8
-18
-5
-10
+4
-20
+8

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 3}
Content: Blue nodes correspond to nodes that Pacman controls and can decide what action to take, while red nodes
correspond to ghost-controlled nodes. Note that all children of ghost-controlled nodes are nodes where the
ghost has moved either left or right from its state in the parent, and vice versa for Pacman-controlled nodes. For simplicity purposes, let’s truncate this game tree to a depth-2 tree, and assign spoofed values to terminal
states as follows:
Naturally, adding ghost-controlled nodes changes the move Pacman believes to be optimal, and the new
optimal move is determined with the minimax algorithm. Instead of maximizing the utility over children
at every level of the tree, the minimax algorithm only maximizes over the children of nodes controlled by
Pacman, while minimizing over the children of nodes controlled by ghosts. Hence, the two ghost nodes
above have values of min(8,5) = 8 and min(10,+8) = 10 respectively. Correspondingly, the root
node controlled by Pacman has a value of max(8,10) = 8. Since Pacman wants to maximize his score,
he’ll go left and take the score of 8 rather than trying to go for the pellet and scoring 10. This is a prime
example of the rise of behavior through computation - though Pacman wants the score of +8 he can get if he
ends up in the rightmost child state, through minimax he "knows" that an optimally-performing ghost will
not allow him to have it. In order to act optimally, Pacman is forced to hedge his bets and counterintuitively
move away from the pellet to minimize the magnitude of his defeat. We can summarize the way minimax
assigns values to states as follows:
8 agent-controlled states, V(s) = max s0
2successors(s)
V(s
0
)
8 opponent-controlled states, V(s) = min s0
2successors(s)
V(s
0
)
8 terminal states, V(s) = known
In implementation, minimax behaves similarly to depth-first search, computing values of nodes in the same
order as DFS would, starting with the the leftmost terminal node and iteratively working its way rightwards. More precisely, it performs a postorder traversal of the game tree. The resulting pseudocode for minimax
is both elegant and intuitively simple, and is presented below:
CS 188, Fall 2018, Note 3 4
00
D0
8
-5
-10
+8

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 4}
Content: Alpha-Beta Pruning
Minimax seems just about perfect - it’s simple, it’s optimal, and it’s intuitive. Yet, its execution is very
similar to depth-first search and it’s time complexity is identical, a dismal O(bm). Recalling that b is the
branching factor and m is the approximate tree depth at which terminal nodes can be found, this yields
far too great a runtime for many games. For example, chess has a branching factor b ⇡ 35 and tree depth
m ⇡ 100. To help mitigate this issue, minimax has an optimization - alpha-beta pruning. Conceptually, alpha-beta pruning is this: if you’re trying to determine the value of a node n by looking at its
successors, stop looking as soon as you know that n’s value can at best equal the optimal value of n’s parent. Let’s unravel what this tricky statement means with an example. Consider the following game tree, with
square nodes corresponding to terminal states, downward-pointing triangles corresponding to minimizing
nodes, and upward-pointing triangles corresponding to maximizer nodes:
Let’s walk through how minimax derived this tree - it began by iterating through the nodes with values 3, 12, and 8, and assigning the value min(3,12,8) = 3 to the leftmost minimizer. Then, it assigned
min(2,4,6) = 2 to the middle minimizer, and min(14,5,2) = 2 to the rightmost minimizer, before finally assigning max(3,2,2) = 3 to the maximizer at the root. However, if we think about this situation, we can come
to the realization that as soon as we visit the child of the middle minimizer with value 2, we no longer need
to look at the middle minimizer’s other children. Why? Since we’ve seen a child of the middle minimizer
with value 2, we know that no matter what values the other children hold, the value of the middle minimizer
can be at most 2. Now that this has been established, let’s think one step further still - the maximizer at
the root is deciding between the value of 3 of the left minimizer, and the value that’s  2, it’s guaranteed to
prefer the 3 returned by the left minimizer over the value returned by the middle minimizer, regardless of
the values of its remaining children. This is precisely why we can prune the search tree, never looking at
the remaining children of the middle minimizer:
CS 188, Fall 2018, Note 3 5
def value(state):
if the state is a terminal state: return the state's utility
if the next agent is MAX: return max-value(state)
if the next agent is MiN: return min-value(state)
def max-value(state):
def min-value(state):
initialize v = -00
initialize v = +oo
for each successor of state:
for each successor of state:
V = max(v, value(successor))
v = min(v, value(successor))
return v
return v3
12
8
2
4
6
14
5
2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 5}
Content: Implementing such pruning can reduce our runtime to as good as O(bm/2), effectively doubling our "solvable" depth. In practice, it’s often a lot less, but generally can make it feasible to search down to at least
one or two more levels. This is still quite significant, as the player who thinks 3 moves ahead is favored to
win over the player who thinks 2 moves ahead. This pruning is exactly what the minimax algorithm with
alpha-beta pruning does, and is implemented as follows:
Take some time to compare this with the pseudocode for vanilla minimax, and note that we can now return
early without searching through every successor. Evaluation Functions
Though alpha-beta pruning can help increase the depth for which we can feasibly run minimax, this still
usually isn’t even close to good enough to get to the bottom of search trees for a large majority of games. As a result, we turn to evaluation functions, functions that take in a state and output an estimate of the
true minimax value of that node. Typically, this is plainly interpreted as "better" states being assigned
higher values by a good evaluation function than "worse" states. Evaluation functions are widely employed
in depth-limited minimax, where we treat non-terminal nodes located at our maximum solvable depth
as terminal nodes, giving them mock terminal utilities as determined by a carefully selected evaluation
function. Because evaluation functions can only yield estimates of the values of non-terminal utilities, this
removes the guarantee of optimal play when running minimax. A lot of thought and experimentation is typically put into the selection of an evaluation function when
designing an agent that runs minimax, and the better the evaluation function is, the closer the agent will come
to behaving optimally. Additionally, going deeper into the tree before using an evaluation function also tends
to give us better results - burying their computation deeper in the game tree mitigates the compromising
of optimality.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 5}
Content: These functions serve a very similar purpose in games as heuristics do in standard search
problems. CS 188, Fall 2018, Note 3 6
3
12
8
2
14
5
2α: MAX's best option on path to root
β: MiN's best option on path to root
def max-value(state, α, β):
def min-value(state, α, β):
initialize v = -00
initialize v = +oo
for each successor of state:
for each successor of state:
v = max(v, value(successor, α, β))
V = min(v, value(successor, α, β))
if v ≥ β return v
if v ≤ α return v
α = max(α, v)
β = min(β, v)
return v
return v

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 6}
Content: The most common design for an evaluation function is a linear combination of features. Eval(s) = w1 f1(s) +w2 f2(s) +...+wn fn(s)
Each fi(s) corresponds to a feature extracted from the input state s, and each feature is assigned a corresponding weight wi. Features are simply some element of a game state that we can extract and assign a
numerical value. For example, in a game of checkers we might construct an evaluation function with 4 features: number of agent pawns, number of agent kings, number of opponent pawns, and number of opponent
kings. We’d then select appropriate weights based loosely on their importance. In our checkers example, it
makes most sense to select positive weights for our agent’s pawns/kings and negative weights for our opponents pawns/kings. Furthermore, we might decide that since kings are more valuable pieces in checkers than
pawns, the features corresponding to our agent’s/opponent’s kings deserve weights with greater magnitude
than the features concerning pawns. Below is a possible evaluation function that conforms to the features
and weights we’ve just brainstormed:
Eval(s) = 2 · agent_kings(s) +agent_pawns(s)2 · opponent_kings(s)opponent_pawns(s)
As you can tell, evaluation function design can be quite free-form, and don’t necessarily have to be linear
functions either. The most important thing to keep in mind is that the evaluation function yields higher
scores for better positions as frequently as possible.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 6}
Content: This may require a lot of fine-tuning and experimenting
on the performance of agents using evaluation functions with a multitude of different features and weights. Expectimax
We’ve now seen how minimax works and how running full minimax allows us to respond optimally against
an optimal opponent. However, minimax has some natural constraints on the situations to which it can respond. Because minimax believes it is responding to an optimal opponent, it’s often overly pessimistic in
situations where optimal responses to an agent’s actions are not guaranteed. Such situations include scenarios with inherent randomness such as card or dice games or unpredictable opponents that move randomly
or suboptimally. We’ll talk about scenarios with inherent randomness much more in detail when we discuss
Markov decision processes in the next note. This randomness can be represented through a generalization of minimax known as expectimax. Expectimax introduces chance nodes into the game tree, which instead of considering the worst case scenario as
minimizer nodes do, considers the average case. More specifically, while minimizers simply compute the
minimum utility over their children, chance nodes compute the expected utility or expected value. Our rule
for determining values of nodes with expectimax is as follows:
8 agent-controlled states, V(s) = max s0
2successors(s)
V(s
0
)
8 chance states, V(s) = Â s0
2successors(s)
p(s
0
|s)V(s
0
)
8 terminal states, V(s) = known
In the above formulation, p(s0|s) refers to either the probability that a given nondeterministic action results
in moving from state s to s0, or the probability that an opponent chooses an action that results in moving
from state s to s0, depending on the specifics of the game and the game tree under consideration. From this
definition, we can see that minimax is simply a special case of expectimax. Minimizer nodes are simply
chance nodes that assign a probability of 1 to their lowest-value child and probability 0 to all other children. CS 188, Fall 2018, Note 3 7


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 7}
Content: In general, probabilities are selected to properly reflect the game state we’re trying to model, but we’ll cover
how this process works in more detail in future notes. For now, it’s fair to assume that these probabilities
are simply inherent game properties. The pseudocode for expectimax is quite similar to minimax, with only a few small tweaks to account for
expected utility instead of minimum utility, since we’re replacing minimizing nodes with chance nodes:
Before we continue, let’s quickly step through a simple example. Consider the following expectimax tree,
where chance nodes are represented by circular nodes instead of the upward/downward facing triangles for
maximizers/minimizers. Assume for simplicity that all children of each chance node have a probability of occurrence of 1
3 . Hence,
from our expectimax rule for value determination, we see that from left to right the 3 chance nodes take on
values of 1
3 · 3+ 13 · 12+ 13 · 9 = 8 , 13 · 2+ 13 · 4+ 13 · 6 = 4 , and 13 · 15+ 13 · 6+ 13 · 0 = 7 . The maximizer
selects the maximimum of these three values, 8 , yielding a filled-out game tree as follows:
CS 188, Fall 2018, Note 3 8
def value(state):
if the state is a terminal state: return the state's utility
if the next agent is MAX: return max-value(state)
if the next agent is EXP: return exp-value(state)
def max-value(state):
def exp-value(state):
initialize v = -00
initialize v = 0
for each successor of state:
for each successor of state:
v = max(v, value(successor))
p = probability(successor)
return v
v += p * value(successor)
return v3
12
9
2
4
6
15

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 8}
Content: As a final note on expectimax, it’s important to realize that, in general, it’s necessary to look at all the
children of chance nodes – we can’t prune in the same way that we could for minimax. Unlike when
computing minimums or maximums in minimax, a single value can skew the expected value computed by
expectimax arbitrarily high or low.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 8}
Content: However, pruning can be possible when we have known, finite bounds
on possible node values. Mixed Layer Types
Though minimax and expectimax call for alternating maximizer/minimizer nodes and maximizer/chance
nodes respectively, many games still don’t follow the exact pattern of alternation that these two algorithms
mandate. Even in Pacman, after Pacman moves, there are usually multiple ghosts that take turns making
moves, not a single ghost. We can account for this by very fluidly adding layers into our game trees as
necessary. In the Pacman example for a game with four ghosts, this can be done by having a maximizer
layer followed by 4 consecutive ghost/minimizer layers before the second Pacman/maximizer layer. In fact,
doing so inherently gives rise to cooperation across all minimizers, as they alternatively take turns further
minimizing the utility attainable by the maximizer(s). It’s even possible to combine chance node layers with
both minimizers and maximizers. If we have a game of Pacman with two ghosts, where one ghost behaves
randomly and the other behaves optimally, we could simulate this with alternating groups of maximizerchance-minimizer nodes. As is evident, there’s quite a bit of room for robust variation in node layering, allowing development of
game trees and adversarial search algorithms that are modified expectimax/minimax hybrids for any zerosum game. CS 188, Fall 2018, Note 3 9
8
4
8
7
3
12
9
2
4
6
15

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 9}
Content: General Games
Not all games are zero-sum. Indeed, different agents may have have distinct tasks in a game that don’t
directly involve strictly competing with one another. Such games can be set up with trees characterized by
multi-agent utilities. Such utilities, rather than being a single value that alternating agents try to minimize or
maximize, are represented as tuples with different values within the tuple corresponding to unique utilities
for different agents. Each agent then attempts to maximize their own utility at each node they control,
ignoring the utilities of other agents. Consider the following tree:
The red, green, and blue nodes correspond to three separate agents, who maximize the red, green, and blue
utilities respectively out of the possible options in their respective layers.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 9}
Content: Working through this example
ultimately yields the utility tuple (5,2,5) at the top of the tree. General games with multi-agent utilities are
a prime example of the rise of behavior through computation, as such setups invoke cooperation since the
utility selected at the root of the tree tends to yield a reasonable utility for all participating agents. Utilities
Thoughout our discussion of games, the concept of utility has come up repeatedly. Utility values are generally hard-wired into games, and agents run some variation of the algorithms discussed in this note to select
an action. We’ll now discuss what’s necessary in order to generate a viable utility function. Rational agents must follow the principle of maximum utility - they must always select the action that
maximizes their expected utility. However, obeying this principle only benefits agents that have rational
preferences. To construct an example of irrational preferences, say there exist 3 objects, A, B, and C, and
our agent is currently in possession of A. Say our agent has the following set of irrational preferences:
• Our agent prefers B to A plus $1
• Our agent prefers C to B plus $1
• Our agent prefers A to C plus $1
CS 188, Fall 2018, Note 3 10
1,6,6
7,1,2
6,1,2
7,2,1
5,1,7
1,5,2
7,7,1
5,2,5

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 10}
Content: A malicious agent in possession of B and C can trade our agent B for A plus a dollar, then C for B plus a
dollar, then A again for C plus a dollar. Our agent has just lost $3 for nothing! In this way, our agent can be
forced to give up all of its money in an endless and nightmarish cycle. Let’s now properly define the mathematical language of preferences:
• If an agent prefers receiving a prize A to receiving a prize B, this is written A  B
• If an agent is indifferent between receiving A or B, this is written as A ⇠ B
• A lottery is a situation with different prizes resulting with different probabilities. To denote lottery
where A is received with probability p and B is received with probability (1 p), we write
L = [p, A; (1 p), B]
In order for a set of preferences to be rational, they must follow the five Axioms of Rationality:
• Orderability: (A  B)_(B  A)_(A ⇠ B)
A rational agent must either prefer one of A or B, or be indifferent between the two. • Transitivity: (A  B)^(B  C) ) (A  C)
If a rational agent prefers A to B and B to C, then it prefers A to C. • Continuity: A  B  C ) 9p [p, A; (1 p), C] ⇠ B
If a rational agent prefers A to B but B to C, then it’s possible to construct a lottery L between A
and C such that the agent is indifferent between L and B with appropriate selection of p. • Substitutability: A ⇠ B ) [p, A; (1 p), C] ⇠ [p, B; (1 p), C]
A rational agent indifferent between two prizes A and B is also indifferent between any two
lotteries which only differ in substitutions of A for B or B for A. • Monotonicity: A  B ) (p  q , [p, A; (1 p), B] ⌫ [q, A; (1q), B]
If a rational agent prefers A over B, then given a choice between lotteries involving only A and B,
the agent prefers the lottery assigning the highest probability to A. If all five axioms are satisfied by an agent, then it’s guaranteed that the agent’s behavior is describable as
a maximization of expected utility. More specifically, this implies that there exists a real-valued utility
function U that when implemented will assign greater utilities to preferred prizes, and also that the utility
of a lottery is the expected value of the utility of the prize resulting from the lottery. These two statements
can be summarized in two concise mathematical equivalences:
U(A)  U(B) , A ⌫ B (1)
U([p1, S1; ... ; pn, Sn]) = Â
i
piU(Si) (2)
If these constraints are met and an appropriate choice of algorithm is made, the agent implementing such
a utility function is guaranteed to behave optimally. Let’s discuss utility functions in greater detail with a
concrete example. Consider the following lottery:
L = [0.5, $0; 0.5, $1000]
CS 188, Fall 2018, Note 3 11


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 11}
Content: This represents a lottery where you receive $1000 with probability 0.5 and $0 with probability 0.5. Now
consider three agents A1, A2, and A3 which have utility functions U1($x) = x, U2($x) = px, and U3($x) = x2
respectively. If each of the three agents were faced with a choice between participting in the lottery and
receiving a flat payment of $500, which would they choose? The respective utilities for each agent of
participating in the lottery and accepting the flat payment are listed in the following table:
Agent Lottery Flat Payment
1 500 500
2 15.81 22.36
3 500000 250000
These utility values for the lotteries were calculated as follows, making use of equation (2) above:
U1(L) = U1([0.5, $0; 0.5, $1000]) = 0.5 ·U1($1000) +0.5 ·U1($0) = 0.5 · 1000+0.5 · 0 = 500
U2(L) = U2([0.5, $0; 0.5, $1000]) = 0.5 ·U2($1000) +0.5 ·U2($0) = 0.5 ·
p
1000+0.5 ·
p
0 = 15.81
U3(L) = U1([0.5, $0; 0.5, $1000]) = 0.5 ·U3($1000) +0.5 ·U3($0) = 0.5 · 10002 +0.5 · 02 = 500000
With these results, we can see that agent A1 is indifferent between participating in the lottery and receiving
the flat payment (the utilities for both cases are identical). Such an agent is known as risk-neutral.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Notes.pdf', 'page': 11}
Content: Similarly,
agent A2 prefers the flat payment to the lottery and is known as risk-averse and agent A3 prefers the lottery
to the flat payment and is known as risk-seeking. Summary
In this note, we shifted gears from considering standard search problems where we simply attempt to find a
path from our starting point to some goal, to considering adversarial search problems where we may have
opponents that attempt to hinder us from reaching our goal. Two primary algorithms were considered:
• Minimax - Used when our opponent(s) behaves optimally, and can be optimized using a-b pruning. Minimax provides more conservative actions than expectimax, and so tends to yield favorable results
when the opponent is unknown as well. • Expectimax - Used when we facing a suboptimal opponent(s), using a probability distribution over
the moves we believe they will make to compute the expectated value of states. In most cases, it’s too computationally expensive to run the above algorithms all the way to the level of
terminal nodes in the game tree under consideration, and so we introduced the notion of evaluation functions
for early termination. Finally, we considered the problem of defining utility functions for agents such that
they make rational decisions. With appropriate function selection, we can additionally make our agents
risk-seeking, risk-averse, or risk-neutral. CS 188, Fall 2018, Note 3 12


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 5
9th Oct 2023
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 1}
Content: AI: Representation and Problem Solving
Adversarial Search
Slide credits: Pat Virtue, http://ai.berkeley.edu


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 2}
Content: Outline
• History / Overview
• Zero-Sum Games (Minimax) 
• Evaluation Functions
• Search Efficiency (α-β Pruning) 
• Games of Chance (Expectimax) 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 3}
Content: Game Playing Stateof-the-Art 
Checkers: 
• 1950: First computer player. • 1959: Samuel’s self-taught program. • 1994: First computer world champion: Chinook ended 40-year-reign of 
human champion Marion Tinsley using complete 8-piece endgame. • 2007: Checkers solved! Endgame database of 39 trillion states 
Chess: 
• 1945-1960: Zuse, Wiener, Shannon, Turing, Newell & Simon, McCarthy. • 1960s onward: gradual improvement under “standard model” 
• 1997: special-purpose chess machine Deep Blue defeats human 
champion Gary Kasparov in a six-game match. Deep Blue examined 
200M positions per second and extended some lines of search up to 40 
ply. Current programs running on a PC rate > 3200 (vs 2870 for Magnus 
Carlsen). Go: 
• 1968: Zobrist’s program plays legal Go, barely (b>300!) 
• 2005-2014: Monte Carlo tree search enables rapid advances: current 
programs beat strong amateurs, and professionals with a 3-4 stone 
handicap. • 2015: AlphaGo from DeepMind beats Lee Sedol 
SOLVED!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 3}
Content: EXPERT
p
HUMAN
ABRICK
Checkers
Chess
Go
Pacman.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 4}
Content: Types of Games
• Deterministic or stochastic? • Perfect information (fully observable)? • One, two, or more players? • Turn-taking or simultaneous? • Zero sum? 2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 5}
Content: Zero-Sum Games
Zero-Sum Games
• Agents have opposite utilities 
• Pure competition: 
• One maximizes, the other minimizes
General Games
• Agents have independent utilities
• Cooperation, indifference, competition, 
shifting alliances, and more are all possible
Q

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 6}
Content: Standard Games
• Standard games are deterministic, observable, two-player, turntaking, zero-sum
• Game formulation:
• Initial state: s0
• Players: Player(s) indicates whose move it is
• Actions: Actions(s) for player on move
• Transition model: Result(s,a)
• Terminal test: Terminal-Test(s)
• Terminal values: Utility(s,p) for player p
• Or just Utility(s) for player making the decision at root


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 7}
Content: Adversarial Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 8}
Content: Single-Agent Trees
8
2 0 … 2 6 … 4 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 9}
Content: Value of a State
8
2 0 … 2 6 … 4 6
Value of a state: 
The best achievable 
outcome (utility) 
from that state
Terminal States:
V(s) = known
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 10}
Content: Idea 1: Many Single-Agent Trees
8
2 0 … 2 6 … 4 6
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)
Choose the best 
action for each 
agent independently


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 11}
Content: Idea 2: Joint State/Action Spaces
Combine the states and actions of the N agents
𝑆0 = (𝑆0
𝐴
, 𝑆0
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 12}
Content: Idea 2: Joint State/Action Spaces
Combine the states and actions of the N agents
𝑆𝐾 = (𝑆𝐾
𝐴
, 𝑆𝐾
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 13}
Content: Idea 2: Joint State/Action Spaces
Search looks through all combinations of all agents’ states and actions
Think of one brain controlling many agents
𝑆𝐾 = (𝑆𝐾
𝐴
, 𝑆𝐾
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 14}
Content: Idea 2: Joint State/Action Spaces
Search looks through all combinations of all agents’ states and actions
Think of one brain controlling many agents
What is the size of 
the state space? What is the size of 
the action space? What is the size of 
the search tree? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 15}
Content: Idea 3: Centralized Decision Making
Each agent proposes their actions and computer confirms the joint plan
Example: Autonomous driving through intersections
https://www.youtube.com/watch?v=4pbAI40dK0A


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 16}
Content: Idea 4: Alternate Searching One Agent at a Time
Agent 1
Agent 2
Agent 1
Search one agent’s actions from a state, search the next agent’s actions 
from those resulting states , etc…
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)
Choose the best 
cascading combination 
of actions


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 17}
Content: Idea 4: Alternate Searching One Agent at a Time
Search one agent’s actions from a state, search the next agent’s actions 
from those resulting states , etc…
What is the size of 
the state space? What is the size of 
the action space? What is the size of 
the search tree? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 18}
Content: Minimax
States
Actions
Values
00000000000000

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 19}
Content: Tic-Tac-Toe Game Tree
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 20}
Content: Tic-Tac-Toe Game Tree
This is a zero-sum game, 
the best action for X is 
the worst action for O 
and vice versa
How do we define 
best and worst? MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 21}
Content: Tic
-Tac-Toe Game Tree Instead of taking the 
max utility at every 
level, alternate max 
and min
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+1111100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 22}
Content: Tic-Tac-Toe Minimax
MAX nodes: under Agent’s control
V(s) = max V(s’)
s’  successors(s)
MIN nodes: under Opponent’s control
V(s) = min V(s’)
s’  successors(s)
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+11111

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 23}
Content: Small Pacman Example
-8 -5 -10 +8
-8 -10
-8
MAX nodes: under Agent’s control
V(s) = max V(s’)
s’  successors(s)
Terminal States:
V(s) = known
MIN nodes: under Opponent’s control
V(s) = min V(s’)
s’  successors(s)
00000000000000

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 24}
Content: Minimax Code
def
max_value(state):
if state.is leaf:
return state.value
#
TODO
Also handle depth
 limit
best value = -10000000
for action in state.actions:
next_state = state.result(action)
next_value = min_value(next_state)
if next_value > best_value:
best_value = next_value
return best_value
def
min value(state):

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 25}
Content: What is the minimax value at the root
1. 2
2. 3
3. 6
4. 12
5. 14
3 12 8 2 4 6 14 5 2
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 26}
Content: 3 12
8246 1452
322
3
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 27}
Content: What kind of search is Minimax Search? A) BFS
B) DFS
C) UCS
D) A*
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 28}
Content: What kind of search is Minimax Search? A) BFS
B) DFS
C) UCS
D) A*
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 29}
Content: Minimax Efficiency
• How efficient is minimax? • Just like (exhaustive) DFS
• Time: O(b
m)
• Space: O(bm)
• Example: For chess, b  35, m  100
• Exact solution is completely infeasible
• Humans can’t do this either, so how do 
we play chess? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 30}
Content: Small Size Robot Soccer
• Joint State/Action space and search for our team
• Adversarial search to predict the opponent team
https://www.youtube.com/watch?v=YihJguq26ek


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 31}
Content: Generalized minimax
• What if the game is not zero-sum, or has multiple players? • Generalization of minimax:
• Terminals have utility tuples
• Node values are also utility tuples
• Each player maximizes its own component
• Can give rise to cooperation and
competition dynamically…
1,1,6 0,0,7 9,9,0 8,8,1 9,9,0 7,7,2 0,0,8 0,0,7
0,0,7 8,8,1 7,7,2 0,0,8
8,8,1 7,7,2
8,8,1
900.00.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 32}
Content: Resource Limits
王

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 33}
Content: Resource Limits
• Problem: In realistic games, cannot search to leaves! • Solution 1: Bounded lookahead
• Search only to a preset depth limit or horizon
• Use an evaluation function for non-terminal positions
• Guarantee of optimal play is gone
• Example:
• Suppose we have 100 seconds, can explore 10K nodes / sec
• So can check 1M nodes per move
• For chess, b=~35 so reaches about depth 4 – not so good
? ?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 33}
Content: ? ? -1 -2 4 9
4
min
max
-2 4


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 34}
Content: Depth Matters
• Evaluation functions are always 
imperfect
• Deeper search => better play 
(usually)
• Or, deeper search gives same 
quality of play with a less accurate 
evaluation function
• An important example of the 
tradeoff between complexity of 
features and complexity of 
computation
N

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 35}
Content: Evaluation Functions
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 36}
Content: Evaluation Functions
• Evaluation functions score non-terminals in depth-limited search
• Ideal function: returns the actual minimax value of the position
• In practice: typically weighted linear sum of features:
• EVAL(s) = w1 f1(s) + w2f2(s) + …. + wnfn(s)
• E.g., w1 = 9, f1(s) = (num white queens – num black queens), etc.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 36}
Content: • Terminate search only in quiescent positions, i.e., no major 
changes expected in feature values
1
业业业
1
Y
1
中品
W
为美
Blacktomove
Whiteslightlybetter1
1
1
1
1
品业
M
Whitetomove
Blackwinning

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 37}
Content: Evaluation for Pacman
00.00.00.00.0000.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 38}
Content: Solution 2: Game Tree Pruning


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 39}
Content: Intuition: prune the branches that can’t be chosen
3 12 8 2 4 6 14 5 2
3 2 2
3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 40}
Content: Alpha-Beta Pruning Example
3 12 8 2 14 5 2
α =3 α =3
α = best option so far from any 
MAX node on this path
The order of generation matters: more pruning
is possible if good moves come first
3
3
We can prune when: min node won’t be 
higher than 2, while parent max has seen 
something larger in another branch


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 41}
Content: Alpha-Beta Implementation
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v
α: MAX’s best option on path to root
β: MIN’s best option on path to root


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 42}
Content: What is the value of the blue triangle? A) 10
B) 8
C) 4
D) 50
a
d
e
f
10
8
4
500
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 43}
Content: What is the value of the blue triangle? A) 10
B) 8
C) 4
D) 50
84
8
0
Pa
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 44}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 45}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = ∞
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 46}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = ∞
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 47}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = 10
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 48}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 10
𝑣 = 10
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 49}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 10
𝑣 = 8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 50}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 51}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 52}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 53}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 54}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = ∞
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 55}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 56}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 57}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 58}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 59}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 60}
Content: What is the value of the top node? A) 10
B) 100
C) 2
D) 4
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 61}
Content: Which branches are pruned? A) e, l
B) g, l
C) g, k, l
D) g, n
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 62}
Content: 1
? 10
? ? 10
10 100
? ? 2
2
? β =
α =
α= α= v=
β =
Which branches are pruned? A) e, l
B) g, l
C) g, k, l
D) g, n
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 1.pdf', 'page': 63}
Content: Alpha-Beta Pruning Properties
• Theorem: This pruning has no effect on minimax value computed for the root! • Good child ordering improves effectiveness of pruning
• Iterative deepening helps with this
• With “perfect ordering”:
• Time complexity drops to O(b
m/2)
• Doubles solvable depth! • 1M nodes/move => depth=8, respectable
• This is a simple example of metareasoning (computing about what to compute)
10 10 0
max
min


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 5
9th Oct 2023
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 1}
Content: AI: Representation and Problem Solving
Adversarial Search
Slide credits: Pat Virtue, http://ai.berkeley.edu


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 2}
Content: Outline
• History / Overview
• Zero-Sum Games (Minimax) 
• Evaluation Functions
• Search Efficiency (α-β Pruning) 
• Games of Chance (Expectimax) 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 3}
Content: Game Playing Stateof-the-Art 
Checkers: 
• 1950: First computer player. • 1959: Samuel’s self-taught program. • 1994: First computer world champion: Chinook ended 40-year-reign of 
human champion Marion Tinsley using complete 8-piece endgame. • 2007: Checkers solved! Endgame database of 39 trillion states 
Chess: 
• 1945-1960: Zuse, Wiener, Shannon, Turing, Newell & Simon, McCarthy. • 1960s onward: gradual improvement under “standard model” 
• 1997: special-purpose chess machine Deep Blue defeats human 
champion Gary Kasparov in a six-game match. Deep Blue examined 
200M positions per second and extended some lines of search up to 40 
ply. Current programs running on a PC rate > 3200 (vs 2870 for Magnus 
Carlsen). Go: 
• 1968: Zobrist’s program plays legal Go, barely (b>300!) 
• 2005-2014: Monte Carlo tree search enables rapid advances: current 
programs beat strong amateurs, and professionals with a 3-4 stone 
handicap. • 2015: AlphaGo from DeepMind beats Lee Sedol 
SOLVED!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 3}
Content: EXPERT
p
HUMAN
ABRICK
Checkers
Chess
Go
Pacman.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 4}
Content: Types of Games
• Deterministic or stochastic? • Perfect information (fully observable)? • One, two, or more players? • Turn-taking or simultaneous? • Zero sum? 2

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 5}
Content: Zero-Sum Games
Zero-Sum Games
• Agents have opposite utilities 
• Pure competition: 
• One maximizes, the other minimizes
General Games
• Agents have independent utilities
• Cooperation, indifference, competition, 
shifting alliances, and more are all possible
Q

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 6}
Content: Standard Games
• Standard games are deterministic, observable, two-player, turntaking, zero-sum
• Game formulation:
• Initial state: s0
• Players: Player(s) indicates whose move it is
• Actions: Actions(s) for player on move
• Transition model: Result(s,a)
• Terminal test: Terminal-Test(s)
• Terminal values: Utility(s,p) for player p
• Or just Utility(s) for player making the decision at root


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 7}
Content: Adversarial Search


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 8}
Content: Single-Agent Trees
8
2 0 … 2 6 … 4 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 9}
Content: Value of a State
8
2 0 … 2 6 … 4 6
Value of a state: 
The best achievable 
outcome (utility) 
from that state
Terminal States:
V(s) = known
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 10}
Content: Idea 1: Many Single-Agent Trees
8
2 0 … 2 6 … 4 6
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)
Choose the best 
action for each 
agent independently


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 11}
Content: Idea 2: Joint State/Action Spaces
Combine the states and actions of the N agents
𝑆0 = (𝑆0
𝐴
, 𝑆0
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 12}
Content: Idea 2: Joint State/Action Spaces
Combine the states and actions of the N agents
𝑆𝐾 = (𝑆𝐾
𝐴
, 𝑆𝐾
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 13}
Content: Idea 2: Joint State/Action Spaces
Search looks through all combinations of all agents’ states and actions
Think of one brain controlling many agents
𝑆𝐾 = (𝑆𝐾
𝐴
, 𝑆𝐾
𝐵
)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 14}
Content: Idea 2: Joint State/Action Spaces
Search looks through all combinations of all agents’ states and actions
Think of one brain controlling many agents
What is the size of 
the state space? What is the size of 
the action space? What is the size of 
the search tree? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 15}
Content: Idea 3: Centralized Decision Making
Each agent proposes their actions and computer confirms the joint plan
Example: Autonomous driving through intersections
https://www.youtube.com/watch?v=4pbAI40dK0A


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 16}
Content: Idea 4: Alternate Searching One Agent at a Time
Agent 1
Agent 2
Agent 1
Search one agent’s actions from a state, search the next agent’s actions 
from those resulting states , etc…
Non-Terminal States:
V(s) = max V(s’)
s’  successors(s)
Choose the best 
cascading combination 
of actions


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 17}
Content: Idea 4: Alternate Searching One Agent at a Time
Search one agent’s actions from a state, search the next agent’s actions 
from those resulting states , etc…
What is the size of 
the state space? What is the size of 
the action space? What is the size of 
the search tree? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 18}
Content: Minimax
States
Actions
Values
00000000000000

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 19}
Content: Tic-Tac-Toe Game Tree
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 20}
Content: Tic-Tac-Toe Game Tree
This is a zero-sum game, 
the best action for X is 
the worst action for O 
and vice versa
How do we define 
best and worst? MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 21}
Content: Tic
-Tac-Toe Game Tree Instead of taking the 
max utility at every 
level, alternate max 
and min
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+1111100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 22}
Content: Tic-Tac-Toe Minimax
MAX nodes: under Agent’s control
V(s) = max V(s’)
s’  successors(s)
MIN nodes: under Opponent’s control
V(s) = min V(s’)
s’  successors(s)
MAX (X)
X
X
MIN (O)
X
X
X
X
o
X
MAX (X)
o
xox
X
MIN (O)
X
X
o
X
X
o
X
X
o
X
TERMINAL
o
X
o
o
X
X
o
X
X
O
X
0
Utility
-1
0
+11111

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 23}
Content: Small Pacman Example
-8 -5 -10 +8
-8 -10
-8
MAX nodes: under Agent’s control
V(s) = max V(s’)
s’  successors(s)
Terminal States:
V(s) = known
MIN nodes: under Opponent’s control
V(s) = min V(s’)
s’  successors(s)
00000000000000

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 24}
Content: Minimax Code
def
max_value(state):
if state.is leaf:
return state.value
#
TODO
Also handle depth
 limit
best value = -10000000
for action in state.actions:
next_state = state.result(action)
next_value = min_value(next_state)
if next_value > best_value:
best_value = next_value
return best_value
def
min value(state):

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 25}
Content: What is the minimax value at the root
1. 2
2. 3
3. 6
4. 12
5. 14
3 12 8 2 4 6 14 5 2
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 26}
Content: 3 12
8246 1452
322
3
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 27}
Content: What kind of search is Minimax Search? A) BFS
B) DFS
C) UCS
D) A*
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 28}
Content: What kind of search is Minimax Search? A) BFS
B) DFS
C) UCS
D) A*
0
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 29}
Content: Minimax Efficiency
• How efficient is minimax? • Just like (exhaustive) DFS
• Time: O(b
m)
• Space: O(bm)
• Example: For chess, b  35, m  100
• Exact solution is completely infeasible
• Humans can’t do this either, so how do 
we play chess? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 30}
Content: Small Size Robot Soccer
• Joint State/Action space and search for our team
• Adversarial search to predict the opponent team
https://www.youtube.com/watch?v=YihJguq26ek


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 31}
Content: Generalized minimax
• What if the game is not zero-sum, or has multiple players? • Generalization of minimax:
• Terminals have utility tuples
• Node values are also utility tuples
• Each player maximizes its own component
• Can give rise to cooperation and
competition dynamically…
1,1,6 0,0,7 9,9,0 8,8,1 9,9,0 7,7,2 0,0,8 0,0,7
0,0,7 8,8,1 7,7,2 0,0,8
8,8,1 7,7,2
8,8,1
900.00.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 32}
Content: Resource Limits
王

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 33}
Content: Resource Limits
• Problem: In realistic games, cannot search to leaves! • Solution 1: Bounded lookahead
• Search only to a preset depth limit or horizon
• Use an evaluation function for non-terminal positions
• Guarantee of optimal play is gone
• More plies make a BIG difference
• Example:
• Suppose we have 100 seconds, can explore 10K nodes / sec
• So can check 1M nodes per move
• For chess, b=~35 so reaches about depth 4 – not so good
? ?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 33}
Content: ? ? -1 -2 4 9
4
min
max
-2 4


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 34}
Content: Depth Matters
• Evaluation functions are always 
imperfect
• Deeper search => better play 
(usually)
• Or, deeper search gives same 
quality of play with a less accurate 
evaluation function
• An important example of the 
tradeoff between complexity of 
features and complexity of 
computation
N

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 35}
Content: Evaluation Functions
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 36}
Content: Evaluation Functions
• Evaluation functions score non-terminals in depth-limited search
• Ideal function: returns the actual minimax value of the position
• In practice: typically weighted linear sum of features:
• EVAL(s) = w1 f1(s) + w2f2(s) + …. + wnfn(s)
• E.g., w1 = 9, f1(s) = (num white queens – num black queens), etc.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 36}
Content: • Terminate search only in quiescent positions, i.e., no major 
changes expected in feature values
1
业业业
1
Y
1
中品
W
为美
Blacktomove
Whiteslightlybetter1
1
1
1
1
品业
M
Whitetomove
Blackwinning

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 37}
Content: Evaluation for Pacman
00.00.00.00.0000.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 38}
Content: Solution 2: Game Tree Pruning


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 39}
Content: Intuition: prune the branches that can’t be chosen
3 12 8 2 4 6 14 5 2
3 2 2
3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 40}
Content: Alpha-Beta Pruning Example
3 12 8 2 14 5 2
α =3 α =3
α = best option so far from any 
MAX node on this path
The order of generation matters: more pruning
is possible if good moves come first
3
3
We can prune when: min node won’t be 
higher than 2, while parent max has seen 
something larger in another branch


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 41}
Content: Alpha-Beta Implementation
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v
α: MAX’s best option on path to root
β: MIN’s best option on path to root


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 42}
Content: What is the value of the blue triangle? A) 10
B) 8
C) 4
D) 50
a
d
e
f
10
8
4
500
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 43}
Content: What is the value of the blue triangle? A) 10
B) 8
C) 4
D) 50
84
8
0
Pa
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 44}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 45}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = ∞
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 46}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = ∞
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 47}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = ∞
𝑣 = 10
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 48}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 10
𝑣 = 10
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 49}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 10
𝑣 = 8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 50}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 51}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = −∞
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 52}
Content: Alpha-Beta Small Example
𝛼 = −∞
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 53}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 54}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = ∞
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 55}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 56}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 57}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 58}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 59}
Content: Alpha-Beta Small Example
𝛼 = 8
𝛽 = ∞
𝑣 = 8
def min-value(state , α, β):
initialize v = +∞
for each successor of state:
v = min(v, value(successor, α, β))
if v ≤ α
return v
β = min(β, v)
return v
def max-value(state, α, β):
initialize v = -∞
for each successor of state:
v = max(v, value(successor, α, β))
if v ≥ β
return v
α = max(α, v)
return v 𝛼 = −∞
𝛽 = 8
𝑣 = 8
𝛼 = 8
𝛽 = ∞
𝑣 = 4
8 4
8
a
d
e
f
10
8
4
50

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 60}
Content: What is the value of the top node? A) 10
B) 100
C) 2
D) 4
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 61}
Content: Which branches are pruned? A) e, l
B) g, l
C) g, k, l
D) g, n
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 62}
Content: 1
? 10
? ? 10
10 100
? ? 2
2
? β =
α =
α= α= v=
β =
Which branches are pruned? A) e, l
B) g, l
C) g, k, l
D) g, n
a
g
k
m
10
6
100
8
1
2
20
40
P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 63}
Content: Alpha-Beta Pruning Properties
• Theorem: This pruning has no effect on minimax value computed for the root! • Good child ordering improves effectiveness of pruning
• Iterative deepening helps with this
• With “perfect ordering”:
• Time complexity drops to O(b
m/2)
• Doubles solvable depth! • 1M nodes/move => depth=8, respectable
• This is a simple example of metareasoning (computing about what to compute)
10 10 0
max
min


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 64}
Content: Games with uncertain outcomes
×1
×5
2
+
10
x1
+
×5
X2
+

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 65}
Content: 
O

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 66}
Content: 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 67}
Content: Modeling Assumptions
Dangerous Optimism
Assuming chance when the world is adversarial
Dangerous Pessimism
Assuming the worst case when it’s not likely
QQ

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 68}
Content: Chance outcomes in trees
10 10 9 100 10 10 9 100
10 9 10 9 10 100
Tictactoe, chess
Minimax
Tetris, investing
Expectimax
Backgammon, Monopoly
Expectiminimax
00X1
X5
2
+
十
5
2x

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 69}
Content: Minimax
function value(s) returns a value
if Terminal-Test(s) then return Utility(s)
if Player(s) = MAX then return maxa in Actions(s) value(Result(s,a))
if Player(s) = MIN then return mina in Actions(s) value(Result(s,a))
function decision(s) returns an action
return the action a in Actions(s) with the highest 
value(Result(s,a))


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 70}
Content: Expectiminimax
function value(s) returns a value
if Terminal-Test(s) then return Utility(s)
if Player(s) = MAX then return maxa in Actions(s) value(Result(s,a))
if Player(s) = MIN then return mina in Actions(s) value(Result(s,a))
if Player(s) = CHANCE then return suma in Actions(s) Pr(a) * value(Result(s,a))
function decision(s) returns an action
return the action a in Actions(s) with the highest 
value(Result(s,a))


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 71}
Content: Probabilities


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 72}
Content: Probabilities
A random variable represents an event whose outcome
is unknown
A probability distribution is an assignment of weights
to outcomes
Example: Traffic on freeway
▪ Random variable: T = whether there’s traffic
▪ Outcomes: T in {none, light, heavy}
▪ Distribution:
P(T=none) = 0.25, P(T=light) = 0.50, P(T=heavy) = 0.25
Probabilities over all possible outcomes sum to one
0.25
0.50
0.25


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 73}
Content: • The expected value of a random variable is the average, 
weighted by the probability distribution over outcomes
• Example: How long to get to the airport? Expected Value
Probability: 0.25 0.50 0.25
Time: 20 min 30 min 60 min
x + x + x 35 min


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 74}
Content: Expectations
Probability: 0.25 0.50 0.25
Time: 20 min 30 min 60 min
x + x + x
20 30 60
0.25
0.5
0.25
𝑉 𝑠 = max
𝑎
𝑉 𝑠′,
where 𝑠′ 𝑢 =𝑠 𝑟𝑒 𝑙𝑡(𝑠, 𝑎)
Max node notation Chance node notation
𝑉 𝑠 = ෍
𝑠′
𝑃 𝑠′ 𝑉(𝑠′)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 75}
Content: Expectimax Pseudocode
suma in Action(s) Pr(a) * value(Result(s,a))
58 247 -12
1/2
1/3
1/6
v = (1/2) (8) + (1/3) (24) + (1/6) (-12) = 10


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 76}
Content: Expectimax Example
3 12 9 2 4 6 15 6 0


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 77}
Content: What Values to Use? • For worst-case minimax reasoning, evaluation function scale doesn’t matter
• We just want better states to have higher evaluations (get the ordering right)
• Minimax decisions are invariant with respect to monotonic transformations on values
• Expectiminimax decisions are invariant with respect to positive affine transformations
• Expectiminimax evaluation functions have to be aligned with actual win probabilities! 0 40 20 30 x
2
0 1600 400 900
x>y => f(x)>f(y) f(x) = Ax+B where A>0 


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 5 Part 2.pdf', 'page': 78}
Content: Summary
• Multi-agent problems can require more space or deeper trees to search
• Games require decisions when optimality is impossible
• Bounded-depth search and approximate evaluation functions
• Games force efficient use of computation
• Alpha-beta pruning
• Game playing has produced important research ideas
• Reinforcement learning (checkers)
• Iterative deepening (chess)
• Rational metareasoning (Othello)
• Monte Carlo tree search (Go)
• Solution methods for partial-information games in economics (poker)
• Video games present much greater challenges – lots to do! • b = 10500, |S| = 104000, m = 10,000


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 6
These lecture notes are heavily based on notes originally written by Josh Hug and Jacky Liang. Probabilistic Inference
In artificial intelligence, we often want to model the relationships between various nondeterministic events. If the weather predicts a 40% chance of rain, should I carry my umbrella?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 0}
Content: How many scoops of ice cream
should I get if the more scoops I get, the more likely I am to drop it all? If there was an accident 15 minutes
ago on the freeway on my route to Oracle Arena to watch the Warriors’ game, should I leave now or in 30
minutes? All of these questions (and innumerable more) can be answered with probabilistic inference. We’re assuming that you’ve learned the foundations of probability in CS70, so these notes will not review
basic concepts of probability like PDFs, conditional probabilities, independence, and conditional independence. In previous sections of this class, we modeled the world as existing in a specific state that is always known. For the next several weeks, we will instead use a new model where each possible state for the world has
its own probability. For example, we might build a weather model, where the state consists of the season,
temperature and weather. Our model might say that P(winter, 35, cloudy) = 0.023. This number represents
the probability of the specific outcome that it is winter, 35, and cloudy. More precisely, our model is a joint distribution, i.e. a table of probabilities which captures the likelihood
of each possible outcome, also known as an assignment. As an example, consider the table below:
Season Temperature Weather Probability
summer hot sun 0.30
summer hot rain 0.05
summer cold sun 0.10
summer cold rain 0.05
winter hot sun 0.10
winter hot rain 0.05
winter cold sun 0.15
winter cold rain 0.20
This model allows us to answer questions that might be of interest to us, for example:
• What is the probability that it is sunny? P(W = sun)
• What is the probability distribution for the weather, given that we know it is winter? P(W | S = winter)
• What is the probability that it is winter, given that we know it is rainy and cold? P(S = winter | T =
cold,W = rain)
• What is the probability distribution for the weather and season give that we know that it is cold? P(S,W | T = cold)
CS 188, Fall 2018, Note 6 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: Given a joint PDF, we can trivially perform compute any desired probablity distribution P(Q1 ...Qk | e1 ...ek)
using a simple and intuitive procedure known as inference by enumeration, for which we define three types
of variables we will be dealing with:
1. Query variables Qi, which are unknown and appear on the left side of the probability distribution we
are trying to compute.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 1}
Content: 2. Evidence variables ei, which are observed variables whose values are known and appear on the right
side of the probability distribution we are trying to compute. 3. Hidden variables, which are values present in the overall joint distribution but not in the distribution
we are currently trying to compute. In this procedure, we collect all the rows consistent with the observed evidence variables, sum out all the
hidden variables, and finally normalize the table so that it is a probability distribution (i.e. values sum to 1). For example, if we wanted to compute P(W | S = winter), we’d select the four rows where S is winter, then
sum out over T and normalize. This yields the following probability table:
W S Unnormalized Sum Probability
sun winter 0.10+0.15 = 0.25 0.25/(0.25+0.25) = 0.5
rain winter 0.05+0.20 = 0.25 0.25/(0.25+0.25) = 0.5
Hence P(W = sun | S = winter) = 0.5 and P(W = rain | S = winter) = 0.5, and we learn that in winter
there’s a 50% chance of sun and a 50% chance of rain (classic California weather). So long as we have the joint PDF table, inference by enumeration (IBE) can be used to compute any desired
probablity distribution, even for multiple query variables Q1...Qk. Bayes Nets (Representation)
While inference by enumeration can compute probabilities for any query we might desire, representing an
entire joint distribution in the memory of a computer is impractical for real problems - if each of n variables
we wish to represent can take on d possible values (it has a domain of size d), then our joint distribution
table will have dn entries, exponential in the number of variables and quite impractical to store! Bayes nets avoid this issue by taking advantage of the idea of conditional probability. Rather than storing
information in a giant table, probabilities are instead distributed across a large number of smaller local
probability tables along with a directed acyclic graph (DAG) which captures the relationships between
variables. The local probability tables and the DAG together encode enough information to compute any
probability distribution that we could have otherwise computed given the entire joint distribution. Specifically, each node in the graph represents a single random variable and each directed edge represents
one of the conditional probability distributions we choose to store (i.e. an edge from node A to node B
indicates that we store the probability table for P(B|A)). Each node is conditionally independent of all its
ancestor nodes in the graph, given all of its parents. Thus, if we have a node representing variable X, we
store P(X|A1,A2,...,AN), where A1,...,AN are the parents of X. As an example of a Bayes Net, consider a model where we have five binary random variables described
below:
CS 188, Fall 2018, Note 6 2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 2}
Content: • B: Burglary occurs. • A: Alarm goes off. • E: Earthquake occurs. • J: John calls. • M: Mary calls. Assume the alarm can go off if either a burglary or an earthquake occurs, and that Mary and John will call
if they hear the alarm. We can represent these dependencies with the graph shown below. As a reality check, it’s important to internalize that Bayes Nets are only a type of model. Models attempt
to capture the way the world works, but because they are always a simplification they are always wrong. However, with good modeling choices they can still be good enough approximations that they are useful for
solving real problems in the real world.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 2}
Content: In general, they will not account for every variable or even every
interaction between variables. Returning to our discussion, we formally define a Bayes Net as consisting of:
• A directed acyclic graph of nodes, one per variable X. • A conditional distribution for each node P(X|A1 ...An), where Ai is the i
th parent of X, stored as a
conditional probability table or CPT. Each CPT has n+2 columns: one for the values of each of the
n parent variables A1 ...An, one for the values of X, and one for the conditional probability of X. In the alarm model above, we would store probability tables P(B),P(E),P(A | B,E),P(J | A) and P(M | A). Given all of the CPTs for a graph, we can calculate the probability of a given assignment using the chain
rule: P(X1,X2,...,Xn) = ’n
i=1 P(Xi|parents(Xi)). For the alarm model above, we might calculate the probability of one event as follows: P(b,e,+a,+j,m) =
P(b)·P(e)·P(+a|b,e)·P(+j|+a)·P(m|+a). CS 188, Fall 2018, Note 6 3
B
E
A
T
M

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: This works because of the conditional independence relationships given by the graph. Specifically, we rely
on the fact that P(xi|x1,..., xi1) = P(xi|parents(Xi)). Or in other words, that the probability of a specific
value of Xi depends only on the values assigned to Xi’s parents. Bayes Nets (Inference)
Inference is the process of calculating the joint PDF for some set of query variables based on some set
of observed variables. We can solve this problem naively by forming the joint PDF and using inference by
enumeration as described above. This requires the creation of and iteration over an exponentially large table. An alternate approach is to eliminate variables one by one. To eliminate a variable X, we:
1. Join (multiply together) all factors involving X. 2. Sum out X. A factor is defined simply as an unnormalized probability. At all points during variable elimination, each
factor will be proportional to the probability it corresponds to but the underlying distribution for each factor
won’t necessarily sum to 1 as a probability distribution should. Let’s make these ideas more concrete with an example.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: Suppose we have a model as shown below, where
T, C, S, and E can take on binary values, as shown below. Here, T represents the chance that an adventurer
takes a treasure, C represents the chance that a cage falls on the adventurer given that he takes the treasure,
S represents the chance that snakes are released if an adventurer takes the treasure, and E represents the
chance that the adventurer escapes given information about the status of the cage and snakes. In this case, we have the factors P(T), P(C|T), P(S|T), and P(E|C,S).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 3}
Content: Suppose we want to calculate
P(T| + e). The inference by enumeration approach would be to form the 16 row joint PDF P(T,C,S,E),
select only the rows corresponding to +e, then summing out C and S and finally normalizing. The alternate approach is to eliminate C, then S, one variable at a time. We’d proceed as follows:
• Join (multiply) all the factors involving C, forming P(C,+e|T,S) = P(C|T)·P(+e|C,S). • Sum out C from this new factor, leaving us with a new factor P(+e|T,S). • Join all factors involving S, forming P(+e,S|T) = P(S|T)·P(+e|T,S). CS 188, Fall 2018, Note 6 4
T
C
S
E

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 4}
Content: • Sum out S, yielding P(+e|T). Once we have P(+e|T), we can easily compute P(T|+e). While this process is more involved from a conceptual point of view, the maximum size of any factor
generated is only 8 rows instead of 16 as it would be if we formed the entire joint PDF. An alternate way of looking at the problem is to observe that the calculation of P(+e,T) can either be done,
as it is in inference by enumeration, as follows:
ÂsÂc
P(T)P(s|T)P(c|T)P(+e|c,s)
Variable elimination is equivalent to calculating P(+e,T) as follows:
P(T)ÂsP(s|T)ÂcP(c|T)P(+e|c,s)
Bayes Nets (Sampling)
An alternate approach for probabilistic reasoning is to implicitly calculate the probabilities for our query by
simply counting samples. For example, suppose we wanted to calculate P(T| + e). If we had a magic machine that could generate
samples from our distribution, we could collect all samples for which the adventurer escapes the maze, and
then compute the fraction of those escapes for which the adventurer also took the treasure. Put differently,
if we could run simulations of say, a few million adventurers, we’d easily be able to compute any inference
we’d want just by looking at the samples. Given a Bayes Net model, we can easily write a simulator. For example, consider the CPTs given below for
the simplified model with only two variables T and C.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 4}
Content: CS 188, Fall 2018, Note 6 5
T
P(T)
+t
0.99
T
-t
0.01
T
C
P(C|T)
+t
+C
0.95
+t
-C
0.05
-t
+C
0.0
-t
-C
1.0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: A simple simulator in Python would be as follows:
import random
def get_t():
if random.random() < 0.99:
return True
return False
def get_c(t):
if t and random.random() < 0.95:
return True
return False
def get_sample():
t = get_t()
c = get_c(t)
return [t, c]
We call this simple approach prior sampling. The downside of this approach is that it may require the
generation of a very large number of samples in order to perform analysis of unlikely scenarios.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: If we
wanted to compute P(C|t), we’d have to throw away 99% of our samples. One way to mitigate this this problem, we can modify our procedure to early reject any sample inconsistent
with our evidence. For example, for the query P(C|t), we’d avoid generating a value for C unless t is true. This still means we have to throw away most of our samples, but at least the bad samples we generate take
less time to create. We call this approach rejection sampling. These two approaches work for the same reason, which is that any valid sample occurs with the same
probability as specified in the joint PDF. In other words, the probability of every sample is based on the
product of every CPT, or as I personally call it, the "every CPT participates principle". A more exotic approach is likelihood weighting, which ensures that we never generate a bad sample. In
this approach, we manually set all variables equal to the evidence in our query. For example, if we wanted
to compute P(C|t), we’d simply declare that t is false. The problem here is that this may yield samples
that are inconsistent with the correct distribution. As an example, consider the more complex four variable
model for T, C, S, and E given earlier in these notes. If we wanted to compute P(T,S,+c,+e), and simply
picked values for T and S without taking into account the fact that c = false, and e = true, then there’s no
guarantee that our samples actually obey the joint PDF given by the Bayes Net. For example, if the cage
only ever falls if the treasure is taken, then we’d want to ensure that T is always true instead of using the
P(T) distribution given in the Bayes Net. Put differently, if we simply force some variables equal to the evidence, then our samples occur with probability given only equal to the products of the CPTs of the non-evidence variables. This means the joint PDF
has no guarantee of being correct (though may be for some cases like our two variable Bayes Net). Instead,
if we have sampled variables Z1 through Zp and fixed evidence variables E1 through Em a sample is given
by the probability P(Z1...Zp,E1...Em) = ’p
i P(Zi)|Parents(Zi).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 5}
Content: What is missing is that the probability of a
sample does not include all the probabilities of P(Ei|Parents(Ei)), i.e. not every CPT participates. Likelihood weighting solves this issue by using a weight for each sample, which is the probability of the
evidence variables given the sampled variables. That is, instead of counting all samples equally, we can
CS 188, Fall 2018, Note 6 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: define a weight wj for sample j that reflects how likely the observed values for the evidence variables are,
given the sampled values. In this way, we ensure that every CPT participates. To do this, we iterate through
each variable in the Bayes net, as we do for normal sampling), sampling a value if the variable is not an
evidence variable, or changing the weight for the sample if the variable is evidence. For example, suppose we want to calculate P(T|+c,+e). For the jth sample, we’d perform the following
algorithm:
• Set wj to 1.0, and c = true and e = true. • For T: This is not an evidence variable, so we sample tj from P(T). • For C: This is an evidence variable, so we multiply the weight of the sample by P(+c|tj), i.e. wj =
wj ·P(+c|tj). • For S: sample sj from P(S | tj). • For E: multiply the weight of the sample by P(+e|+c,sj), i.e. wj = wj ·P(+e|+c,sj). Then when we perform the usual counting process, we weight sample j by wj instead of 1, where 0 <=
wj <= 1. This approach works because in the final calculations for the probabilities, the weights effectively
serve to replace the missing CPTs. In effect, we ensure that the weighted probability of each sample is given
by P(z1...zp, e1...em)=[’p
i P(zi | Parents(zi))] · [’m
i P(ei) | Parents(ei))]. For all three of our sampling methods (prior sampling, rejection sampling, and likelihod weighting), we
can get increasing amounts of accuracy by generating additional samples. However, of the three, likelihood
weighting is the most computationally efficient, for reasons beyond the scope of this course. Gibbs Sampling is a fourth approach for sampling. In this approach, we first set all variables to some totally
random value (not taking into account any CPTs).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: We then repeatedly pick one variable at a time, clear its
value, and resample it given the values currently assigned to all other variables. For the T,C,S,E example above, we might assign t = true, c = true, s = false, and e = true. We then pick
one of our four variables to resample, say S, and clear it. We then pick a new variable from the distribution
P(S| +t,+c,+e). This requires us knowing this conditional distribution. It turns out that we can easily
compute the distribution of any single variable given all other variables. More specifically, P(S|T,C,E) can
be calculated only using the CPTs that connect S with its neighbors. Thus, in a typical Bayes Net, where
most variables have only a small number of neighbors, we can precompute the conditional distributions for
each variable given all of its neighbors in linear time. We will not prove this, but if we repeat this process enough times, our later samples will eventually converge
to the correct distribution even though we may start from a low-probability assignment of values.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 6}
Content: If you’re
curious, there are some caveats beyond the scope of the course that you can read about under the Failure
Modes section of the Wikipedia article for Gibbs Sampling. Bayes Nets (D-Separation)
One useful question to ask about a set of random variables is whether or not one variable is independent from
another, or if one random variable is conditionally independent of another given a third random variable. Bayes’ Nets representation of joint probability distributions gives us a way to quickly answer such questions
by inspecting the topological structure of the graph. CS 188, Fall 2018, Note 6 7


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 7}
Content: We already mentioned that a node is conditionally independent of all its ancestor nodes in the graph
given all of its parents. We will present all three canonical cases of connected three-node two-edge Bayes’ Nets, or triples, and the
conditional independence relationships they express. Causal Chains
Figure 1: Causal Chain with no observations. Figure 2: Causal Chain with Y observed. Figure 1 is a configuration of three nodes known as a causal chain. It expresses the following representation
of the joint distribution over X, Y, and Z:
P(x, y,z) = P(z|y)P(y|x)P(x)
It’s important to note that X and Z are not guaranteed to be independent, as shown by the following counterexample:
P(y|x) = (
1 if x = y
0 else
P(z|y) = (
1 if z = y
0 else
In this case, P(z|x) = 1 if x = z and 0 otherwise, so X and Z are not independent. However, we can make the statement that X ?? Z | Y, as in Figure 2. Recall that this conditional indepdence
means:
P(X|Z,Y) = P(X|Y)
We can prove this statement as follows:
P(X|Z, y) = P(X,Z, y)
P(Z, y) = P(Z|y)P(y|X)P(X)
Âx P(X, y,Z) = P(Z|y)P(y|X)P(X)
P(Z|y)Âx P(y|x)P(x)
= P(y|X)P(X)
Âx P(y|x)P(x) = P(y|X)P(X)
P(y) = P(X|y)
An analogous proof can be used to show the same thing for the case where X has multiple parents. To
summarize, in the causal chain chain configuration, X ??

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 7}
Content: Z | Y. Common Cause
Another possible configuration for a triple is the common cause. It expresses the following representation:
P(x, y,z) = P(x|y)P(z|y)P(y)
Just like with causal chain, we can show that X is not guaranteed to be independent of Z with the following
counterexample distribution:
CS 188, Fall 2018, Note 6 8
X
Y
ZX
Y
Z

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 8}
Content: Figure 3: Common Cause with no observations. Figure 4: Common Cause with Y observed. P(x|y) = (
1 if x = y
0 else
P(z|y) = (
1 if z = y
0 else
Then P(x|z) = 1 if x = z and 0 otherwise, so X and Z are not independent. But it is true that X ?? Z | Y. That is, X and Z are independent if Y is observed as in Figure 4. We can show
this as follows:
P(X|Z, y) = P(X,Z, y)
P(Z, y) = P(X|y)P(Z|y)P(y)P(Z|y)P(y) = P(X|y)
Common E↵ect
The final possible configuration for a triple is the common effect, as shown in the figures below. Figure 5: Common Effect with no observations. Figure 6: Common Effect with Y observed. It expresses the representation:
P(x, y,z) = P(y|x,z)P(x)P(z)
In the configuration shown in Figure 5, X and Z are independent: X ?? Z. However, they are not necessarily
independent when conditioned on Y (Figure 6). As an example, suppose all three are binary variables. X
CS 188, Fall 2018, Note 6 9
Y
X
ZY
X
ZX
Z
YX
Z
Y

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 9}
Content: and Z are true and false with equal probability:
P(X = true) = P(X = f alse) = 0.5
P(Z = true) = P(Z = f alse) = 0.5
and Y is determined by whether X and Z have the same value:
P(Y|X,Z) =
8
><
>:
1 if X = Z and Y = true
1 if X 6= Z and Y = f alse
0 else
Then X and Z are independent if Y is unobserved. But if Y is observed, then knowing X will tell us the value
of Z, and vice-versa.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 9}
Content: So X and Z are not conditionally independent given Y. Common Effect can be viewed as “opposite” to Causal Chains and Common Cause – X and Z are guaranteed
to be independent if Y is not conditioned on. But when conditioned on Y, X and Z may be dependent
depending on the specific probability values for P(Y | X,Z)). This same logic applies when conditioning on descendents of Y in the graph. If one of Y’s descendent nodes
is observed, as in Figure 7, X and Z are not guaranteed to be independent. Figure 7: Common Effect with child observations. General Case, and D-separation
We can use the previous three cases as building blocks to help us answer conditional independence questions
on an arbitrary Bayes’ Net with more than three nodes and two edges. We formulate the problem as follows:
Given a Bayes Net G, two nodes X and Y, and a (possibly empty) set of nodes {Z1,...Zk} that represent
observed variables, must the following statement be true: X ?? Y|{Z1,... Zk}? D-separation (directed separation) is a property of the structure of the Bayes Net graph that implies this
conditional independence relationship, and generalizes the cases we’ve seen above. If a set of variables
Z1,···Zk d-separates X and Y, then X ?? Y | {Z1,···Zk} in all possible distributions that can be encoded by
the Bayes net. CS 188, Fall 2018, Note 6 10
X
Z
Y
W

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 10}
Content: We start with an algorithm that is based on a notion of reachability from node X to node Y. (Note: this
algorithm is not quite correct! We’ll see how to fix it in a moment.)
1. Shade all observed nodes {Z1,...Zk} in the graph. 2. If there exists an undirected path from X and Y that is not blocked by a shaded node, X and Y are
“connected”. 3. If X and Y are connected, they’re not conditionally independent given {Z1,...Zk}. Otherwise, they
are. However, this algorithm only works if the Bayes’ Net has no Common Effect structure within the graph, because if it exists, then two nodes are “reachable” when the Y node in Common Effect is activated (observed). To adjust for this, we arrive at the following d-separation algorithm:
1. Shade all observed nodes {Z1,...,Zk} in the graph.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 10}
Content: 2. Enumerate all undirected paths from X to Y. 3. For each path:
(a) Decompose the path into triples (segments of 3 nodes). (b) If all triples are active, this path is active and d-connects X to Y. 4. If no path d-connects X and Y, then X and Y are d-separated, so they are conditionally independent
given {Z1,...,Zk}
Any path in a graph from X to Y can be decomposed into a set of 3 consecutive nodes and 2 edges - each
of which is called a triple. A triple is active or inactive depending on whether or not the middle node is
observed. If all triples in a path are active, then the path is active and d-connects X to Y, meaning X is
not guaranteed to be conditionally independent of Y given the observed nodes. If all paths from X to Y are
inactive, then X and Y are conditionally independent given the observed nodes. Active triples: We can enumerate all possibilities of active and inactive triples using the three canonical
graphs we presented above in Figure 8 and 9. CS 188, Fall 2018, Note 6 11


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 11}
Content: Figure 8: Active triples Figure 9: Inactive triples
Examples
Here are some examples of applying the d-separation algorithm:
This graph contains the common effect and causual
chain canonical graphs. a) R ?? B – Guaranteed
b) R ?? B | T – Not guaranteed
c) R ?? B | T0 – Not guaranteed
d) R ?? T0 | T – Guaranteed
CS 188, Fall 2018, Note 6 12
0008
C
OO
00
O
OQ
OQR
B
T
T'

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 12}
Content: This graph contains combinations of all three canonical graphs (can you list them all?). a) L ?? T0 | T – Guaranteed
b) L ?? B – Guaranteed
c) L ?? B | T – Not guaranteed
d) L ?? B | T0 – Not guaranteed
e) L ?? B | T,R – Guaranteed
This graph contains combinations of all three canonical graphs. a) T ?? D – Not guaranteed
b) T ??

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Notes.pdf', 'page': 12}
Content: D | R – Guaranteed
c) T ?? D | R,S – Not guaranteed
Conclusion
To summarize, Bayes’ Nets is a powerful representation of joint probability distributions. Its topological
structure encodes independence and conditional independence relationships, and we can use it to model
arbitrary distributions to perform inference and sampling. CS 188, Fall 2018, Note 6 13
L
R
B
D
T
T'R
T
D
S

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 7
12 Nov 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 1}
Content: [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley.]
Probability


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 2}
Content: Today
 Probability
 Random Variables
 Joint and Marginal Distributions
 Conditional Distribution
 Product Rule, Chain Rule, Bayes’ Rule
 Inference
 Independence
 You’ll need all this stuff A LOT for the 
next few weeks, so make sure you go 
over it now! 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 3}
Content: Inference in Ghostbusters
 A ghost is in the grid 
somewhere
 Sensor readings tell how 
close a square is to the 
ghost
 On the ghost: red
 1 or 2 away: orange
 3 or 4 away: yellow
 5+ away: green
P(red | 3) P(orange | 3) P(yellow | 3) P(green | 3)
0.05 0.15 0.5 0.3
 Sensors are noisy, but we know P(Color | Distance)
[Demo: Ghostbuster – no probability (L12D1) ]
网

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 4}
Content: Video of Demo Ghostbuster – No probability


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 5}
Content: Uncertainty
 General situation:
 Observed variables (evidence): Agent knows certain 
things about the state of the world (e.g., sensor 
readings or symptoms)
 Unobserved variables: Agent needs to reason about 
other aspects (e.g. where an object is or what disease is 
present)
 Model: Agent knows something about how the known 
variables relate to the unknown variables
 Probabilistic reasoning gives us a framework for 
managing our beliefs and knowledge
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.110.17
0.10
0.10
0.09
0.17
0.10
<0.01
0.09
0.17<0.01
<0.01
0.03
<0.01
0.05
0.05
<0.01
0.05
0.81<0.01

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 6}
Content: Random Variables
 A random variable is some aspect of the world about 
which we (may) have uncertainty
 R = Is it raining?  T = Is it hot or cold?  D = How long will it take to drive to work?  L = Where is the ghost?  We denote random variables with capital letters
 Random variables have domains
 R in {true, false} (often write as {+r, -r})
 T in {hot, cold}
 D in [0, )
 L in possible locations, maybe {(0,0), (0,1), …}


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 7}
Content: Probability Distributions
 Associate a probability with each value
 Temperature:
T P
hot 0.5
cold 0.5
W P
sun 0.6
rain 0.1
fog 0.3
meteor 0.0
 Weather: 
PM
P
100

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 8}
Content: Shorthand notation:
OK if all domain entries are unique
Probability Distributions
 Unobserved random variables have distributions
 A distribution is a TABLE of probabilities of values
 A probability (lower case value) is a single number
 Must have: and
T P
hot 0.5
cold 0.5
W P
sun 0.6
rain 0.1
fog 0.3
meteor 0.0
PM
P
1P(W :
=rain)
0.1
IP(X
Vc
0
=P(X
P
1P(
P(W
razn
raznP(hot
P(T
hot
川P(cold)
P(T
(1o

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 9}
Content: Joint Distributions
 A joint distribution over a set of random variables:
specifies a real number for each assignment (or outcome): 
 Must obey:
 Size of distribution if n variables with domain sizes d?  For all but the smallest distributions, impractical to write out! T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
7
2
n
了
？P(X1
=c1, X2
川
CnD
cn. C
C
1
2
?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 9}
Content: 了
了P(
α1,C2,. Cn）
0P(1,c2,. Cn)
1
=
x1,c2,...MnP
M
+
了

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 10}
Content: Probabilistic Models
 A probabilistic model is a joint distribution 
over a set of random variables
 Probabilistic models:
 (Random) variables with domains 
 Assignments are called outcomes
 Joint distributions: say whether assignments 
(outcomes) are likely
 Normalized: sum to 1.0
 Ideally: only certain variables directly interact
 Constraint satisfaction problems:
 Variables with domains
 Constraints: state whether assignments are 
possible
 Ideally: only certain variables directly interact
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
T W P
hot sun T
hot rain F
cold sun F
cold rain T
Distribution over T,W
Constraint over T,W
二00
？

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 11}
Content: Events
 An event is a set E of outcomes
 From a joint distribution, we can 
calculate the probability of any event
 Probability that it’s hot AND sunny?  Probability that it’s hot?  Probability that it’s hot OR sunny?  Typically, the events we care about 
are partial assignments, like P(T=hot)
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
P(E)
P(c1
Cn
c1...Cn
n)EEP
M
+
了

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 12}
Content: Quiz: Events
 P(+x, +y) ?  P(+x) ?  P(-y OR +x) ? XYP
+x +y 0.2
+x-y 0.3
-x +y 0.4
-x-y 0.1
D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 13}
Content: Marginal Distributions
 Marginal distributions are sub-tables which eliminate variables 
 Marginalization (summing out): Combine collapsed rows by adding
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
T P
hot 0.5
cold 0.5
W P
sun 0.6
rain 0.4
P
M
+
了P(t)
P(t,s)
SPP(s)
P(t, s)
fM
P
1P(X1 =c1) =
P(X1 = x1, X2 = ∞2)
C2P(x,y)
P(x)
P(v)=
MP(x,y)
+0.1+0.3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 14}
Content: Quiz: Marginal Distributions
X Y P
+x +y 0.2
+x -y 0.3
-x +y 0.4
-x -y 0.1
X P
+x
-x
Y P
+y
-y
DP(c)
P(
(x,y)
hD(y)
P(
P(? x,y)
IDP(x,y)
P(x)
P(v)=
MP(x,y)
+0.1+0.3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 15}
Content: Conditional Probabilities
 A simple relation between joint and conditional probabilities
 In fact, this is taken as the definition of a conditional probability
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
P(a) P(b)
P(a,b)
P(a,b)
P(a|b)
P
6P
M
+
了P(W
[T
=??? S
C)P(W
川
S, T
C
P
T
C0.2
0.5×P(W
= s,T
= c) + P(W
= r,T :
=C)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 16}
Content: Quiz: Conditional Probabilities
X Y P
+x +y 0.2
+x -y 0.3
-x +y 0.4
-x -y 0.1
 P(+x | +y) ?  P(-x | +y) ?  P(-y | +x) ? D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 17}
Content: Conditional Distributions
 Conditional distributions are probability distributions over 
some variables given fixed values of others
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
W P
sun 0.8
rain 0.2
W P
sun 0.4
rain 0.6
Conditional Distributions Joint Distribution
M
hot
>P
W
cold
7
1P
M
+
了一

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 18}
Content: Normalization Trick
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
W P
sun 0.4
rain 0.6
P(W :
==
CM)d
川
r, T
C
P
M
T
十
P
M
T
C
S,
川
川
C
 r,P(W
 C
=
P
T
 C0.3
0.6
0.3
0.2
十P
M
+
了P(W
S, T
C
P
M
T
+
P
M
T
C
S,
川
C
r.P(W
川
S, T
C
P
T
C0.2
0.4
0.3
0.2
十M
D
C

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 19}
Content: SELECT the joint 
probabilities 
matching the 
evidence
Normalization Trick
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
W P
sun 0.4
rain 0.6
T W P
cold sun 0.2
cold rain 0.3
NORMALIZE the 
selection
(make it sum to one)
P(W :
T=
=??? 三r
 c)M)d
川
r, T
C
P
M
T
十
P
M
T
C
S,
川
川
C
 r,P(W
 C
=
P
T
 C0.3
0.6
0.3
0.2
十P
M
+
了P(W
S, T
C
P
M
T
+
P
M
T
C
S,
川
C
r.P(W
川
S, T
C
P
T
C0.2
0.4
0.3
0.2
十M
D
CM
D
C

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 20}
Content: Normalization Trick
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
W P
sun 0.4
rain 0.6
T W P
cold sun 0.2
cold rain 0.3
SELECT the joint 
probabilities 
matching the 
evidence
NORMALIZE the 
selection
(make it sum to one)
 Why does this work? Sum of selection is P(evidence)!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 20}
Content: (P(T=c), here)
P
M
+
了M
D
CM
D
CP(
(1,2)
P(
(C1,C2)
P(c1|∞2)
=
P(c2)
Zc1
P(x1,2)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 21}
Content: Quiz: Normalization Trick
X Y P
+x +y 0.2
+x -y 0.3
-x +y 0.4
-x -y 0.1
SELECT the joint 
probabilities 
matching the 
evidence
NORMALIZE the 
selection
(make it sum to one)
 P(X | Y=-y) ? D

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 22}
Content:  (Dictionary) To bring or restore to a normal condition
 Procedure:
 Step 1: Compute Z = sum over all entries
 Step 2: Divide every entry by Z
 Example 1
To Normalize
All entries sum to ONE
W P
sun 0.2
rain 0.3 Z = 0.5
W P
sun 0.4
rain 0.6
 Example 2
T W P
hot sun 20
hot rain 5
cold sun 10
cold rain 15
Normalize
Z = 50
Normalize
T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 23}
Content: Probabilistic Inference
 Probabilistic inference: compute a desired probability 
from other known probabilities (e.g. conditional from 
joint)
 We generally compute conditional probabilities 
 P(airport on time | no reported accidents) = 0.90
 These represent the agent’s beliefs given the evidence
 Probabilities change with new evidence:
 P(airport on time | no accidents, 5 a.m.) = 0.95
 P(airport on time | no accidents, 5 a.m., raining) = 0.80
 Observing new evidence causes beliefs to be updated


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 24}
Content: Inference by Enumeration
 General case:
 Evidence variables: 
 Query* variable:
 Hidden variables: All variables
* Works fine with 
multiple query 
variables, too
 We want:
 Step 1: Select the 
entries consistent 
with the evidence
 Step 2: Sum out H to get joint 
of Query and evidence
 Step 3: Normalize
1
N
n
7
7E1
Ek
:e1
ekH
H
7P
e
1
e
k
√
&D
e
1
e
k
&
7P(Q,1
h1
hr,
e1
ek)
h...hr1
N
n
7
7P(x)
-3
0.05
-1
0.25
0.07
1
0.2
5
0.01
2
0.15Z
P(Q,e
e1
ek
b1
P(Qle1
P(Q,6
ek
-Z
e1
ek

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 25}
Content: 28
Inference by Enumeration
Season
Temp
Weather
P
summer
hot
sun
0.35
P(S I sun)? summer
hot
rain
0.01
hot
fog
summer
0.01
summer
hot
meteor
0.00
summer
cold
sun
0.10
summer
cold
rain
0.05
summer
cold
fog
0.09
summer
cold
meteor
0.00
winter
hot
sun
0.10
winter
hot
rain
0.01
winter
hot
fog
0.02
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 26}
Content: 29
Inference by Enumeration
Season
Temp
Weather
P
summer
hot
sun
0.35
P(S I sun)? summer
hot
rain
0.01
1.Enumerateoptionswith
hot
fog
summer
0.01
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
summer
cold
rain
0.05
cold
fog
summer
0.09
summer
cold
meteor
0.00
winter
hot
sun
0.10
winter
hot
rain
0.01
winter
hot
fog
0.02
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 27}
Content: 30
Inference by Enumeration
Season
Temp
Weather
hot
summer
sun
0.35
P(S I sun)? summer
hot
rain
1.Enumerateoptionswith
summer
hot
fog
0.01
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
summer
cold
rain
summer
cold
fog
0.09
summer
cold
meteor
winter
hot
sun
0.10
Mintei
hot
rain
winter
hot
fog
0.02
winter
hot
meteor
winter
cold
sun
0.15
winter
cold
rain
winter
cold
fog
0.18
wjnter
cald
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 28}
Content: 31
Inference by Enumeration
Season
Temp
Weather
P
summer
hot
sun
0.35
P(S I sun)? summer
hot
rain
0.01
1.Enumerateoptionswith
summer
hot
fog
0.01
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
2.Sumoutirrelevant
summer
cold
rain
0.05
variable(s)
summer
cold
fog
0.09
summer
cold
meteor
0.00
winter
hot
sun
0.10
winter
hot
rain
0.01
winter
hot
fog
0.02
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 29}
Content: 32
Inference by Enumeration
Season
Temp
Weather
P
hot
summer
sun
0.35
P(SIsun)? summer
hot
rain
0.01
1.Enumerateoptionswith
summer
hot
fog
0.01
0.45
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
2.Sumoutirrelevant
variable(s)
summer
cold
rain
0.05
summer
cold
fog
0.09
summer
cold
meteor
0.00
winter
hot
sun
0.10
winter
hot
rain
0.01
winter
hot
fog
0.02
0.25
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 30}
Content: 33
Inference by Enumeration
Season
Temp
Weather
P
summer
hot
sun
0.35
P(S I sun)? summer
hot
rain
0.01
1.Enumerateoptionswith
summer
hot
fog
0.01
0.45
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
2.Sumoutirrelevant
variable(s)
summer
cold
rain
0.05
summer
cold
fog
0.09
3.Normalize
summer
cold
meteor
0.00
winter
hot
sun
0.10
winter
hot
rain
0.01
winter
hot
fog
0.02
0.25
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 31}
Content: 34
Inference by Enumeration
Season
Temp
Weather
P
summer
hot
sun
0.35
P(SI sun)? summer
hot
rain
0.01
1.Enumerateoptionswith
summer
hot
fog
0.01
0.45
sun
summer
hot
meteor
0.00
summer
cold
sun
0.10
2.Sumoutirrelevant
variable(s)
summer
cold
rain
0.05
summer
cold
fog
0.09
3.Normalize
summer
cold
meteor
0.00
P(S|sun)=
winter
hot
sun
0.10
{summer:0.45/(0.45+0.25),
winter
hot
rain
0.01
winter
hot
fog
0.02
winter: 0.25/(0.45+0.25))
0.25
winter
hot
meteor
0.00
winter
cold
sun
0.15
winter
cold
rain
0.20
winter
cold
fog
0.18
winter
cold
meteor
0.00

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 32}
Content: Inference by Enumeration
 P(W)?  P(W | winter)?  P(W | winter, hot)? S T W P
summer hot sun 0.30
summer hot rain 0.05
summer cold sun 0.10
summer cold rain 0.05
winter hot sun 0.10
winter hot rain 0.05
winter cold sun 0.15
winter cold rain 0.20


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 33}
Content:  Obvious problems:
 Worst-case time complexity O(d
n
) 
 Space complexity O(d
n
) to store the joint distribution
Inference by Enumeration


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 34}
Content: The Product Rule
 Sometimes have conditional distributions but want the joint
P(x,y)
P(xly):
P(
(y)P(? (y)P(
P(
y)
x,y
I8
X
=

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 35}
Content: The Product Rule
 Example:
R P
sun 0.8
rain 0.2
D W P
wet sun 0.1
dry sun 0.9
wet rain 0.7
dry rain 0.3
D W P
wet sun 0.08
dry sun 0.72
wet rain 0.14
dry rain 0.06
P
M
? 1D
M
了M
P
1P(? (y)P(
P(
y)
x,y
I

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 36}
Content: The Chain Rule
 More generally, can always write any joint distribution as an 
incremental product of conditional distributions
 Why is this always true? P(x1,2,3) = P(x1)P(x2|x1)P(x3|1,2)P(x1,x2,...cn) =
11
P(xic...i-1)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 37}
Content: Bayes Rule


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 38}
Content: Bayes’ Rule
 Two ways to factor a joint distribution over two variables:
 Dividing, we get:
 Why is this at all helpful?  Lets us build one conditional from its reverse
 Often one conditional is tricky but the other one is simple
 Foundation of many systems we’ll see later (e.g. ASR, MT)
 In the running for most important AI equation!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 38}
Content: That’s my rule! P(c,y)
P(c|y)P(y)P(y|∞)
P(cly) :
(P(c)
=
P
(y)y

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 39}
Content: Inference with Bayes’ Rule
 Example: Diagnostic probability from causal probability:
 Example:
 M: meningitis, S: stiff neck
 Note: posterior probability of meningitis still very small
 Note: you should still get stiff necks checked out! Why? P(s I m) = 0.8
Example
P(m) = 0.0001
givens
P(s) = 0.01
P(s|m)P(m)
0.8x0.0001
P(m|s)=
P(s)
0.01

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 40}
Content: Quiz: Bayes’ Rule
 Given:
 What is P(W | dry) ? R P
sun 0.8
rain 0.2
D W P
wet sun 0.1
dry sun 0.9
wet rain 0.7
dry rain 0.3
P
M
? 1M
P
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 41}
Content: Ghostbusters, Revisited
 Let’s say we have two distributions:
 Prior distribution over ghost location: P(G)
 Let’s say this is uniform
 Sensor reading model: P(R | G)
 Given: we know what our sensors do
 R = reading color measured at (1,1)
 E.g. P(R = yellow | G=(1,1)) = 0.1
 We can calculate the posterior 
distribution P(G|r) over ghost locations 
given a reading using Bayes’ rule:
[Demo: Ghostbuster – with probability (L12D2) ]
0.17
0.10
0.10
0.09
0.17
0.10
<0.01
0.09
0.170.11
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.11(46)d
(6)d(6|)d x

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 1.pdf', 'page': 42}
Content: Video of Demo Ghostbusters with Probability


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 0}
Content: Bayes’ Nets
[These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.]


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 1}
Content: Probabilistic Models
 Models describe how (a portion of) the world works
 Models are always simplifications
 May not account for every variable
 May not account for all interactions between variables
 “All models are wrong; but some are useful.”
– George E. P. Box
 What do we do with probabilistic models?  We (or our agents) need to reason about unknown 
variables, given evidence
 Example: explanation (diagnostic reasoning)
 Example: prediction (causal reasoning)
 Example: value of information


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 2}
Content: Independence


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 3}
Content:  Two variables are independent if:
 This says that their joint distribution factors into a product two 
simpler distributions
 Another form:
 We write: 
 Independence is a simplifying modeling assumption
 Empirical joint distributions: at best “close” to independent
 What could we assume for {Weather, Traffic, Cavity, Toothache}? Independence
y : P(c,y)
P(c)P(y)
Vc, y
=h‘cA
P(c)
P(c)
y)
=

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 4}
Content: Example: Independence? T W P
hot sun 0.4
hot rain 0.1
cold sun 0.2
cold rain 0.3
T W P
hot sun 0.3
hot rain 0.2
cold sun 0.3
cold rain 0.2
T P
hot 0.5
cold 0.5
W P
sun 0.6
rain 0.4
M
D
1
1DM
DP
M
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 5}
Content: Example: Independence
 N fair, independent coin flips:
H 0.5
T 0.5
H 0.5
T 0.5
H 0.5
T 0.5
P
1PP(X1. X2,
P
Xn.on0.411
7

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 6}
Content: Conditional Independence
 P(Toothache, Cavity, Catch)
 If I have a cavity, the probability that the probe catches in it 
doesn't depend on whether I have a toothache:
 P(+catch | +toothache, +cavity) = P(+catch | +cavity)
 The same independence holds if I don’t have a cavity:
 P(+catch | +toothache, -cavity) = P(+catch| -cavity)
 Catch is conditionally independent of Toothache given Cavity:
 P(Catch | Toothache, Cavity) = P(Catch | Cavity)
 Equivalent statements:
 P(Toothache | Catch , Cavity) = P(Toothache | Cavity)
 P(Toothache, Catch | Cavity) = P(Toothache | Cavity) P(Catch | Cavity)
 One can be derived from the other easily


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 7}
Content: Conditional Independence
 Unconditional (absolute) independence very rare (why?)
 Conditional independence is our most basic and robust form 
of knowledge about uncertain environments.  X is conditionally independent of Y given Z
if and only if:
or, equivalently, if and only if
Vx, y,2 : P(x, y2)
P(c|2)P(y2)
川X
YVx, y,z : P(xl
P(c
2, y)
Z

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 8}
Content: Conditional Independence
 What about this domain:
 Traffic
 Umbrella
 Raining


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 9}
Content: Conditional Independence
 What about this domain:
 Fire
 Smoke
 Alarm


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 10}
Content: Conditional Independence and the Chain Rule
 Chain rule: 
 Trivial decomposition:
 With assumption of conditional independence:
 Bayes’nets / graphical models help us express conditional independence assumptions
P(Rain)P(Traffic|Rain)P(Umbrella|Rain,
TrafficP(Traffic,
Rain, UmbrellaP(Traffic,
Rain, UmbrellaP(Rain)P(Traffic|Rain)P(Umbrella
RainP(X1, X2,... Xn) = P(X1)P(X2|X1)P(X3|X1, X2) ..

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 11}
Content: Inference in Ghostbusters
 A ghost is in the grid 
somewhere
 Sensor readings tell how 
close a square is to the 
ghost
 On the ghost: red
 1 or 2 away: orange
 3 or 4 away: yellow
 5+ away: green
网

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 12}
Content: Video of Demo Ghostbusters with Probability


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 13}
Content: 15
Ghostbusters model
Variables and ranges:
0.11
0.11
0.11
G (ghost location) in {(1,1),..,(3,3)}
0.11
0.11
0.11
Cx,y
(color measured at square x,y) in
{red,orange,yellow,green}
0.11
0.11
0.11
Ghostbuster physics:
Uniform prior distribution over ghost location: P(G)
Sensor model: P(Cxy I G) (depends only on distance to G)
■ E.g. P(C1,1 = yellow | G =(1,1)) = 0.1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 14}
Content: 16
Ghostbusters model, contd. P(G, C1,1, .. C3,3)
has 9 x 49 = 2,359,296 entries!!! 0.11
00
0.11
Ghostbuster independence:
0.11
0.11
0.11
Are
e C1,1 and
C1,2 independent? 0.11
0.11
0.11
E.g., does P(C1,1 = yellow) = P(C1,1 = yellow I C1,2 = orange)
Ghostbuster physics again:
P(Cxy I G) depends only on distance to G
■ So P(C1,1 = yellow I G = (2,3)) = P(C1,1 = yellow I G = (2,3), C1,2 = orange)
· I.e., C1,1 is conditionally independent of C1,2 given G 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 15}
Content: 17
Ghostbusters model, contd. Apply the chain rule to decompose the joint probability model:
P(G, C1,1, . C3,3) = P(G) P(C1,1 I G) P(C1,2 I G, C1,1) P(C1,3 I G, C1,1, C1,2) ... P(C3,3 I G, C1,1 , C3,2)
Now simplify using conditional independence:
P(G, C1,1, . C3,3) = P(G) P(C1,1 I G) P(C1,2 I G) P(C1,3 I G) .. P(C3,3 I G)
I.e., conditional independence properties of ghostbuster physics simplify the probability
model from exponential to quadratic in the number of squares

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 16}
Content: Bayes’Nets: Big Picture
DISTRIBUTIONS
IN N EASY STEPS!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 17}
Content: Bayes’ Nets: Big Picture
 Two problems with using full joint distribution tables 
as our probabilistic models:
 Unless there are only a few variables, the joint is WAY too 
big to represent explicitly
 Hard to learn (estimate) anything empirically about more 
than a few variables at a time
 Bayes’ nets: a technique for describing complex joint 
distributions (models) using simple, local 
distributions (conditional probabilities)
 More properly called graphical models
 We describe how variables locally interact
 Local interactions chain together to give global, indirect 
interactions
 For about 10 min, we’ll be vague about how these 
interactions are specified
0.411
7

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 18}
Content: Graphical Model Notation
 Nodes: variables (with domains)
 Can be assigned (observed) or unassigned 
(unobserved)
 Arcs: interactions
 Similar to CSP constraints
 Indicate “direct influence” between variables
 Formally: encode conditional independence 
(more later)
 For now: imagine that arrows mean 
direct causation (in general, they don’t!)
WeatherCavity
er
Toothache
Catch

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 19}
Content: Example: Coin Flips
 N independent coin flips
 No interactions between variables: absolute independence
X1 X2 Xn


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 20}
Content: 
Example: Traffic
Variables:
T: There is traffic
U: I'm holding my umbrella
R: It rains
R
T
U

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 21}
Content: 
Example: Smoke alarm
Variables:
F
F: There is fire
S: There is smoke
A: Alarm sounds
S
A

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 22}
Content: 
Example: Ghostbusters
Variables:
0.11
0.11
G: The ghost's location
C1,1,.. C3,3 :
0.11
0.11
0.11
G
The observation at each location
0.11
0.11
0.11
Want to estimate:
P( G | C1,1, .. C3,3 )
C1,1
C1,2
C3,3
This is called a Naive Bayes model:
One discrete query variable (often called the class or category variable)
All other variables are (potentially) evidence variables
Evidence variables are all conditionally independent given the query variable

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 23}
Content: 
Example Bayes' Net: Car Insurance
Age
SocioEcon
GoodStudent
ExtraCar
RiskAversion
MakeModel
VehicleYear
YearsLicensed
Mileage
DrivingSkill
AntiTheft
SafetyFeatures
CarValue
Garaged
Airbag
DrivingRecord
Ruggedness
DrivingBehavior
Cushioning
Theft
Accident
OwnCarDamage
OwnCarCost
OtherCost
MedicalCost
LiabilityCost
PropertyCost

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 24}
Content: 
Example Bayes' Net: Car Won't Start
alternator
fanbelt
battery age
broken
broken
battery
no charging
dead
battery
battery
fuel line
starter
no oil
flat
no gas
blocked
broken
meter
carwon't
lights
oil light
gasgauge
dipstick
start

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 25}
Content: Bayes’ Net Semantics
Build Your
Own
Bnyes Net

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 26}
Content: 
Bayes Net Syntax
A set of nodes, one per variable X;
P(G)
(1,1)
(1,2)
G
(1,3)
A directed, acyclic graph
0.11
0.11
0.11
A conditional distribution for each node
given its parent variables in the graph
C3,3)
CPT (conditional probability table); each row is a
P(C1,1 I G)
distribution for child given values of its parents
g
(1,1)
0.01
0.1
0.3
0.59
(1,2)
0.1
0.3
0.5
0.1
(1,3)
0.3
0.5
0.19
0.01
Bayes net = Topology (graph) + Local Conditional Probabilities

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 27}
Content: Example: Alarm Network
 Variables
 B: Burglary
 E: Earthquake
 A: Alarm goes off
 M: Mary calls
 J: John calls


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 28}
Content: 
Example: Alarm Network
P(E)
P(B)
true
false
true
false
Burglary
Earthquake
0.002
0.998
0.001
0.999
B
E
P(A|B,E)
true
false
true
true
0.95
0.05
Alarm
true
false
0.94
0.06
false
true
0.29
0.71
Number of free parameters
false
false
0.001
0.999
in each CPT:
John
Mary
calls
calls
Parent range sizes d1,...,dk
A
P(JIA)
A
P(M|A)
true
false
true
false
Child range size d
true
0.9
0.1
true
0.7
0.3
Each table row must sum to 1
false
0.05
0.95
false
0.01
0.99
(d-1) II; d;

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 29}
Content: 
Bayes net global semantics
Bayes nets encode joint distributions as product of
conditional distributions on each variable:
P(X1,..,Xn) = II; P(X; I Parents(X;))
Exploits sparse structure: number of parents is
usuallysmall

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 30}
Content: 33
Size of a Bayes Net
How big is a joint distribution over N
Both give you the power to calculate
variables, each with d values? P(X1, X2, .., XN)
QN
Bayes Nets: huge space savings with sparsity! How big is an N-node net if nodes
Also easier to elicit local CPTs
have at most k parents? Also faster to answer queries (coming)
O(N * dk)
idwod

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 31}
Content: 34
Example
P(E)
P(b,-e, a, -j, -m) =
P(B)
true
false
true
false
Burglary
Earthquake
P(b) P(-e) P(a|b,-e) P(-jla) P(-m|a)
0.002
0.998
0.001
0.999
=.001x.998x.94x.1x.3=.000028
B
E
P(A|B,E)
true
false
true
true
0.95
0.05
Alarm
true
false
0.94
0.06
false
true
0.29
0.71
false
false
0.001
0.999
John
Mary
calls
calls
A
P(J|A)
A
P(M|A)
true
false
true
false
true
0.9
0.1
true
0.7
0.3
false
0.05
0.95
false
0.01
0.99
32

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 32}
Content: 35
Conditional independence in BNs
Compare the Bayes net global semantics
P(X1...,Xn) = II; P(X; I Parents(X;))
with the chain rule identity
(T-!x"x I 'X)d !II = ('x"Tx)d
Assume (without loss of generality) that X1...,X, sorted in topological order according to
the graph (i.e., parents before children), so Parents(X;) ≤ X1,..,Xi-1
So the Bayes net asserts conditional independences P(X, I X1,..,Xi-1) = P(X, I Parents(X;))
To ensure these are valid, choose parents for node X, that "shield" it from other predecessors

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 33}
Content: 
Conditional independence semantics
Every variable is conditionally independent of its non-descendants given its parents
Conditional independence semantics <=> global semantics
n
Um
X
Zj
Y
34

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 34}
Content: 
Example: Burglary
P(B)
P(E)
true
false
Burglary
true
false
0.001
0.999
0.002
0.998
Earthquake
Burglary
Earthquake
Alarm
Alarm
B
E
P(A|B,E)
true
false
true
true
0.95
0.05
true
false
0.94
0.06
false
true
0.29
0.71
false
false
0.001
0.999
35

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 35}
Content: 38
Example: Burglary
P(A)
Alarm
true
false
Alarm
Burglary
Earthquake
B
P(E|A,B)
A
P(B|A)
true
false
Burglary
Earthquake
true
false
true
true
true
true
false
false
false
true
false
false
36

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 36}
Content: Example: Traffic
R
T
+r 1/4
-r 3/4
+r +t 3/4
-t 1/4
-r +t 1/2
-t 1/2
RRD
t
-r
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 37}
Content: Example: Traffic
 Causal direction
R
T
+r 1/4
-r 3/4
+r +t 3/4
-t 1/4
-r +t 1/2
-t 1/2
+r +t 3/16
+r -t 1/16
-r +t 6/16
-r -t 6/16
RR1
1
R

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 38}
Content: Example: Reverse Traffic
 Reverse causality? T
R
+t 9/16
-t 7/16
+t +r 1/3
-r 2/3
-t +r 1/7
-r 6/7
+r +t 3/16
+r -t 1/16
-r +t 6/16
-r -t 6/16
1
1
RR
T
1

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 39}
Content: Causality?  When Bayes’ nets reflect the true causal patterns:
 Often simpler (nodes have fewer parents)
 Often easier to think about
 Often easier to elicit from experts
 BNs need not actually be causal
 Sometimes no causal net exists over the domain 
(especially if variables are missing)
 E.g. consider the variables Traffic and Drips
 End up with arrows that reflect correlation, not causation
 What do the arrows really mean?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 39}
Content:  Topology may happen to encode causal structure
 Topology really encodes conditional independence
?P(cc1.,...i-1)
= P(c;parents(Xi))

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 8 Part 2.pdf', 'page': 40}
Content: 43
Summary
Independence and conditional independence are
important forms of probabilistic knowledge
Bayes net encode joint distributions efficiently by
taking advantage of conditional independence
Global joint probability = product of local conditionals
Allows for flexible tradeoff between model accuracy
and memory/compute efficiency
A
A
B
B
A
B
B
E
E
E
Strict Independence
NaiveBayes
Sparse Bayes Net
Joint Distribution

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 2
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Constraint Satisfaction Problems
In the previous note, we learned how to find optimal solutions to search problems, a type of planning
problem. Now, we’ll learn about solving a related class of problems, constraint satisfaction problems
(CSPs). Unlike search problems, CSPs are a type of identification problem, problems in which we must
simply identify whether a state is a goal state or not, with no regard to how we arrive at that goal. CSPs are
defined by three factors:
1. Variables - CSPs possess a set of N variables X1,...,XN that can each take on a single value from some
defined set of values. 2. Domain - A set {x1,...,xd} representing all possible values that a CSP variable can take on. 3.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 0}
Content: Constraints - Constraints define restrictions on the values of variables, potentially with regard to other
variables. Consider the N-queens identification problem: given an N ⇥ N chessboard, can we find a configuration in
which to place N queens on the board such that no two queens attack each another? We can formulate this problem as a CSP as follows:
1. Variables - Xi j, with 0  i, j < N. Each Xi j represents a grid position on our N ⇥N chessboard, with i
and j specifying the row and column number respectively. 2. Domain - {0,1}. Each Xi j can take on either the value 0 or 1, a boolean value representing the
existence of a queen at position (i, j) on the board. 3. Constraints -
• 8i, j, k (Xi j,Xik) 2 {(0,0),(0,1),(1,0)}. This constraint states that if two variables have the same
value for i, only one of them can take on a value of 1. This effectively encapsulates the condition
that no two queens can be in the same row. CS 188, Fall 2018, Note 2 1
业
业
不
业

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 1}
Content: • 8i, j, k (Xi j,Xk j) 2 {(0,0),(0,1),(1,0)}. Almost identically to the previous constraint, this constraint states that if two variables have the same value for j, only one of them can take on a value
of 1, encapsulating the condition that no two queens can be in the same column. • 8i, j, k (Xi j,Xi+k, j+k) 2 {(0,0),(0,1),(1,0)}. With similar reasoning as above, we can see that
this constraint and the next represent the conditions that no two queens can be in the same major
or minor diagonals, respectively. • 8i, j, k (Xi j,Xi+k, jk) 2 {(0,0),(0,1),(1,0)}. • Â
i, j
Xi j = N. This constraint states that we must have exactly N grid positions marked with a 1,
and all others marked with a 0, capturing the requirement that there are exactly N queens on the
board. Constraint satisfaction problems are NP-hard, which loosely means that there exists no known algorithm for
finding solutions to them in polynomial time. Given a problem with N variables with domain of size O(d) for
each variable, there are O(dN) possible assignments, exponential in the number of variables. We can often
get around this caveat by formulating CSPs as search problems, defining states as partial assignments
(variable assignments to CSPs where some variables have been assigned values while others have not). Correspondingly, the successor function for a CSP state outputs all states with one new variable assigned,
and the goal test verifies all variables are assigned and all constraints are satisfied in the state it’s testing. Constraint satisfaction problems tend to have significantly more structure than traditional search problems,
and we can exploit this structure by combining the above formulation with appropriate heuristics to hone in
on solutions in a feasible amount of time. Constraint Graphs
Let’s introduce a second CSP example: map coloring. Map coloring solves the problem where we’re given
a set of colors and must color a map such that no two adjacent states or regions have the same color. Constraint satisfaction problems are often represented as constraint graphs, where nodes represent variables
and edges represent constraints between them. There are many different types of constraints, and each is
handled slightly differently:
• Unary Constraints - Unary constraints involve a single variable in the CSP. They are not represented
in constraint graphs, instead simply being used to prune the domain of the variable they constrain
when necessary.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 1}
Content: CS 188, Fall 2018, Note 2 2


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 2}
Content: • Binary Constraints - Binary constraints involve two variables. They’re represented in constraint
graphs as traditional graph edges. • Higher-order Constraints - Constraints involving three or more variables can also be represented with
edges in a CSP graph, they just look slightly unconventional. Consider map coloring the map of Australia:
The constraints in this problem are simply that no two adjacent states can be the same color. As a result, by
drawing an edge between every pair of states that are adjacent to one another, we can generate the constraint
graph for the map coloring of Australia as follows:
The value of constraint graphs is that we can use them to extract valuable information about the structure of
the CSPs we are solving. By analyzing the graph of a CSP, we can determine things about it like whether
it’s sparsely or densely connected/constrained and whether or not it’s tree-structured. We’ll cover this more
in depth as we discuss solving constraint satisfaction problems in more detail.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 2}
Content: CS 188, Fall 2018, Note 2 3
Northern
Territory
Western
Queensland
Australia
South
Australia
NewSouthWales
Victoria
TasmaniaNT
WA
SA
NSW
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 3}
Content: Solving Constraint Satisfaction Problems
Constraint satisfaction problems are traditionally solved using a search algorithm known as backtracking
search. Backtracking search is an optimization on depth first search used specifically for the problem of
constraint satisfaction, with improvements coming from two main principles:
1. Fix an ordering for variables, and select values for variables in this order.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 3}
Content: Because assignments are
commutative (e.g. assigning WA = Red, NT = Green is identical to NT = Green, WA = Red), this
is valid. 2. When selecting values for a variable, only select values that don’t conflict with any previously assigned values. If no such values exist, backtrack and return to the previous variable, changing its
value. The pseudocode for how recursive backtracking works is presented below:
For a visualization of how this process works, consider the partial search trees for both depth first search
and backtracking search in map coloring:
CS 188, Fall 2018, Note 2 4
function BACKTRACKING-SEARCH(csp) returns solution/failure
return RECURSIVE-BACKTRACKING({}, csp)
function RECURSIVE-BACKTRACKING(assignment, csp) returns soln/failure
if assignment is complete then return assignment
var< SELECT-UNASSIGNED-VARIABLE(VARIABLES[csp], assignment, csp)
for each value in ORDER-DoMAIN-VALUEs(var, assignment, csp)
op(
if value is consistent with assignment given CoNsTRAINTs[csp] then
add {var = value} to assignment
result < RECURSIVE-BACKTRACKING(assignment, csp)
if result ≠ failure then return result
remove {var = value} from assignment
return failure

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 4}
Content: Note how DFS regretfully colors everything red before ever realizing the need for change, and even then
doesn’t move too far in the right direction towards a solution. On the other hand, backtracking search only
assigns a value to a variable if that value violates no constraints, leading to a significantly less backtracking. Though backtracking search is a vast improvement over the brute-forcing of depth first search, we can get
more gains in speed still with further improvements through filtering, variable/value ordering, and structural
explotation. Filtering
The first improvement to CSP performance we’ll consider is filtering, which checks if we can prune the
domains of unassigned variables ahead of time by removing values we know will result in backtracking. A naïve method for filtering is forward checking, which whenever a value is assigned to a variable Xi,
prunes the domains of unassigned variables that share a constraint with Xi that would violate the constraint
if assigned. Whenever a new variable is assigned, we can run forward checking and prune the domains
of unassigned variables adjacent to the newly assigned variable in the constraint graph. Consider our map
coloring example, with unassigned variables and their potential values:
Note how as we assign WA = red and then Q = green, the size of the domains for NT, NSW, and SA (states
adjacent to WA, Q, or both) decrease in size as values are eliminated. The idea of forward checking can be
generalized into the principle of arc consistency. For arc consistency, we interpret each undirected edge of
the constraint graph for a CSP as two directed edges pointing in opposite directions. Each of these directed
edges is called an arc. The arc consistency algorithm works as follows:
• Begin by storing all arcs in the constraint graph for the CSP in a queue Q. • Iteratively remove arcs from Q and enforce the condition that in each removed arc Xi ! Xj, for every
remaining value v for the tail variable Xi, there is at least one remaining value w for the head variable
Xj such that Xi = v,Xj = w does not violate any constraints. If some value v for Xi would not work
with any of the remaining values for Xj, we remove v from the set of possible values for Xi. • If at least one value is removed for Xi when enforcing arc consistency for an arc Xi ! Xj, add arcs
of the form Xk ! Xi to Q, for all unassigned variables Xk. If an arc Xk ! Xi is already in Q during
this step, it doesn’t need to be added again.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 4}
Content: • Continue until Q is empty, or the domain of some variable is empty and triggers a backtrack. The arc consistency algorithm is typically not the most intuitive, so let’s walk through a quick example with
map coloring:
CS 188, Fall 2018, Note 2 5
WA
NT
NSW
V
SA
NT
Q
WA
SA
NSW

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 5}
Content: We begin by adding all arcs between unassigned variables sharing a constraint to a queue Q, which gives us
Q = [SA ! V,V ! SA,SA ! NSW,NSW ! SA,SA ! NT,NT ! SA,V ! NSW,NSW ! V]
For our first arc, SA !V, we see that for every value in the domain of SA, {blue}, there is at least one value
in the domain of V, {red,green,blue}, that violates no constraints, and so no values need to be pruned from
SA’s domain. However, for our next arc V ! SA, if we set V = blue we see that SA will have no remaining
values that violate no constraints, and so we prune blue from V’s domain. Because we pruned a value from the domain of V, we need to enqueue all arcs with V at the head - SA !V,
NSW ! V. Since NSW ! V is already in Q, we only need to add SA ! V, leaving us with our updated
queue
Q = [SA ! NSW,NSW ! SA,SA ! NT,NT ! SA,V ! NSW,NSW ! V,SA ! V]
We can continue this process until we eventually remove the arc SA ! NT from Q. Enforcing arc consistency on this arc removes blue from SA’s domain, leaving it empty and triggering a backtrack. Note that the
arc NSW ! SA appears before SA ! NT in Q and that enforcing consistency on this arc removes blue from
the domain of NSW. Arc consistency is typically implemented with the AC-3 algorithm (Arc Consistency Algorithm #3), for
which the pseudocode is as follows:
CS 188, Fall 2018, Note 2 6
S
WA
NT
Q
NSW
V
SA
NT
Q
WA
SA
口
NSWS
WA
NT
Q
NSW
V
SA
NT
Q
WA
SA
口
NSWWA
NT
Q
NSW
V
SA
NT
Q
WA
SA
NSW

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 6}
Content: The AC-3 algorithm has a worst case time complexity of O(ed3), where e is the number of arcs (directed
edges) and d is the size of the largest domain. Overall, arc consistency is more holistic of a domain pruning
technique than forward checking and leads to fewer backtracks, but requires running significantly more
computation in order to enforce. Accordingly, it’s important to take into account this tradeoff when deciding
which filtering technique to implement for the CSP you’re attempting to solve. As an interesting parting note about consistency, arc consistency is a subset of a more generalized notion
of consistency known as k-consistency, which when enforced guarantees that for any set of k nodes in the
CSP, a consistent assignment to any subset of k  1 nodes guarantees that the kth node will have at least
one consistent value. This idea can be further extended through the idea of strong k-consistency. A graph
that is strong k-consistent possesses the property that any subset of k nodes is not only k-consistent but
also k  1, k  2,...,1 consistent as well. Not surprisingly, imposing a higher degree of consistency on a
CSP is more expensive to compute. Under this generalized definition for consistency, we can see that arc
consistency is equivalent to 2-consistency. Ordering
We’ve delineated that when solving a CSP, we fix some ordering for both the variables and values involved. In practice, it’s often much more effective to compute the next variable and corresponding value "on the fly"
with two broad principles, minimum remaining values and least constraining value:
• Minimum Remaining Values (MRV) - When selecting which variable to assign next, using an MRV
policy chooses whichever unassigned variable has the fewest valid remaining values (the most constrained variable). This is intuitive in the sense that the most constrained variable is most likely to run
out of possible values and result in backtracking if left unassigned, and so it’s best to assign a value to
it sooner than later. • Least Constraining Value (LCV) - Similarly, when selecting which value to assign next, a good policy
to implement is to select the value that prunes the fewest values from the domains of the remaining unassigned values. Notably, this requires additional computation (e.g.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 6}
Content: rerunning arc consistency/forward checking or other filtering methods for each value to find the LCV), but can still yield
speed gains depending on usage. CS 188, Fall 2018, Note 2 7
function AC-3( csp) returns the CSP, possibly with reduced domains
inputs: csp, a binary CSP with variables {X1, X2, ..., Xn}
local variables: queue, a queue of arcs, initially all the arcs in csp
while queue is not empty do
tail
head
(X, X;)←REMOVE-FIRST(queue)
if REMOVE-INCONSISTENT-VALUES(X, X§) then
for each X in NEIGHBORs[Xi] do
add (Xk, Xi) to queue
function REMOVE-INCONSISTENT-VALUES( X, X§) returns true iff succeeds
removed← false
for each r in DoMAIN[X;] do
if no value y in DoMAIN[X)] allows (c,y) to satisfy the constraint X; → Xj
then delete r from DoMAIN[Xi];
; removed←true
return removed

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 7}
Content: Structure
A final class of improvements to solving constraint satisfaction problems are those that exploit their structure. In particular, if we’re trying to solve a tree-structured CSP (one that has no loops in its constraint
graph), we can reduce the runtime for finding a solution from O(dN) all the way to O(nd2), linear in the
number of variables. This can be done with the tree-structured CSP algorithm, outlined below:
• First, pick an arbitrary node in the constraint graph for the CSP to serve as the root of the tree (it
doesn’t matter which one because basic graph theory tells us any node of a tree can serve as a root). • Convert all undirected edges in the tree to directed edges that point away from the root.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 7}
Content: Then linearize
(or topologically sort) the resulting directed acyclic graph. In simple terms, this just means order the
nodes of the graph such that all edges point rightwards. Noting that we select node A to be our root
and direct all edges to point away from A, this process results in the following conversion for the CSP
presented below:
• Perform a backwards pass of arc consistency. Iterating from i = n down to i = 2, enforce arc consistency for all arcs Parent(Xi) ! Xi. For the linearized CSP from above, this domain pruning will
eliminate a few values, leaving us with the following:
• Finally, perform a forward assignment. Starting from X1 and going to Xn, assign each Xi a value
consistent with that of its parent. Because we’ve enforced arc consistency on all of these arcs, no
matter what value we select for any node, we know that its children will each all have at least one
consistent value. Hence, this iterative assignment guarantees a correct solution, a fact which can be
proven inductively without difficulty. The tree structured algorithm can be extended to CSPs that are reasonably close to being tree-structured
with cutset conditioning. Cutset conditioning involves first finding the smallest subset of variables in a
constraint graph such that their removal results in a tree (such a subset is known as a cutset for the graph). For example, in our map coloring example, South Australia (SA) is the smallest possible cutset:
CS 188, Fall 2018, Note 2 8
A
E
A
B
C
D
E
F
B
D
C
FA
B
C
D
E
F

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 8}
Content: Once the smallest cutset is found, we assign all variables in it and prune the domains of all neighboring
nodes. What’s left is a tree-structured CSP, upon which we can solve with the tree-structured CSP algorithm
from above! The initial assignment to a cutset of size c may leave the resulting tree-structured CSP(s) with
no valid solution after pruning, so we may still need to backrack up to dc times. Since removal of the cutset
leaves us with a tree-structured CSP with (n  c) variables, we know this can be solved (or determined
that no solution exists) in O((n  c)d2). Hence, the runtime of cutset conditioning on a general CSP is
O(dc(nc)d2), very good for small c. Local Search
As a final topic of interest, backtracking search is not the only algorithm that exists for solving constraint
satisfaction problems.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 8}
Content: Another widely used algorithm is local search, for which the idea is childishly
simple but remarkably useful. Local search works by iterative improvement - starting with some random
assignment to values then repeatedly selecting the variable that violates the most constraints and resetting it
to the value that violates the fewest constraints (a policy known as the min-conflicts heuristic). Under such
a policy, constraint satisfaction problems like N-queens becomes both very time efficient and space efficient
to solve. For example, in following example with 4 queens, we arrive at a solution after only 2 iterations:
In fact, local search appears to run in almost constant time and have a high probability of success not only
for N-queens with arbitrarily large N, but also for any randomly generated CSP! However, despite these
advantages, local search is both incomplete and suboptimal and so won’t necessarily converge to an optimal
solution. Additionally, there is a critical ratio around which using local search becomes extremely expensive:
CS 188, Fall 2018, Note 2 9
NT
WA
SA
NSW
NT
NT
NT
Q
Q
WA
WA
WA
SA
NSW
SA
NSW
SA
NSW
NT
NT
NT
WA
WA
WA
NSW
NSW
NSW业
业
业
业
业
业
W
业-
业
业
业
h =5
h=2
h = 0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 9}
Content: Summary
It’s important to remember that constraint satisfaction problems in general do not have an efficient algorithm
which solves them in polynomial time with respect to the number of variables involved. However, by using
various heuristics, we can often find solutions in an acceptable amount of time:
• Filtering - Filtering handles pruning the domains of unassigned variables ahead of time to prevent
unnecessary backtracking. The two important filtering techniques we’ve covered are forward checking
and arc consistency. • Ordering - Ordering handles selection of which variable or value to assign next to make backtracking
as unlikely as possible.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9 Notes.pdf', 'page': 9}
Content: For variable selection, we learned about a MRV policy and for value selection
we learned about a LCV policy. • Structure - If a CSP is tree-structured or close to tree-structured, we can run the tree-structured CSP
algorithm on it to derive a solution in linear time. Similarly, if a CSP is close to tree structured, we
can use cutset conditioning to transform the CSP into one or more independent tree-structured CSPs
and solve each of these separately. CS 188, Fall 2018, Note 2 10
number of constraints
R=
number of variables
CPU
time
R
critical
ratio

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 0}
Content: Dr. Seemab latif
Lecture 4
8th Oct 2024
A R T I F I C I A L I N T E L L I G E N C E
ArtificialIntelligence
03
+
MachineLearning
DeepLearning
:0

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 1}
Content: Constraint Satisfaction Problems
Slide credits: Pat Virtue, http://ai.berkeley.edu


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 2}
Content: What is Search For?  Assumptions about the world: a single agent, deterministic actions, fully observed 
state, discrete state space
 Planning: sequences of actions
 The path to the goal is the important thing
 Paths have various costs, depths
 Heuristics give problem-specific guidance
 Identification: assignments to variables
 The goal itself is important, not the path
 All paths at the same depth (for some formulations)
 CSPs are specialized for identification problems
LK

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 3}
Content: Constraint Satisfaction Problems
MAP
MAP
COLORING
CONSTRAINT3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 4}
Content: Constraint Satisfaction Problems
 Standard search problems:
 State is a “black box”: arbitrary data structure
 Goal test can be any function over states
 Successor function can also be anything
 Constraint satisfaction problems (CSPs):
 A special subset of search problems
 State is defined by variables Xi with values from a 
domain D (sometimes D depends on i)
 Goal test is a set of constraints specifying allowable 
combinations of values for subsets of variables
 Simple example of a formal representation language
 Allows useful general-purpose algorithms with more 
power than standard search algorithms
二MAP
MAP
COLORING
CONSTRAINTS

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 5}
Content: Example: Formal representation language
SymbolicReasoner
[((<)asnoW V (x)D)A]xA
Cat(x)VMouse(y)VCatch(x,y)
Catch
Catch(x,y)
Cat(Tom);Mouse(Jerry)
(Tom,
Cat(Tom);Mouse(Jerry)
Jerry)
Catch(Tom,Jerry)
(a)Formal languageasknowledgerepresentation and symbolicreasoner

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 6}
Content: CSP Examples
Northern
Territory
Western
Queensland
Australia
South
Australia
NewSouthWales
Victoria
Tasmania

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 7}
Content: Example: Map Coloring
 Variables:
 Domains:
 Constraints: adjacent regions must have different 
colors
 Solutions are assignments satisfying all 
constraints, e.g.:
Implicit:
Explicit:
Northern
Territory
Western
Queensland
Australia
South
Australia
NewSouthWales
Victoria
TasmaniaWA,T
NT,
Q,
NSW, V,
SA,
T{red, green, blue
D{WA=red,
NT=green,
Q=red,
Nsw=green
V=red, SA=blue, T=green}子(WA, NT) E {(red, green), (red, blue), . .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 7}
Content: .}

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 8}
Content: Example: N-Queens
 Formulation:
 Variables:
 Domains:
 Constraints
https://vsit.edu.in/vlab/AI/main%20ckt/simulator/4QueenSolution.htm
业
业
业
业1XXij
N
2,J (Xi), Xib) ∈ {(0, 0),(0, 1),(1,0)}
Vi, j, kWVi, j, k
(Xij,Xkj)
∈ {(0,0),(0,1),(1,0)}
(Xij, Xi+k,j+k) ∈ {(0,0),(0,1),(1,0)}
Vi, j, k
(Xij,Xi+k,j-k) ∈ {(0,0),(0,1),(1,0)}
Vi, j, k

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 9}
Content: Constraint Graphs
NT
WA
SA
NSW
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 10}
Content: Constraint Graphs
 Binary CSP: each constraint relates (at most) two 
variables
 Binary constraint graph: nodes are variables, arcs 
show constraints
 General-purpose CSP algorithms use the graph 
structure to speed up search. E.g., Tasmania is an 
independent subproblem! NT
WA
SA
NSW
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 11}
Content: Example: Cryptarithmetic
 Variables:
 Domains:
 Constraints:
O
F
T
U
W
R
X
X
XT
wO
Wo
+一
T
F
UR
OX1
FT
X2
W R
X3
U[0, 1,2,3,4,5,6,7,8,9}X1
R+
O:
O
10alldiff(F, T, U, W, R, SEND
THE
MONEY

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 12}
Content: Example: Sudoku
 Variables:
 Each (open) square
 Domains:
 {1,2,…,9}
 Constraints:
9-way alldiff for each row
9-way alldiff for each column
9-way alldiff for each region
(or can have a bunch of 
pairwise inequality 
constraints)
8
4
84
1
6
5
1
1
3
8
9
6
8
4
3
2
9
5
1
7
2
7
8
26
2
3

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 13}
Content: Varieties of CSPs and Constraints


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 14}
Content: Varieties of CSPs
 Discrete Variables
 Finite domains
 Size d means O(d
n
) complete assignments
 E.g., Boolean CSPs, including Boolean satisfiability (NPcomplete)
 Infinite domains (integers, strings, etc.)
 E.g., job scheduling, variables are start/end times for each job
 Linear constraints solvable, nonlinear undecidable
 Continuous variables
 E.g., start/end times for Hubble Telescope observations
 Linear constraints solvable in polynomial time by LP methods 
(see cs170 for a bit of this theory)


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 15}
Content: Varieties of Constraints
 Varieties of Constraints
 Unary constraints involve a single variable (equivalent to 
reducing domains), e.g.:
 Binary constraints involve pairs of variables, e.g.:
 Higher-order constraints involve 3 or more variables:
e.g., cryptarithmetic column constraints
 Preferences (soft constraints):
 E.g., red is better than green
 Often representable by a cost for each variable assignment
 Gives constrained optimization problems
 (We’ll ignore these until we get to Bayes’ nets)
D
gre
ee
ee
nV

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 16}
Content: Real-World CSPs
 Assignment problems: e.g., who teaches what class
 Timetabling problems: e.g., which class is offered when and where?  Hardware configuration
 Transportation scheduling
 Factory scheduling
 Circuit layout
 Fault diagnosis
 … lots more!  Many real-world problems involve real-valued variables…
Th
F
M
T
W
DO:

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 17}
Content: Solving CSPs


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 18}
Content: Standard Search Formulation
 Standard search formulation of CSPs
 States defined by the values assigned 
so far (partial assignments)
 Initial state: the empty assignment, {}
 Successor function: assign a value to an 
unassigned variable
 Goal test: the current assignment is 
complete and satisfies all constraints
 We’ll start with the straightforward, 
naïve approach, then improve it
二

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 19}
Content: Search Methods
 What would BFS do?  What would DFS do?  What problems does naïve search have? NT
WA
SA
NSW
V
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 20}
Content: Video of Demo Coloring -- DFS


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 21}
Content: Backtracking Search
X

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 22}
Content: Backtracking Search
 Backtracking search is the basic uninformed algorithm for solving CSPs
 Idea 1: One variable at a time
 Variable assignments are commutative, so fix ordering
 I.e., [WA = red then NT = green] same as [NT = green then WA = red]
 Only need to consider assignments to a single variable at each step
 Idea 2: Check constraints as you go
 I.e. consider only values which do not conflict previous assignments
 Might have to do some computation to check the constraints
 “Incremental goal test”
 Depth-first search with these two improvements
is called backtracking search (not the best name)
 Can solve n-queens for n  25
X

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 23}
Content: Backtracking Example
MX

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 24}
Content: Backtracking Search
 Backtracking = DFS + variable-ordering + fail-on-violation
 What are the choice points? function BACKTRACKING-SEARCH(csp) returns solution/failure
return RECURSIVE-BACKTRACKING({}, csp)
function RECURSIVE-BACKTRACKING(assignment, csp) returns soln/failure
if assignment is complete then return assignment
var SELECT-UNASSIGNED-VARIABLE(VARIABLES[csp], assignment, csp)
for each value in ORDER-DoMAIN-VALUEs(var, assignment, csp) do
if value is consistent with assignment given CoNsTRAINTs[csp] then
add {var = value} to assignment
result  RECURSIVE-BACKTRACKING(assignment, csp)
if result ≠ failure then return result
remove {var = value} from assignment
return failure

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 25}
Content: Video of Demo Coloring – Backtracking


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 26}
Content: Improving Backtracking
 General-purpose ideas give huge gains in speed
 Ordering:
 Which variable should be assigned next?  In what order should its values be tried?  Filtering: Can we detect inevitable failure early?  Structure: Can we exploit the problem structure? 巧

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 27}
Content: Filtering


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 28}
Content:  Filtering: Keep track of domains for unassigned variables and cross off bad options
 Forward checking: Cross off values that violate a constraint when added to the existing 
assignment
Filtering: Forward Checking
WA
SA
NT Q
NSW
V
WA
NT
NSW
SAWA
NT
NSW
SA

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 29}
Content: Video of Demo Coloring – Backtracking with Forward Checking


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 30}
Content: Filtering: Constraint Propagation
 Forward checking propagates information from assigned to unassigned variables, but 
doesn't provide early detection for all failures:
 NT and SA cannot both be blue!  Why didn’t we detect this yet?  Constraint propagation: reason from constraint to constraint
WA SA
NT Q
NSW
V
WA
NT
NSW
SA
TWA
NT
NSW
SA
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 31}
Content: Consistency of A Single Arc
 An arc X  Y is consistent iff for every x in the tail there is some y in the head which 
could be assigned without violating a constraint
 Forward checking: Enforcing consistency of arcs pointing to each new assignment
Delete from the tail! WA SA
NT Q
NSW
V
WA
NT
NSW
V
SA
TWA
NT
NSW
SA
TWA
NT
NSW
SA
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 32}
Content: Arc Consistency of an Entire CSP
 A simple form of propagation makes sure all arcs are consistent:
 Important: If X loses a value, neighbors of X need to be rechecked!  Arc consistency detects failure earlier than forward checking
 Can be run as a preprocessor or after each assignment 
 What’s the downside of enforcing arc consistency? Remember: Delete 
from the tail!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 32}
Content: WA SA
NT Q
NSW
V
WA
NT
NSW
V
SA
TWA
NT
NSW
V
SA
T

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 33}
Content: Enforcing Arc Consistency in a CSP
 Runtime: O(n2d
3
), can be reduced to O(n2d
2
)
 … but detecting all possible future problems is NP-hard – why? [Demo: CSP applet (made available by aispace.org) -- n-queens]
function AC-3( csp) returns the CSP, possibly with reduced domains
inputs: csp, a binary CSP with variables {X1, X2, ..., Xn}
local variables: queue, a queue of arcs, initially all the arcs in csp
while queue is not empty do
(X, X;)←REMOVE-FIRST(queue)
if REMOVE-INCONSISTENT-VALUES(X, X§) then
for each X in NEIGHBORs[X] do
add (Xk, Xi) to queue
function REMOVE-INCONSISTENT-VALUES( X, X§) returns true iff succeeds
removed←—false
for each c in DoMAIN[X] do
if no value y in DoMAIN[X§] allows (,y) to satisfy the constraint X; → X§
then delete c from DoMAIN[Xi]; removed← true
return removed

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 34}
Content: Video of Demo Arc Consistency – CSP Applet – n Queens


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 35}
Content: Limitations of Arc Consistency
 After enforcing arc 
consistency:
 Can have one solution left
 Can have multiple solutions left
 Can have no solutions left (and 
not know it)
 Arc consistency still runs 
inside a backtracking search! What went 
wrong here? 

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 36}
Content: Video of Demo Coloring – Backtracking with Forward Checking –
Complex Graph


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 37}
Content: Video of Demo Coloring – Backtracking with Arc Consistency –
Complex Graph


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 38}
Content: Ordering


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 39}
Content: Ordering: Minimum Remaining Values
 Variable Ordering: Minimum remaining values (MRV):
 Choose the variable with the fewest legal left values in its domain
 Why min rather than max?  Also called “most constrained variable”
 “Fail-fast” ordering
SL
M

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 40}
Content: Ordering: Least Constraining Value
 Value Ordering: Least Constraining Value
 Given a choice of variable, choose the least 
constraining value
 I.e., the one that rules out the fewest values in 
the remaining variables
 Note that it may take some computation to 
determine this! (E.g., rerunning filtering)
 Why least rather than most?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Lec 9.pdf', 'page': 40}
Content:  Combining these ordering ideas makes
1000 queens feasible


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 5
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Reinforcement Learning
In the previous note, we discussed Markov decision processes, which we solved using techniques such as
value iteration and policy iteration to compute the optimal values of states and extract optimal policies. Solving Markov decision processes is an example of offline planning, where agents have full knowledge
of both the transition function and the reward function, all the information they need to precompute optimal
actions in the world encoded by the MDP without ever actually taking any actions. In this note, we’ll discuss
online planning, during which an agent has no prior knowledge of rewards or transitions in the world (still
represented as a MDP). In online planning, an agent must try exploration, during which it performs actions
and receives feedback in the form of the successor states it arrives in and the corresponding rewards it
reaps. The agent uses this feedback to estimate an optimal policy through a process known as reinforcement
learning before using this estimated policy for exploitation, or reward maximization. Let’s start with some basic terminology. At each timestep during online planning, an agent starts in a state
s, then takes an action a and ends up in a successor state s0, attaining some reward r. Each (s,a,s0,r) tuple is
known as a sample.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 0}
Content: Often, an agent continues to take actions and collect samples in succession until arriving
at a terminal state. Such a collection of samples is known as an episode. Agents typically go through many
episodes during exploration in order to collect sufficient data needed for learning. There are two types of reinforcement learning, model-based learning and model-free learning. Modelbased learning attempts to estimate the transition and reward functions with the samples attained during
exploration before using these estimates to solve the MDP normally with value or policy iteration. Modelfree learning, on the other hand, attempts to estimate the values or q-values of states directly, without ever
using any memory to construct a model of the rewards and transitions in the MDP. Model-Based Learning
In model-based learning an agent generates an approximation of the transition function, Tˆ(s,a,s0), by keeping counts of the number of times it arrives in each state s0 after entering each q-state (s,a). The agent can
CS 188, Fall 2018, Note 5 1
Agent
State: s
Actions:a
Reward:r
Environment

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 1}
Content: then generate the the approximate transition function Tˆ upon request by normalizing the counts it has collected - dividing the count for each observed tuple (s,a,s0
) by the sum over the counts for all instances where
the agent was in q-state (s,a). Normalization of counts scales them such that they sum to one, allowing them
to be interpreted as probabilities. Consider the following example MDP with states S = {A,B,C,D,E, x},
with x representing the terminal state, and discount factor g = 1:
Assume we allow our agent to explore the MDP for four episodes under the policy pexplore delineated above
(a directional triangle indicates motion in the direction the triangle points, and a blue squares represents
taking exit as the action of choice), and yield the following results:
We now have a collective 12 samples, 3 from each episode with counts as follows:
s a s0 count
A exit x 1
B east C 2
C east A 1
C east D 3
D exit x 3
E north C 2
Recalling that T(s,a,s0) = P(s0|a,s), we can estimate the transition function with these counts by dividing
the counts for each tuple (s,a,s0) by the total number of times we were in q-state (s,a) and the reward
function directly from the rewards we reaped during exploration:
CS 188, Fall 2018, Note 5 2
A
B
D
D
△
EEpisode 1
Episode 2
B,east, C, -1
B,east, C, -1
C,east, D, -1
C, east, D, -1
D, exit, x, +10
D, exit, x, +10
Episode 3
Episode 4
E, north, C, -1
E, north, C, -1
C, east,
D, -1
C, east,
 A, -1
D, exit,
x, +10
A, exit,
x, -10

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 2}
Content: • Transition Function: Tˆ(s,a,s0
)
– Tˆ(A, exit, x) = #(A,exit,x)
#(A,exit) = 1
1 = 1
– Tˆ(B, east,C) = #(B,east,C)
#(B,east) = 2
2 = 1
– Tˆ(C, east,A) = #(C,east,A)
#(C,east) = 1
4 = 0.25
– Tˆ(C, east,D) = #(C,east,D)
#(C,east) = 3
4 = 0.75
– Tˆ(D, exit, x) = #(D,exit,x)
#(D,exit) = 3
3 = 1
– Tˆ(E,north,C) = #(E,north,C)
#(E,north) = 2
2 = 1
• Reward Function: Rˆ(s,a,s0)
– Rˆ(A, exit, x) = 10
– Rˆ(B, east,C) = 1
– Rˆ(C, east,A) = 1
– Rˆ(C, east,D) = 1
– Rˆ(D, exit, x)=+10
– Rˆ(E,north,C) = 1
By the law of large numbers, as we collect more and more samples by having our agent experience more
episodes, our models of Tˆ and Rˆ will improve, with Tˆ converging towards T and Rˆ acquiring knowledge of
previously undiscovered rewards as we discover new (s,a,s0) tuples. Whenever we see fit, we can end our
agent’s training to generate a policy pexploit by running value or policy iteration with our current models for
Tˆ and Rˆ and use pexploit for exploitation, having our agent traverse the MDP taking actions seeking reward
maximization rather than seeking learning. We’ll soon discuss methods for how to allocate time between
exploration and explotation effectively. Model-based learning is very simple and intuitive yet remarkably
effective, generating Tˆ and Rˆ with nothing more than counting and normalization. However, it can be
expensive to maintain counts for every (s,a,s0) tuple seen, and so in the next section on model-free learning
we’ll develop methods to bypass maintaining counts altogether and avoid the memory overhead required by
model-based learning. Model-Free Learning
Onward to model-free learning! There are several model-free learning algorithms, and we’ll cover three
of them: direct evaluation, temporal difference learning, and Q-learning. Direct evaluation and temporal
difference learning fall under a class of algorithms known as passive reinforcement learning. In passive
reinforcement learning, an agent is given a policy to follow and learns the value of states under that policy
as it experiences episodes, which is exactly what is done by policy evaluation for MDPs when T and R are
known. Q-learning falls under a second class of model-free learning algorithms known as active reinforcement learning, during which the learning agent can use the feedback it receives to iteratively update its
policy while learning until eventually determining the optimal policy after sufficient exploration. Direct Evaluation
The first passive reinforcement learning technique we’ll cover is known as direct evaluation, a method
that’s as boring and simple as the name makes it sound. All direct evaluation does is fix some policy p and
have the agent that’s learning experience several episodes while following p. As the agent collects samples
through these episodes it maintains counts of the total utility obtained from each state and the number of
times it visited each state. At any point, we can compute the estimated value of any state s by dividing the
total utility obtained from s by the number of times s was visited. Let’s run direct evaluation on our example
from earlier, recalling that g = 1.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 2}
Content: CS 188, Fall 2018, Note 5 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 3}
Content: Walking through the first episode, we can see that from state D to termination we acquired a total reward of
10, from state C we acquired a total reward of (1) +10 = 9, and from state B we acquired a total reward
of (1)+(1)+10 = 8. Completing this process yields the total reward across episodes for each state and
the resulting estimated values as follows:
s Total Reward Times Visited Vp (s)
A 10 1 10
B 16 2 8
C 16 4 4
D 30 3 10
E 4 2 2
Though direct evaluation eventually learns state values for each state, it’s often unnecessarily slow to converge because it wastes information about transitions between states. In our example, we computed Vp (E) = 2 and Vp (B) = 8, though based on the feedback we received both
states only have C as a successor state and incur the same reward of 1 when transitioning to C. According
to the Bellman equation, this means that both B and E should have the same value under p. However, of the
4 times our agent was in state C, it transitioned to D and reaped a reward of 10 three times and transitioned
to A and reaped a reward of 10 once. It was purely by chance that the single time it received the 10
reward it started in state E rather than B, but this severely skewed the estimated value for E. With enough
episodes, the values for B and E will converge to their true values, but cases like this cause the process to
take longer than we’d like.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 3}
Content: This issue can be mitigated by choosing to use our second passive reinforcement
learning algorithm, temporal difference learning. CS 188, Fall 2018, Note 5 4
Episode 1
Episode 2
B, east, C, -1
B,east, C, -1
A
C,east, D, -1
C, east, D, -1
D
exit, x, +10
D, exit, x, +10
B
C
D
Episode 3
Episode 4
△
E, north, C, -1
E, north, C, -1
E
C, east,
D, -1
C, east,
A, -1
D
exit,
x, +10
A, exit,
x, -10-10
A
+8
+4
+10
B
C
D
-2
E

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 4}
Content: Temporal Di↵erence Learning
Temporal difference learning (TD learning) uses the idea of learning from every experience, rather than
simply keeping track of total rewards and number of times states are visited and learning at the end as direct
evaluation does. In policy evaluation, we used the system of equations generated by our fixed policy and
the Bellman equation to determine the values of states under that policy (or used iterative updates like with
value iteration). Vp (s) = Â
s0
T(s,p(s),s
0
)[R(s,p(s),s
0
) +gVp (s
0
)]
Each of these equations equates the value of one state to the weighted average over the discounted values of
that state’s successors plus the rewards reaped in transitioning to them. TD learning tries to answer the question of how to compute this weighted average without the weights, cleverly doing so with an exponential
moving average. We begin by initializing 8s, Vp (s) = 0. At each timestep, an agent takes an action p(s)
from a state s, transitions to a state s0, and receives a reward R(s,p(s),s0). We can obtain a sample value by
summing the received reward with the discounted current value of s0 under p:
sample = R(s,p(s),s
0
) +gVp (s
0
)
This sample is a new estimate for Vp (s). The next step is to incorporate this sampled estimate into our
existing model for Vp (s) with the exponential moving average, which adheres to the following update rule:
Vp (s) (1a)Vp (s) +a ·sample
Above, a is a parameter constrained by 0  a  1 known as the learning rate that specifies the weight
we want to assign our existing model for Vp (s), 1a, and the weight we want to assign our new sampled
estimate, a. It’s typical to start out with learning rate of a = 1, accordingly assigning Vp (s) to whatever the
first sample happens to be, and slowly shrinking it towards 0, at which point all subsequent samples will be
zeroed out and stop affecting our model of Vp (s). Let’s stop and analyze the update rule for a minute. Annotating the state of our model at different points in
time by defining Vp
k (s) and samplek as the estimated value of state s after the kth update and the kth sample
respectively, we can reexpress our update rule:
Vp
k (s) (1a)Vpk1(s) +a ·samplek
This recursive definition for Vp
k (s) happens to be very interesting to expand:
Vp
k (s) (1a)Vpk1(s) +a ·samplek
Vp
k (s) (1a)[(1a)Vpk2(s) +a ·samplek1] +a ·samplek
Vp
k (s) (1a)
2
Vp
k2(s)+(1a)·a ·samplek1 +a ·samplek
. .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 4}
Content: . Vp
k (s) (1a)
k
Vp
0 (s) +a · [(1a)
k1 ·sample1 +...+ (1a)·samplek1 +samplek]
Vp
k (s) a · [(1a)
k1 ·sample1 +...+ (1a)·samplek1 +samplek]
Because 0  (1  a)  1, as we raise the quantity (1  a) to increasingly larger powers, it grows closer
and closer to 0. By the update rule expansion we derived, this means that older samples are given exponentially less weight, exactly what we want since these older samples are computed using older (and hence
worse) versions of our model for Vp (s)! This is the beauty of temporal difference learning - with a single
straightfoward update rule, we are able to:
CS 188, Fall 2018, Note 5 5


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 5}
Content: • learn at every timestep, hence using information about state transitions as we get them since we’re
using iteratively updating versions ofVp (s0)in our samples rather than waiting until the end to perform
any computation. • give exponentially less weight to older, potentially less accurate samples.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 5}
Content: • converge to learning true state values much faster with fewer episodes than direct evaluation. Q-Learning
Both direct evaluation and TD learning will eventually learn the true value of all states under the policy
they follow. However, they both have a major inherent issue - we want to find an optimal policy for our
agent, which requires knowledge of the q-values of states. To compute q-values from the values we have,
we require a transition function and reward function as dictated by the Bellman equation. Q⇤(s,a) = Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +gV⇤(s
0
)]
Resultingly, TD learning or direct evaluation are typically used in tandem with some model-based learning
to acquire estimates of T and R in order to effectively update the policy followed by the learning agent. This
became avoidable by a revolutionary new idea known as Q-learning, which proposed learning the q-values
of states directly, bypassing the need to ever know any values, transition functions, or reward functions. As a result, Q-learning is entirely model-free. Q-learning uses the following update rule to perform what’s
known as q-value iteration:
Qk+1(s,a) Â
s0
T(s,a,s
0
)[R(s,a,s
0
) +g max
a0 Qk(s
0
,a0)]
Note that this update is only a slight modification over the update rule for value iteration. Indeed, the only
real difference is that the position of the max operator over actions has been changed since we select an
action before transitioning when we’re in a state, but we transition before selecting a new action when we’re
in a q-state. With this new update rule under our belt, Q-learning is derived essentially the same way as TD learning, by
acquiring q-value samples:
sample = R(s,a,s
0
) +g max
a0 Q(s
0
,a0)
and incoporating them into an exponential moving average. Q(s,a) (1a)Q(s,a) +a ·sample
As long as we spend enough time in exploration and decrease the learning rate a at an appropriate pace, Qlearning learns the optimal q-values for every q-state. This is what makes Q-learning so revolutionary - while
TD learning and direct evaluation learn the values of states under a policy by following the policy before
determining policy optimality via other techniques, Q-learning can learn the optimal policy directly even by
taking suboptimal or random actions. This is called off-policy learning (contrary to direct evaluation and
TD learning, which are examples of on-policy learning). Approximate Q-Learning
Q-learning is an incredible learning technique that continues to sit at the center of developments in the field
of reinforcement learning. Yet, it still has some room for improvement. As it stands, Q-learning just stores
CS 188, Fall 2018, Note 5 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 6}
Content: all q-values for states in tabular form, which is not particularly efficient given that most applications of
reinforcement learning have several thousands or even millions of states. This means we can’t visit all states
during training and can’t store all q-values even if we could for lack of memory. Figure 1 Figure 2 Figure 3
Above, if Pacman learned that Figure 1 is unfavorable after running vanilla Q-learning, it would still have
no idea that Figure 2 or even Figure 3 are unfavorable as well. Approximate Q-learning tries to account
for this by learning about a few general situations and extrapolating to many similar situations. The key to
generalizing learning experiences is the feature-based representation of states, which represents each state
as a vector known as a feature vector. For example, a feature vector for Pacman may encode
• the distance to the closest ghost. • the distance to the closest food pellet. • the number of ghosts.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 6}
Content: • is Pacman trapped? 0 or 1
With feature vectors, we can treat values of states and q-states as linear value functions:
V(s) = w1 · f1(s) +w2 · f2(s) +...+wn · fn(s) = ~w· ~f(s)
Q(s,a) = w1 · f1(s,a) +w2 · f2(s,a) +...+wn · fn(s,a) = ~w· ~f(s,a)
where ~f(s) = ⇥
f1(s) f2(s) ... fn(s)
⇤T and ~f(s,a) = ⇥
f1(s,a) f2(s,a) ... fn(s,a)
⇤T represent the
feature vectors for state s and q-state (s,a) respectively and ~w = ⇥
w1 w2 ... wn
⇤
represents a weight
vector. Defining di f f erence as
di f f erence = [R(s,a,s
0
) +g max
a0 Q(s
0
,a0)]Q(s,a)
approximate Q-learning works almost identically to Q-learning, using the following update rule:
wi wi +a · di f f erence · fi(s,a)
Rather than storing Q-values for each and every state, with approximate Q-learning we only need to store a
single weight vector and can compute Q-values on-demand as needed. As a result, this gives us not only a
more generalized version of Q-learning, but a significantly more memory-efficient one as well. As a final note on Q-learning, we can reexpress the update rule for exact Q-learning using di f f erence as
follows:
Q(s,a) Q(s,a) +a · di f f erence
CS 188, Fall 2018, Note 5 7
11

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 7}
Content: This second notation gives us a slightly different but equally valuable interpration of the update: it’s computing the difference between the sampled estimated and the current model of Q(s,a), and shifting the model
in the direction of the estimate with the magnitude of the shift being proportional to the magnitude of the
difference. Exploration and Exploitation
We’ve now covered several different methods for an agent to learn an optimal policy, and harped on the
fact that "sufficient exploration" is necessary for this without really elaborating on what’s really meant
by "sufficient". In the upcoming two sections, we’ll discuss two methods for distributing time between
exploration and exploitation: e-greedy policies and exploration functions. e-Greedy Policies
Agents following an e-greedy policy define some probability 0  e  1, and act randomly and explore with
probability e. Accordingly, they follow their current established policy and exploit with probability (1e).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 7}
Content: This is a very simple policy to implement, yet can still be quite difficult to handle. If a large value for e is
selected, then even after learning the optimal policy, the agent will still behave mostly randomly. Similarly,
selecting a small value for e means the agent will explore infrequently, leading Q-learning (or any other
selected learning algorithm) to learn the optimal policy very slowly. To get around this, e must be manually
tuned and lowered over time to see results. Exploration Functions
This issue of manually tuning e is avoided by exploration functions, which use a modified q-value iteration
update to give some preference to visiting less-visited states. The modified update is as follows:
Q(s,a) (1a)Q(s,a) +a · [R(s,a,s
0
) +g max
a0 f(s
0
,a0)]
where f denotes an exploration function. There exists some degree of flexibility in designing an exploration
function, but a common choice is to use
f(s,a) = Q(s,a) + k
N(s,a)
with k being some predetermined value, and N(s,a) denoting the number of times q-state (s,a) has been
visited. Agents in a state s always select the action that has the highest f(s,a) from each state, and hence
never have to make a probabilistic decision between exploration and exploitation. Instead, exploration is
automatically encoded by the exploration function, since the term k
N(s,a) can give enough of a "bonus" to
some infrequently-taken action such that it is selected over actions with higher q-values. As time goes on
and states are visited more frequently, this bonus decreases towards 0 for each state and f(s,a) regresses
towards Q(s,a), making exploitation more and more exclusive. CS 188, Fall 2018, Note 5 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 8}
Content: Summary
It’s very important to remember that reinforcement learning has an underlying MDP, and the goal of reinforcement learning is to solve this MDP by deriving an optimal policy. The difference between using
reinforcement learning and using methods like value iteration and policy iteration is the lack of knowledge
of the transition function T and the reward function R for the underlying MDP. As a result, agents must
learn the optimal policy through online trial-by-error rather than pure offline computation. There are many
ways to do this:
• Model-based learning - Runs computation to estimate the values of the transition function T and the
reward function R and uses MDP-solving methods like value or policy iteration with these estimates. • Model-free learning - Avoids estimation of T and R, instead using other methods to directly estimate
the values or q-values of states. – Direct evaluation - follows a policy p and simply counts total rewards reaped from each state
and the total number of times each state is visited. If enough samples are taken, this converges to
the true values of states under p, albeit being slow and wasting information about the transitions
between states. – Temporal difference learning - follows a policy p and uses an exponential moving average with
sampled values until convergence to the true values of states under p. TD learning and direct
evaluation are examples of on-policy learning, which learn the values for a specific policy before
deciding whether that policy is suboptimal and needs to be updated. – Q-Learning - learns the optimal policy directly through trial and error with q-value iteration
updates. This an example of off-policy learning, which learns an optimal policy even when
taking suboptimal actions. – Approximate Q-Learning - does the same thing as Q-learning but uses a feature-based representation for states to generalize learning.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes A.pdf', 'page': 8}
Content: CS 188, Fall 2018, Note 5 9


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 7
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Decision Networks
In the third note, we learned about game trees and algorithms such as minimax and expectimax which we
used to determine optimal actions that maximized our expected utility. Then in the sixth note, we discussed
Bayes’ nets and how we can use evidence we know to run probabilistic inference to make predictions. Now
we’ll discuss a combination of both Bayes’ nets and expectimax known as a decision network that we can
use to model the effect of various actions on utilities based on an overarching graphical probabilistic model. Let’s dive right in with the anatomy of a decision network:
• Chance nodes - Chance nodes in a decision network behave identically to Bayes’ nets. Each outcome
in a chance node has an associated probability, which can be determined by running inference on the
underlying Bayes’ net it belongs to. We’ll represent these with ovals. • Action nodes - Action nodes are nodes that we have complete control over; they’re nodes representing
a choice between any of a number of actions which we have the power to choose from. We’ll represent
action nodes with rectangles. • Utility nodes - Utility nodes are children of some combination of action and chance nodes. They
output a utility based on the values taken on by their parents, and are represented as diamonds in our
decision networks. Consider a situation when you’re deciding whether or not to take an umbrella when you’re leaving for class
in the morning, and you know there’s a forecasted 30% chance of rain.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 0}
Content: Should you take the umbrella? If
there was a 80% chance of rain, would your answer change? This situation is ideal for modeling with a
decision network, and we do it as follows:
CS 188, Fall 2018, Note 7 1


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 1}
Content: As we’ve done throughout this course with the various modeling techniques and algorithms we’ve discussed,
our goal with decision networks is again to select the action which yields the maximum expected utility
(MEU). This can be done with a fairly straightforward and intuitive procedure:
• Start by instantiating all evidence that’s known, and run inference to calculate the posterior probabilities of all chance node parents of the utility node into which the action node feeds. • Go through each possible action and compute the expected utility of taking that action given the
posterior probabilities computed in the previous step. The expected utility of taking an action a given
evidence e and n chance nodes is computed with the following formula:
EU(a|e) = Âx1,...,xnP(x1,...,xn|e)U(a, x1,...,xn)
where each xi represents a value that the i
th chance node can take on. We simply take a weighted
sum over the utilities of each outcome under our given action with weights corresponding to the
probabilities of each outcome.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 1}
Content: • Finally, select the action which yielded the highest utility to get the MEU. Let’s see how this actually looks by calculating the optimal action (should we leave or take our umbrella) for
our weather example, using both the conditional probability table for weather given a bad weather forecast
(forecast is our evidence variable) and the utility table given our action and the weather:
CS 188, Fall 2018, Note 7 2
Umbrella
Weather
Forecast

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 2}
Content: Note that we have omitted the inference computation for the posterior probabilities P(W|F = bad), but we
could compute these using any of the inference algorithms we discussed for Bayes Nets. Instead, here we
simply assume the above table of posterior probabilities for P(W|F = bad) as given. Going through both
our actions and computing expected utilities yields:
EU(leave|bad) = ÂwP(w|bad)U(leave,w)
= 0.34 · 100+0.66 · 0 = 34
EU(take|bad) = ÂwP(w|bad)U(take,w)
= 0.34 · 20+0.66 · 70 = 53
All that’s left to do is take the maximum over these computed utilities to determine the MEU:
MEU(F = bad) = maxa EU(a|bad) = 53
The action that yields the maximum expected utility is take, and so this is the action recommended to us
by the decision network. More formally, the action that yields the MEU can be determined by taking the
argmax over expected utilities. Outcome Trees
We mentioned at the start of this note that decision networks involved some expectimax-esque elements, so
let’s discuss what exactly that means. We can unravel the selection of an action corresponding to the one
that maximizes expected utility in a decision network as an outcome tree. Our weather forecast example
from above unravels into the following outcome tree:
CS 188, Fall 2018, Note 7 3
A
W
U(A,W)
Umbrella
leave
sun
100
leave
rain
0
take
sun
20
take
rain
70
Weather
W
P(W|F=bad)
sun
0.34
rain
0.66
Forecast
peq=
?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 3}
Content: The root node at the top is a maximizer node, just like in expectimax, and is controlled by us. We select
an action, which takes us to the next level in the tree, controlled by chance nodes. At this level, chance
nodes resolve to different utility nodes at the final level with probabilities corresponding to the posterior
probabilities derived from probabilistic inference run on the underlying Bayes’ net. What exactly makes
this different from vanilla expectimax? The only real difference is that for outcome trees we annotate our
nodes with what we know at any given moment (inside the curly braces). The Value of Perfect Information
In everything we’ve covered up to this point, we’ve generally always assumed that our agent has all the
information it needs for a particular problem and/or has no way to acquire new information. In practice, this
is hardly the case, and one of the most important parts of decision making is knowing whether or not it’s
worth gathering more evidence to help decide which action to take. Observing new evidence almost always
has some cost, whether it be in terms of time, money, or some other medium. In this section, we’ll talk
about a very important concept - the value of perfect information (VPI) - which mathematically quantifies
the amount an agent’s maximum expected utility is expected to increase if it observes some new evidence. We can compare the VPI of learning some new information with the cost associated with observing that
information to make decisions about whether or not it’s worthwhile to observe. General Formula
Rather than simply presenting the formula for computing the value of perfect information for new evidence,
let’s walk through an intuitive derivation.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 3}
Content: We know from our above definition that the value of perfect
information is the amount our maximum expected utility is expected to increase if we decide to observe new
evidence. We know our current maximum utility given our current evidence e:
MEU(e) = maxa ÂsP(s|e)U(s,a)
Additionally, we know that if we observed some new evidence e0 before acting, the maximum expected
utility of our action at that point would become
MEU(e, e0) = maxa ÂsP(s|e, e0)U(s,a)
CS 188, Fall 2018, Note 7 4
{b}
leave
take
w|{b}
w|{b}
rain
rain
sun
sun
U(t,s)
U(t,r)
U(l,s)
U(l,r)

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 4}
Content: However, note that we don’t know what new evidence we’ll get. For example, if we didn’t know the weather
forecast beforehand and chose to observe it, the forecast we observe might be either good or bad. Because
we don’t know what what new evidence e0 we’ll get, we must represent it as a random variable E0. How
do we represent the new MEU we’ll get if we choose to observe a new variable if we don’t know what the
evidence gained from observation will tell us? The answer is to compute the expected value of the maximum
expected utility which, while being a mouthful, is the natural way to go:
MEU(e,E0) = Â
e0
P(e0|e)MEU(e, e0)
Observing a new evidence variable yields a different MEU with probabilities corresponding to the probabilities of observing each value for the evidence variable, and so by computing MEU(e,E0
) as above, we
compute what we expect our new MEU will be if we choose to observe new evidence. We’re just about done
now - returning to our definition for VPI, we want to find the amount our MEU is expected to increase if we
choose to observe new evidence. We know our current MEU and the expected value of the new MEU if we
choose to observe, so the expected MEU increase is simply the difference of these two terms! Indeed,
V PI(E0|e) = MEU(e,E0) MEU(e)
where we can read V PI(E0|e) as "the value of observing new evidence E’ given our current evidence e". Let’s work our way through an example by revisiting our weather scenario one last time:
If we don’t observe any evidence, then our maximum expected utility can be computed as follows:
MEU(?) = maxa EU(a)
= maxa ÂwP(w)U(a,w)
= max{0.7 · 100+0.3 · 0,0.7 · 20+0.3 · 70}
= max{70,35}
= 70
Note that the convention when we have no evidence is to write MEU(?), denoting that our evidence is the
empty set. Now let’s say that we’re deciding whether or not to observe the weather forecast. We’ve already
computed that MEU(F = bad) = 53, and let’s assume that running an identical computation for F = good
CS 188, Fall 2018, Note 7 5
A
W
U
Umbrella
leave
sun
100
U
leave
rain
0
take
sun
20
Weather
take
rain
70
w
P(W)
F
P(F)
sun
0.7
good
0.59
rain
0.3
Forecast
bad
0.41

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 5}
Content: yields MEU(F = good) = 95. We’re now ready to compute MEU(e,E0
):
MEU(e,E0) = MEU(F)
= Â
e0
P(e0|e)MEU(e, e0)
= Â
f
P(F = f)MEU(F = f)
= P(F = good)MEU(F = good) +P(F = bad)MEU(F = bad)
= 0.59 · 95+0.41 · 53
= 77.78
Hence we conclude V PI(F) = MEU(F) MEU(?) = 77.7870 = 7.78 . Properties of VPI
The value of perfect information has several very important properties, namely:
• Nonnegativity. 8E0, e V PI(E0|e)  0
Observing new information always allows you to make a more informed decision, and so your maximum expected utility can only increase (or stay the same if the information is irrelevant for the
decision you must make). • Nonadditivity.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes B.pdf', 'page': 5}
Content: V PI(Ej,Ek|e) 6= V PI(Ej|e) +V PI(Ek|e) in general. This is probably the trickiest of the three properties to understand intuitively. It’s true because generally observing some new evidence Ej might change how much we care about Ek; therefore we
can’t simply add the VPI of observing Ej to the VPI of observing Ek to get the VPI of observing
both of them. Rather, the VPI of observing two new evidence variables is equivalent to observing
one, incorporating it into our current evidence, then observing the other. This is encapsulated by the
order-independence property of VPI, described more below. • Order-independence. V PI(Ej,Ek|e) = V PI(Ej|e) +V PI(Ek|e,Ej) = V PI(Ek|e) +V PI(Ej|e,Ek)
Observing multiple new evidences yields the same gain in maximum expected utility regardless of the
order of observation. This should be a fairly straightforward assumption - because we don’t actually
take any action until after observing any new evidence variables, it doesn’t actually matter whether
we observe the new evidence variables together or in some arbitrary sequential order. CS 188, Fall 2018, Note 7 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2018 Note 9
These lecture notes are heavily based on notes originally written by Nikhil Sharma. Machine Learning
In the previous few notes of this course, we’ve learned about various types of models that help us reason
under uncertainty. Until now, we’ve assumed that the probabilistic models we’ve worked with can be taken
for granted, and the methods by which the underlying probability tables we worked with were generated have
been abstracted away. We’ll begin to break down this abstraction barrier as we delve into our discussion
of machine learning, a broad field of computer science that deals with constructing and/or learning the
parameters of a specified model given some data. There are many machine learning algorithms which deal with many different types of problems and different types of data, classified according to the tasks they hope to accomplish and the types of data that they
work with. Two primary subgroups of machine learning algorithms are supervised learning algorithms
and unsupervised learning algorithms. Supervised learning algorithms infer a relationship between input
data and corresponding output data in order to predict outputs for new, previously unseen input data.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 0}
Content: Unsupervised learning algorithms, on the other hand, have input data that doesn’t have any corresponding output
data and so deal with recognizing inherent structure between or within datapoints and grouping and/or processing them accordingly. In this class, the algorithms we’ll discuss will be limited to supervised learning
tasks. (a) Training (b) Validation (c) Testing
Once you have a dataset that you’re ready to learn with, the machine learning process usually involves
splitting your dataset into three distinct subsets. The first, training data, is used to actually generate a
model mapping inputs to outputs. Then, validation data (also known as hold-out or development data)
is used to measure your model’s performance by making predictions on inputs and generating an accuracy
score. If your model doesn’t perform as well as you’d like it to, it’s always okay to go back and train again,
either by adjusting special model-specific values called hyperparameters or by using a different learning
algorithm altogether until you’re satisfied with your results. Finally, use your model to make predictions on
the third and final subset of your data, the test set. The test set is the portion of your data that’s never seen by
CS 188, Fall 2018, Note 9 1
Apple
CarPractice
ExamFinal
Exam! P

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 1}
Content: your agent until the very end of development, and is the equivalent of a "final exam" to gauge performance
on real-world data. Naive Bayes
We’ll further motivate our discussion of machine learning with a concrete example of a machine learning
algorithm. Let’s consider the common problem of building an email spam filter which sorts messages into
spam (unwanted email) or ham (wanted email). Such a problem is called a classification problem – given
various datapoints (in this case, each email is a datapoint), our goal is to group them into one of two or more
classes. For classification problems, we’re given a training set of datapoints along with their corresponding
labels, which are typically one of a few discrete values. As we’ve discussed, our goal will be to use this
training data (emails, and a spam/ham label for each one) to learn some sort of relationship that we can use
to make predictions on new and previously unseen datapoints. In this section we’ll describe how to construct
a specific type of model for solving classification problems known as a Naive Bayes Classifier. To train a model to classify emails as spam or ham, we need some training data consisting of preclassified
emails that we can learn from. However, emails are simply strings of text, and in order to learn anything
useful, we need to extract certain attributes from each of them known as features. Features can be anything
ranging from specific word counts to text patterns (e.g. whether words are in all caps or not) to pretty much
any other attribute of the data that you can imagine. The specific features extracted for training are often
dependent on the specific problem you’re trying to solve and which features you decide to select can often
impact the performance of your model dramatically.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 1}
Content: Deciding which features to utilize is known as feature
engineering and is fundamental to machine learning, but for the purposes of this course you can assume
you’ll always be given the extracted features for any given dataset. Now let’s say you have a dictionary of n words, and from each email you extract a feature vector F 2 Rn
where the i
th entry in F is a random variable Fi which can take on a value of either a 0 or a 1 depending on
whether the i
th word in your dictionary appears in the email under consideration. For example, if F200 is the
feature for the word free, we will have F200 = 1 if free appears in the email, and 0 if it does not. With these
definitions, we can define more concretely how to predict whether or not an email is spam or ham – if we
can generate a joint probability table between each Fi and and the label Y, we can compute the probability
any email under consideration is spam or ham given it’s feature vector. Specifically, we can compute both
P(Y = spam|F1 = f1,...,Fn = fn)
and
P(Y = ham|F1 = f1,...,Fn = fn)
and simply label the email depending on which of the two probabilities is higher. Unfortunately, since
we have n features and 1 label, each of which can take on 2 distinct values, the joint probability table
CS 188, Fall 2018, Note 9 2
SPAM
SPAM

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 2}
Content: corresponding to this distribution mandates a table size that’s exponential in n with 2n+1 entries - very
impractical! This problem is solved by modeling the joint probability table with a Bayes’ net, making the
critical simplifying assumption that each feature Fi is independent of all other features given the class label. This is a very strong modeling assumption (and the reason that Naive Bayes is called naive), but it simplifies
inference and usually works well in practice. It leads to the following Bayes’ net to represent our desired
joint probability distribution
Note that the rules of d-separation delineated earlier in the course make it immediately clear that in this
Bayes’ net each Fi is conditionally independent of all the others, given Y. Now we have one table for
P(Y) with 2 entries, and n tables for each P(Fi | Y) each with 22 = 4 entries for a total of 4n + 2 entries -
linear in n! This simplifying assumption highlights the tradeoff that arises from the concept of statistical
efficiency; we sometimes need to compromise our model’s complexity in order to stay within the limits of
our computational resources. Indeed, in cases where the number of features is sufficiently low, it’s common to make more assumptions
about relationships between features to generate a better model (corresponding to adding edges to your
Bayes’ net). With this model we’ve adopted, making predictions for unknown datapoints amounts to running
inference on our Bayes’ net. We have observed values for F1,···Fn, and want to choose the value of Y that
has the highest probability conditioned on these featuers:
prediction(f1,··· fn) = argmax
y
P(Y = y|F1 = f1,...FN = fn)
= argmax
y
P(Y = y,F1 = f1,...FN = fn)
= argmax
y
P(Y = y)
n
’
i=1
P(Fi = fi|Y = y)
where the first step is because the highest probability class will be the same in the normalized or unnormalized distribution, and the second comes directly from the Naive Bayes’ independence assumption that
features are independent given the class label (as seen in the graphical model structure). Generalizing away from a spam filter, assume now that there are k class labels (possible values for Y). Additionally, after noting that our desired probabilities - the probability of each label yi given our features,
P(Y = yi|F1 = f1,...,Fn = fn) - is proportional to the joint P(Y = yi,F1 = f1,...,Fn = fn), we can compute:
P(Y,F1 = f1,...,Fn = fn) =
2
6
6
6
4
P(Y = y1,F1 = f1,...,Fn = fn)
P(Y = y2,F1 = f1,...,Fn = fn)
. .

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 2}
Content: . P(Y = yk,F1 = f1,...,Fn = fn)
3
7
7
7
5 =
2
6
6
6
4
P(Y = y1)’i P(Fi = fi|Y = y1)
P(Y = y2)’i P(Fi = fi|Y = y2)
. . . P(Y = yk)’i P(Fi = fi|Y = yk)
3
7
7
7
5
CS 188, Fall 2018, Note 9 3
Y
F
F
F
1
2
n

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 3}
Content: Our prediction for class label corresponding to the feature vector F is simply the label corresponding to the
maximum value in the above computed vector:
prediction(F) = argmax
yi
P(Y = yi)’
j
P(Fj = fj|Y = yi)
We’ve now learned the basic theory behind the modeling assumptions of the Naive Bayes’ classifier and
how to make predictions with one, but have yet to touch on how exactly we learn the conditional probability
tables used in our Bayes’ net from the input data. This will have to wait for our next topic of discussion,
parameter estimation.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 3}
Content: Parameter Estimation
Assume you have a set of N sample points or observations, x1,..., xN, and you believe that this data was
drawn from a distribution parametrized by an unknown value q. In other words, you believe that the
probability Pq (xi) of each of your observations is a function of q. For example, we could be flipping a coin
which has probability q of coming up heads. How can you "learn" the most likely value of q given your sample? For example, if we have 10 coin flips,
and saw that 7 of them were heads, what value should we choose for q? One answer to this question is to
infer that q is equal to the value that maximizes the probability of having selected your sample x1,..., xN
from your assumed probability distribution. A frequently used and fundamental method in machine learning known as maximum likelihood estimation (MLE) does exactly this. Maximum likelihood estimation
typically makes the following simplifying assumptions:
• Each sample is drawn from the same distribution. In other words, each xi is identically distributed. In our coin flipping example, each coin flip has the same chance, q, of coming up heads. • Each sample xi is conditionally independent of the others, given the parameters for our distribution. This is a strong assumption, but as we’ll see greatly helps simplify the problem of maximum likelihood
estimation and generally works well in practice. In the coin flipping example, the outcome of one flip
doesn’t affect any of the others. • All possible values of q are equally likely before we’ve seen any data (this is known as a uniform
prior). The first two assumptions above are often referred to as independent, identically distributed (i.i.d.). Let’s now define the likelihood L (q) of our sample, a function which represents the probability of having
drawn our sample from our distribution. For a fixed sample x1, xN, the likelihood is just a function of q:
L (q) = Pq (x1,..., xN)
Using our simplifying assumption that the samples xi are i.i.d., the likelihood function can be reexpressed
as follows:
L (q) =
N
’
i=1
Pq (xi)
How can we find the value of q that maximizes this function? This will be the value of q that best explains
the data we saw. Recall from calculus that at points where a function’s maxima and minima are realized, its
CS 188, Fall 2018, Note 9 4


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 4}
Content: first derivative with respect to each of its inputs (also known as the function’s gradient) must be equal to
zero. Hence, the maximum likelihood estimate for q is a value that satisfies the following equation:
∂
∂ q L (q) = 0
Let’s go through an example to make this concept more concrete. Say you have a bag filled with red and
blue balls and don’t know how many of each there are. You draw samples by taking a ball out of the bag,
noting the color, then putting the ball back in (sampling with replacement). Drawing a sample of three balls
from this bag yields red, red, blue. This seems to imply that we should infer that 2
3 of the balls in the bag
are red and 1
3 of the balls are blue. We’ll assume that each ball being taken out of the bag will be red with
probability q and blue with probability 1q, for some value q that we want to estimate (this is known as a
Bernoulli distribution):
Pq (xi) = (
q xi = red
(1q) xi = blue
The likelihood of our sample is then:
L (q) =
3
’
i=1
Pq (xi) = Pq (x1 = red)Pq (x2 = red)Pq (x2 = blue) = q2 ·(1q)
The final step is to set the derivative of the likelihood to 0 and solve for q:
∂
∂ q L (q) = ∂∂ q q2 ·(1q) = q(23q) = 0
Solving this equation for q yields q = 2
3 , which intuitively makes sense! (There’s a second solution, too,
q = 0 – but this corresponds to a minimum of the likelihood function, as L (0) = 0 < L ( 2
3 ) = 427 .)
Maximum Likelihood for Naive Bayes
Let’s now return to the problem of inferring conditional probability tables for our spam classifier, beginning
with a recap of variables we know:
• n - the number of words in our dictionary. • N - the number of observations (emails) you have for training. For our upcoming discussion, let’s also
define Nh as the number of training samples labeled as ham and Ns as the number of training samples
labeled as spam. Note Nh +Ns = N. • Fi - a random variable which is 1 if the i
th dictionary word is present in an email under consideration,
and 0 otherwise. • Y - a random variable that’s either spam or ham depending on the label of the corresponding email. • f
(j)
i - this references the resolved value of the random variable Fi in the j
th item in the training set. In
other words, each f
(j)
i is a 1 if word i appeared in j
th email under consideration and 0 otherwise. This
is the first time we’re seeing this notation, but it’ll come in handy in the upcoming derivation.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 4}
Content: Disclaimer: Feel free to skip the following mathematical derivation. For CS 188, you’re only required to
know the result of the derivation summarized in the paragraph at the end of this section. CS 188, Fall 2018, Note 9 5


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 5}
Content: Now within each conditional probability table P(Fi|Y), note that we have two distinct Bernoulli distributions:
P(Fi|Y = ham) and P(Fi|Y = spam). For simplicity, let’s specifically consider P(Fi|Y = ham) and try to find
the maximum likelihood estimate for a parameter q = P(Fi = 1|Y = ham) i.e. the probability that the i
th
word in our dictionary appears in a ham email. Since we have Nh ham emails in our training set, we have
Nh observations of whether or not word i appeared in a ham email. Because our model assumes a Bernoulli
distribution for the appearance of each word given its label, we can formulate our likelihood function as
follows:
L (q) =
Nh
’
j=1
P(Fi = f
(j)
i |Y = ham) =
Nh
’
j=1
q f
(j)
i (1q)
1f
(j)
i
The second step comes from a small mathematical trick: if f
(j)
i = 1 then
P(Fi = f
(j)
i |Y = ham) = q1
(1q)
0 = q
and similarly if f
(j)
i = 0 then
P(Fi = f
(j)
i |Y = ham) = q0
(1q)
1 = (1q)
In order to compute the maximum likelihood estimate for q, recall that the next step is to compute the
derivative of L (q) and set it equal to 0. Attempting this proves quite difficult, as it’s no simple task to isolate
and solve for q.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 5}
Content: Instead, we’ll employ a trick that’s very common in maximum likelihood derivations, and
that’s to instead find the value of q the maximizes the log of the likelihood function. Because log(x) is a
strictly increasing function (sometimes referred to as a monotonic transformation), finding a value that
maximizes logL (q) will also maximize L (q). The expansion of logL (q) is below:
logL (q) = log✓ Nh
’
j=1
q f
(j)
i (1q)
1f
(j)
i
◆
=
Nh
Â
j=1
logq f
(j)
i (1q)
1f
(j)
i

=
Nh
Â
j=1
logq f
(j)
i

+
Nh
Â
j=1
log(1q)
1f
(j)
i )

= log(q)
Nh
Â
j=1
f
(j)
i +log(1q)
Nh
Â
j=1
(1 f
(j)
i )
Note that in the above derivation, we’ve used the properties of the log function that log(ac) = c · log(a) and
log(ab) = log(a) +log(b). Now we set the derivative of the log of the likelihood function to 0 and solve for
q:
CS 188, Fall 2018, Note 9 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 6}
Content: ∂
∂ q ✓
log(q)
Nh
Â
j=1
f
(j)
i +log(1q)
Nh
Â
j=1
(1 f
(j)
i )
◆
= 0
1
q
Nh
Â
j=1
f
(j)
i  1
(1q)
Nh
Â
j=1
(1 f
(j)
i ) = 0
1
q
Nh
Â
j=1
f
(j)
i = 1
(1q)
Nh
Â
j=1
(1 f
(j)
i )
(1q)
Nh
Â
j=1
f
(j)
i = q
Nh
Â
j=1
(1 f
(j)
i )
Nh
Â
j=1
f
(j)
i q
Nh
Â
j=1
f
(j)
i = q
Nh
Â
j=1
1q
Nh
Â
j=1
f
(j)
i
Nh
Â
j=1
f
(j)
i = q ·Nh
q = 1
Nh
Nh
Â
j=1
f
(j)
i
We’ve arrived at a remarkably simple final result! According to our formula above, the maximum likelihood
estimate for q (which, remember, is the probability that P(Fi = 1|Y = ham)) corresponds to counting the
number of ham emails in which word i appears and dividing it by the total number of ham emails. You
may think this was a lot of work for an intuitive result (and it was), but the derivation and techniques
will be useful for more complex distributions than the simple Bernoulli distribution we are using for each
feature here. To summarize, in this Naive Bayes model with Bernoulli feature distributions, within any given
class the maximum likelihood estimate for the probability of any outcome corresponds to the count for the
outcome divided by the total number of samples for the given class. The above derivation can be generalized
to cases where we have more than two classes and more than two outcomes for each feature, though this
derivation is not provided here. Smoothing
Though maximum likelihood estimation is a very powerful method for parameter estimation, bad training
data can often lead to unfortunate consequences. For example, if every time the word “minute” appears in
an email in our training set, that email is classified as spam, our trained model will learn that
P(Fminute = 1|Y = ham) = 0
Hence in an unseen email, if the word minute ever shows up, P(Y = ham)’i P(Fi|Y = ham) = 0, and so
your model will never classify any email containing the word minute as ham.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 6}
Content: This is a classic example
of overfitting, or building a model that doesn’t generalize well to previously unseen data. Just because a
specific word didn’t appear in an email in your training data, that doesn’t mean that it won’t appear in an
email in your test data or in the real world. Overfitting with Naive Bayes’ classifiers can be mitigated by
Laplace smoothing. Conceptually, Laplace smoothing with strength k assumes having seen k extra of each
outcome. Hence if for a given sample your maximum likelihood estimate for an outcome x that can take on
CS 188, Fall 2018, Note 9 7


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 7}
Content: |X| different values from a sample of size N is
PMLE(x) = count(x)
N
then the Laplace estimate with strength k is
PLAP,k(x) = count(x) +k
N +k|X|
What does this equation say? We’ve made the assumption of seeing k additional instances of each outcome,
and so act as if we’ve seen count(x) +k rather than count(x) instances of x. Similarly, if we see k additional
instances of each of |X| classes, then we must add k|X| to our original number of samples N. Together,
these two statements yield the above formula. A similar result holds for computing Laplace estimates for
conditionals (which is useful for computing Laplace estimates for outcomes across different classes):
PLAP,k(x|y) = count(x, y) +k
count(y) +k|X|
There are two particularly notable cases for Laplace smoothing. The first is when k = 0, then PLAP,0(x) =
PMLE(x). The second is the case where k = •. Observing a very large, infinite number of each outcome
makes the results of your actual sample inconsequential and so your Laplace estimates imply that each
outcome is equally likely. Indeed:
PLAP,•(x) = 1
|X|
The specific value of k that’s appropriate to use in your model is typically determined by trial-and-error. k is
a hyperparameter in your model, which means that you can set it to whatever you want and see which value
yields the best prediction accuracy/performance on your validation data. Perceptron
Linear Classifiers
The core idea behind Naive Bayes is to extract certain attributes of the training data called features and then
estimate the probability of a label given the features: P(y| f1, f2,... fn).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 7}
Content: Thus, given a new data point, we
can then extract the corresponding features, and classify the new data point with the label with the highest
probability given the features. This all, however, this requires us to estimate distributions, which we did
with MLE. What if instead we decided not to estimate the probability distribution? Lets start by looking
at a simple linear classifier, which we can use for binary classification, which is when the label has two
possibilities, positive or negative. The basic idea of a linear classifier is to do classification using a linear combination of the features– a
value which we call the activation. Concretely, the activation function takes in a data point, multiplies each
feature of our data point, fi(x), by a corresponding weight, wi, and outputs the sum of all the resulting values. In vector form, we can also write this as a dot product of our weights as a vector, w, and our featurized data
point as a vector f(x):
activationw(x) = Â
i
wi fi(x) = wT f(x) = w·f(x)
CS 188, Fall 2018, Note 9 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 8}
Content: How does one do classification using the activation? For binary classification, when the activation of a data
point is positive, we classify that data point with the positive label, and if it is negative, we classify with the
negative label. classify(x) = (
+ if activationw(x) > 0
 if activationw(x) < 0
To understand this geometrically, let us reexamine the vectorized activation function. Using the Law of
Cosines, we can rewrite the dot product as follows, where k · k is the magnitude operator and q is the angle
between w and f(x):
activationw(x) = w·f(x) = kwkkf(x)kcos(q)
Since magnitudes are always non-negative, and our classification rule looks at the sign of the activation, the
only term that matters for determining the class is cos(q). classify(x) = (
+ if cos(q) > 0
 if cos(q) < 0
We, therefore, are interested in when cos(q) is negative or postive. It is easily seen that for q < p
2 , cos(q)
will be somewhere in the interval (0,1], which is positive. For q > p
2 , cos(q) will be somewhere in the
interval [1,0), which is negative.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 8}
Content: You can confirm this by looking at a unit circle. Essentially, our simple
linear classifier is checking to see if the feature vector of a new data point roughly "points" in the same
direction as a predefined weight vector and applies a positive label if it does. classify(x) = (
+ if q < p
2 (i.e. when q is less than 90°, or acute)
 if q > p
2 (i.e. when q is greater than 90°, or obtuse)
Up to this point, we haven’t considered the points where activationw(x) = wT f(x) = 0. Following all the
same logic, we will see that cos(q) = 0 for those points. Furthermore, q = p
2 (i.e q is 90°) for those points. In otherwords, these are the data points with feature vectors that are orthogonal to w. We can add a dotted
blue line, orthogonal to w, where any feature vector that lies on this line will have activation equaling 0. We call this blue line the decision boundary because it is the boundary that separates the region where
we classify data points as positive from the region of negatives. In higher dimensions, a linear decision
boundary is generically called a hyperplane. A hyperplane is a linear surface that is one dimesion lower
than the latent space, thus dividing the surface in two. For general classifiers (non-linear ones), the decision
boundary may not be linear, but is simply defined as surface in the space of feature vectors that separates the
classes. To classify points that end up on the decision boundary, we can apply either label since both classes
are equally valid (in the algorithms below, we’ll classify points on the line as positive). CS 188, Fall 2018, Note 9 9
f(x)
W

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 9}
Content: Decision Boundary
x classified into positive class x classified into negative class
Binary Perceptron
Great, now you know how linear classifiers work, but how do we build a good one? When building a
classifier, you start with data, which are labeled with the correct class, we call this the training set. You
build a classifier by evaluating it on the training data, comparing that to you training labels, and adjusting
the parameters of your classifier until you reach your goal. Let’s explore one specific implementation of a simple linear classifier: the binary perceptron. The perceptron is a binary classifier–though it can be extended to work on more than two classes. The goal of the binary
perceptron is to find a decision boundary that perfectly separates the training data. In other words, we’re
seeking the best possible weights– the best w– such that any featured training point that is multiplied by the
weights, can be perfectly classified. The Algorithm
The perceptron algorithm works as follows:
1. Initialize all weights to 0: w = 0
2.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 9}
Content: For each training sample, with features f(x) and true class label y⇤ 2 {1,+1}, do:
(a) Classify the sample using the current weights, let y be the class predicted by your current w:
y = classify(x) = (
+1 if activationw(x) = wT f(x) > 0
1 if activationw(x) = wT f(x) < 0
(b) Compare the predicted label y to the true label y⇤:
• If y = y⇤, do nothing
• Otherwise, if y 6= y⇤, then update your weights: w w+y⇤f(x)
CS 188, Fall 2018, Note 9 10
W
wTf(x)= 0f(x)
WW
f(x)
+

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 10}
Content: 3. If you went through every training sample without having to update your weights (all samples predicted correctly), then terminate. Else, repeat step 2
Updating weights
Let’s examine and justify the procedure for updating our weights. Recall that in step 2b above, when our
classifier is right, nothing changes. But when our classifier is wrong, the weight vector is updated as follows:
w w+y⇤f(x)
where y⇤ is the true label, which is either 1 or -1, and x is the training sample which we mis-classified. You
can interpret this update rule to be:
Case 1 : mis-classified positive as negative w w+f(x)
Case 2 : mis-classified negative as positve w wf(x)
Why does this work? One way to look at this is to see it as a balancing act. Mis-classification happens either
when the activation for a training sample is much smaller than it should be (causes a Case 1 misclassification)
or much larger than it should be (causes a Case 2 misclassification). Consider Case 1, where activation is negative when it should be positive. In otherwords, the activation is too
small. How we adjust w should strive to fix that and make the activation larger for that training sample. To
convince yourself that our update rule w w + f(x) does that, let us update w and see how the activation
changes. activationw+x(x)=(w+f(x))T x = wT f(x) +f(x)
T f(x) = activationw(x) +f(x)T f(x)
Using our update rule, we see that the new activation increases by f(x)T f(x), which is a postive number,
therefore showing that our update makes sense. Activation is getter larger– closer to becoming positive. You can repeat the same logic for when the classifier is mis-classifying because the activation is too large
(activation is positive when it should be negative). You’ll see that the update will cause the new activation
to decrease by f(x)T f(x), thus getting smaller and closer to classifying correctly. While this makes it clear why we are adding and subtracting something, why would we want to add and
subtract our sample point’s features? A good way to think about it, is that the weights aren’t the only thing
that determines this score. The score is determined by multiplying the weights by the relevant sample.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 10}
Content: This
means that certain parts of a sample contribute more than others. Consider the following situation where x
is a training sample we are given with true label y⇤ = 1:
wT = ⇥
222⇤
, f(x) =
2
4
4
0
1
3
5 activationw(x)=(2 ⇤ 4)+(2 ⇤ 0)+(2 ⇤ 1) = 10
We know that our weights need to be smaller because activation needs to be negative to classify correctly. We don’t want to change them all the same amount though.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 10}
Content: You’ll notice that the first element of our sample,
the 4, contributed much more to our score of 10 than the third element, and that the second element did not
contribute at all. An appropriate weight update, then, would change the first weight alot, the third weight a
little, and the second weight should not be changed at all. After all, the second and third weights might not
even be that broken, and we don’t fix want to fix what isn’t broken! CS 188, Fall 2018, Note 9 11


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 11}
Content: When thinking about a good way to change our weight vector in order to fulfill the above desires, it turns
out just using the sample itself does in fact do what we want; it changes the first weight by alot, the third
weight by a little, and doesn’t change the second weight at all! Note that this is similar to our updates in
approximate Q-learning, which also updated weights by multiples of the feature vectors.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 11}
Content: A visualization may also help. In the figure below, f(x) is the feature vector for a data point with positive
class (y⇤ = +1) that is currently misclassified – it lies on the wrong side of the decision boundary defined
by “old w”. Adding it to the weight vector produces a new weight vector which has a smaller angle to f(x). It also shifts the decision boundary. In this example, it has shifted the decision boundary enough so that x
will now be correctly classified (note that the mistake won’t always be fixed – it depends on the size of the
weight vector, and how far over the boundary f(x) currently is). Mis-classifying x with old w Updating w Updated classification of x
Bias
If you tried to implement a perceptron based on what has been mentioned thus far, you will notice one
particularly unfriendly quirk. Any decision boundary that you end up drawing will be crossing the origin. Basically, your perceptron can only produce a decision boundary that could be represented by the function
w>f(x) = 0, w, f(x) 2 Rn. The problem is, even among problems where there is a linear decision boundary
that separates the positive and negative classes in the data, that boundary may not go through the origin, and
we want to be able to draw those lines. To do so, we will modify our feature and weights to add a bias term: add a feature to your sample feature
vectors that is always 1, and add an extra weight for this feature to your weight vector. Doing so essentially
allows us to produce a decision boundary representable by w>f(x) + b = 0, where b is the weighted bias
term (i.e. 1 * the last weight in the weight vector). Geometrically, we can visualize this by thinking about what the activation function looks like when it is
w>f(x) and when there is a bias w>f(x) +b. To do so, we need to be one dimension higher than the space
of our featurized data (labeled data space in the figures below). In all the above sections, we had only been
looking at a flat view of the data space. Without bias With bias
CS 188, Fall 2018, Note 9 12
W
f(x)W< w+f(x)W
f(x)Decision boundary
wTf(x) = 0
a(x) = wTf (x)
data spacea(x) = wTf(x) +b
Decision boundary
wTf(x) + b= 0
data space

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 12}
Content: Example
Up until now, we have been focused on alot of reading, let’s shake things up a little and see an example, and
run the perceptron algorithm step by step. Let’s run one pass through the data with the perceptron algorithm, taking each data point in order. We’ll start
with the weight vector [w0,w1,w2]=[1,0,0] (where w0 is the weight for our bias feature, which remember
is always 1). Training Set
# f1 f2 y⇤
1 1 1 -
2 3 2 +
3 2 4 +
4 3 4 +
5 2 3 -
Single Perceptron Update Pass
step Weights Score Correct? Update
1 [-1, 0, 0] 1 · 1+0 · 1+0 · 1 = 1 yes none
2 [-1, 0, 0] 1 · 1+0 · 3+0 · 2 = 1 no +[1, 3, 2]
3 [0, 3, 2] 0 · 1+3 · 2+2 · 4 = 14 yes none
4 [0, 3, 2] 0 · 1+3 · 3+2 · 4 = 17 yes none
5 [0, 3, 2] 0 · 1+3 · 2+2 · 3 = 12 no -[1, 2, 3]
6 [-1, 1, -1]
We’ll stop here, but in actuality this algorithm would run for many more passes through the data before all
the data points are classified correctly in a single pass. Multiclass Perceptron
The perceptron presented above is a binary classifier, but we can extend it to account for multiple classes
rather easily. The main difference is in how we set up weights and how we update said weights.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 12}
Content: For the
binary case, we had one weight vector, which had a dimension equal to the number of features (plus the bias
feature). For the multi-class case, we will have one weight vector for each class, so in the 3 class case, we
have 3 weight vectors. In order to classify a sample, we compute a score for each class by taking the dot
product of the feature vector with each of the weight vectors. Whichever class yields the highest score is the
one we choose as our prediction. For example, consider the 3-class case. Let our sample have features f(x) = ⇥
231⇤
and our weights
for classes 0, 1, and 2 be:
w0 = ⇥
221⇤
w1 = ⇥
034⇤
w2 = ⇥
1 4 2
⇤
Taking dot products for each class gives us scores s0 = 11,s1 = 13,s2 = 8. We would thus predict that x
belongs to class 1. An important thing to note is that in actual implementation, we do not keep track of the weights as separate
structures, we usually stack them on top of each other to create a weight matrix. This way, instead of doing
as many dot products as there are classes, we can instead do a single matrix-vector multiplication. This tends
to be much more efficient in practice (because matrix-vector multiplication usually has a highly optimized
implementation). CS 188, Fall 2018, Note 9 13


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 13}
Content: In our above case, that would be:
W =
2
4
22 1
034
1 4 2
3
5, x =
2
4
2
3
1
3
5
And our label would be:
argmax(W x) = argmax(
2
4
11
13
8
3
5) = 1
Along with the structure of our weights, our weight update also changes when we move to a multi-class
case. If we correctly classify our data point, then do nothing just like in the binary case. If we chose
incorrectly, say we chose class y 6= y⇤, then we add the feature vector to the weight vector for the true class
to y⇤, and subtract the feature vector from the weight vector corresponding to the predicted class y. In our
above example, let’s say that the correct class was class 2, but we predicted class 1. We would now take the
weight vector corresponding to class 1 and subtract x from it,
w1 = ⇥
034⇤
⇥
231⇤
= ⇥
203⇤
Next we take the weight vector corresponding to the correct class, class 2 in our case, and add x to it:
w2 = ⇥
1 4 2
⇤
+⇥
231⇤
= ⇥
1 7 1
⇤
What this amounts to is ’rewarding’ the correct weight vector, ’punishing’ the misleading, incorrect weight
vector, and leaving alone an other weight vectors. With the difference in the weights and weight updates
taken into account, the rest of the algorithm is essentially the same; cycle through every sample point,
updating weights when a mistake is make, until you stop making mistakes. In order to incorporate a bias term, do the same as we did for binary perceptron – add an extra feature of 1
to every feature vector, and an extra weight for this feature to every class’s weight vector (this amounts to
adding an extra column to the matrix form). Summary
In this note, we introduced several fundamental principles of machine learning, including:
• Splitting your data into training data, validation data, and test data. • The difference between supervised learning, which learns from labeled data, and unsupervised learning, which doesn’t have labeled data and so attempts to infer inherent structure from it.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes C.pdf', 'page': 13}
Content: We then proceeded to discuss an example of a supervised learning algorithm for classification, Naive Bayes’,
which uses parameter estimation to construct conditional probability tables within a Bayes’ net before running inference to predict the class labels of datapoints. We extended this idea to discuss the problem of
overfitting in the context of Naive Bayes’ and how this issue can be mitigated with Laplace smoothing. Finally, we talked about perceptrons and decision boundaries - methods for classification that don’t explicitly
store conditional probability tables. CS 188, Fall 2018, Note 9 14


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 0}
Content: CS 188 Introduction to Artificial Intelligence
Fall 2017 Note 10
Neural Networks: Motivation
Non-linear Separators
We know how to construct a model that learns a linear boundary for binary classification tasks. This is a
powerful technique, and one that works well when the underlying optimal decision boundary is itself linear.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 0}
Content: However, many practical problems involve the need for decision boundaries that are nonlinear in nature, and
our linear perceptron model isn’t expressive enough to capture this relationship. Consider the following set of data:
We would like to separate the two colors, and clearly there is no way this can be done in a single dimension
(a single dimensional decision boundary would be a point, separating the axis into two regions). To fix this problem, we can add additional (potentially nonlinear) features to construct a decision boundary
from. Consider the same dataset with the addition of x2 as a feature:
With this additional piece of information, we are now able to construct a linear separator in the two dimensional space containing the points. In this case, we were able to fix the problem by mapping our data
to a higher dimensional space by manually adding useful features to datapoints. However, in many highdimensional problems, such as image classification, manually selecting features that are useful is a tedious
problem. This requires domain-specific effort and expertise, and works against the goal of generalization
across tasks. A natural desire is to learn these featurization or transformation functions as well, perhaps
using a nonlinear function class that is capable of representing a wider variety of functions. Multi-layer Perceptron
Let’s examine how we can derive a more complex function from our original perceptron architecture. Consider the following setup, a two-layer perceptron, which is a perceptron that takes as input the outputs of
another perceptron. CS 188, Fall 2017, Note 10 1
一
0
X七
O
O
0
X

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 1}
Content: In fact, we can generalize this to an N-layer perceptron:
With this additional structure and weights, we can express a much wider set of functions. By increasing the complexity of our model, we in turn greatly increase its expressive power. Multi-layer
perceptrons give us a generic way to represent a much wider set of functions. In fact, a multi-layer perceptron is a universal function approximator and can represent any real function, leaving us only with the
problem of selecting the best set of weights to parameterize our network. This is formally stated below:
Theorem. (Universal Function Approximators) A two-layer neural network with a sufficient number
of neurons can approximate any continuous real-valued function to any desired accuracy. Measuring Accuracy
The accuracy of the binary perceptron after making m predictions can be expressed as:
l
acc(w) = 1
m
m
Â
i=1
1(sgn(w ·f(x(i))) == y(i))
where x(i) is datapoint i, w is our weight vector, f is our function that derives a feature vector from a raw
datapoint, and y(i) is the actual class label of x(i). In this context, sgn(x) represents the sign function, which
CS 188, Fall 2017, Note 10 2
W1
Z
>o?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 1}
Content: W
w. 31
W. hw(f(x))
W12
Z
>O? >O? W3
>O? w. 33>O? >O? >O? hw(f(α))
>O? >O? Z
>O? >O? >o? >o? >O?

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 2}
Content: evaluates to 1 when x is negative, and 1 when x is positive. Similarly, 1(x) is an indicator function, which
evaluates to 1 if the quantities within are equivalent and 0 otherwise. Taking this notation into account, we
can note that our accuracy function above is equivalent to dividing the total number of correct predictions
by the raw total number of predictions. Sometimes, we want an output that is more expressive than a binary label. It then becomes useful to produce
a probability for each of the N classes we want to classify into, which reflects our a degree of certainty that
the datapoint belongs to each of the possible classes. To do so, we transition from storing a single weight
vector to storing a weight vector for each class j, and estimate probabilities with the softmax function s(x). The softmax function defines the probability of classifying x(i) to class j as:
s(x(i))j = ef(x(i)
)Twj
ÂN
k=1 ef(x(i))Twk
= P(y(i) = j|x(i))
Given a vector that is output by our function f , softmax performs normalization to output a probability
distribution. To come up with a general loss function for our models, we can use this probability distribution
to generate an expression for the likelihood of a set of weights:
`(w) =
m
’
i=1
P(y(i)|x(i);w)
This expression denotes the likelihood of a particular set of weights explaining the observed labels and
datapoints. We would like to find the set of weights that maximizes this quantity. This is identical to finding
the maximum of the log-likelihood expression (since log is an increasing function, the maximizer of one
will be the maximizer of the other):
``(w) = log
m
’
i=1
P(y(i)|x(i);w) =
m
Â
i=1
logP(y(i)|x(i);w)
(Depending on the application, the formulation as a sum of log probabilities may be more useful – for
example in mini-batched or stochastic gradient descent; see the Neural Networks: Optimization section
below.) In the case where the log likelihood is differentiable with respect to the weights, we will discuss a
simple algorithm to optimize it. Multi-layer Feedforward Neural Networks
We now introduce the idea of an (artificial) neural network.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 2}
Content: This is much like the multi-layer perceptron,
however, we choose a different non-linearity to apply after the individual perceptron nodes. Note that it
is these added non-linearities that makes the network as a whole non-linear and more expressive (without
them, a multi-layer perceptron would simply be a composition of linear functions and hence also linear). In
the case of a multi-layer perceptron, we chose a step function:
f(x) = (
1 if x  0
1 otherwise
Let’s take a look at its graph:
CS 188, Fall 2017, Note 10 3


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 3}
Content: 2 1 0 1 2
1
0.5
0
0.5
1
This is difficult to optimize for a number of reasons which will hopefully become clearer when we address
gradient descent. Firstly, it is not continuous, and secondly, it has a derivative of zero at all points. Intuitively, this means that we cannot know in which direction to look for a local minima of the function, which
makes it difficult to minimize loss in a smooth way.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 3}
Content: Instead of using a step function like above, a better solution is to select a continuous function. We have
many options for such a function, including the sigmoid function (named for the Greek s or ’s’ as it looks
like an ’s’) as well as the rectified linear unit (ReLU).Let’s look at their definitions and graphs below:
Sigmoid: s(x) = 1
1+ex
10 5 0 5 10
0
0.2
0.4
0.6
0.8
1
ReLU: f(x) = (
0 if x < 0
x if x  0
CS 188, Fall 2017, Note 10 4


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 4}
Content: 2 1 0 1 2
0
0.5
1
1.5
2
Calculating the output of a multi-layer perceptron is done as before, with the difference that at the output
of each layer we now apply one of our new non-linearities (chosen as part of the architecture for the neural
network) instead of the initial indicator function. In practice, the choice of nonlinearity is a design choice
that typically requires some experimentation to select a good one for each individual use case. Loss Functions and Multivariate Optimization
Now we have a sense of how a feed-forward neural network is constructed and makes its predictions, we
would like to develop a way to train it, iteratively improving its accuracy, similarly to how we did in the
case of the perceptron. In order to do so, we will need to be able to measure their performance.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 4}
Content: Returning
to our log-likelihood function that we wanted to maximize, we can derive an intuitive algorithm to optimize
our weights given that our function is differentiable. Gradient Ascent / Descent
To maximize our log-likelihood function, we differentiate it to obtain a gradient vector consisting of its
partial derivatives for each parameter:
—w``(w) = 
∂ ``(w)
∂w1
,..., ∂ ``(w)
∂wn

This gradient vector gives the local direction of steepest ascent (or descent if we reverse the vector). Gradient ascent is a greedy algorithm that calculates this gradient for the current values of the weight parameters,
then updates the parameters along the direction of the gradient, scaled by a step size, a. Specifically the
algorithm looks as follows:
Initialize weights w
For i = 0, 1, 2, ... w w +a—w``(w)
If rather than minimizing we instead wanted to minimize a function f , the update should subtract the scaled
gradient (w wa—w f(w)) – this gives the gradient descent algorithm. CS 188, Fall 2017, Note 10 5


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 5}
Content: Neural Networks: Optimization
Now that we have a method for computing gradients for all parameters of the network, we can use gradient
descent methods to optimize the parameters to get high accuracy on our training data. For example, suppose
we have designed some classification network to output probabilities of classes y for data points x, and have
m different training datapoints (see the Measuring Accuracy section for more on this).

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 5}
Content: Let w be all the
parameters of our network. We want to find values for the parameters w that maximize the likelihood of the
true class probabilities for our data, so we have the following function to run gradient ascent on:
``(w) = log
m
’
i=1
P(y(i) | x(i);w) =
m
Â
i=1
logP(y(i) | x(i);w)
where x(1),..., x(m) are the m datapoints in our training set. One way to try to minimize this function is, at each iteration of gradient descent, to use all the data points
x(1),..., x(m) to compute gradients for the parameters w, update the parameters, and repeat until the parameters converge (at which point we’ve reached a local minimum of the function). This technique, known as batch gradient descent, is rarely done in practice, since datasets are typically
large enough that computing gradients for this full likelihood function will be very slow. Instead, we’ll
typically use mini-batching. Mini-batching rotates through randomly sampled batches of k data points at a
time, taking one batch for each step of gradient descent and computing gradients of the loss function using
only that batch (so that the sum above is over the k datapoints in the batch, rather than all m datapoints
in the training set). This allows us to compute each gradient update much more quickly, and often still
makes fast progress toward the minimum of the function. The limit where the batch size k = 1 is known
as stochastic gradient descent (SGD). In SGD, we randomly sample a single example from the training
dataset at each step of gradient descent, compute parameter gradients using the network’s loss on that single
example, update the parameters, and repeat (sampling another example from the training set). Neural Networks: Backpropagation (Optional)
To efficiently calculate the gradients for each parameter in a neural network, we will use an algorithm known
as backpropagation. Backpropagation represents the neural network as a dependency graph of operators
and operands, called a computational graph, such as the one shown below:
The graph structure allows us to efficiently compute both the network’s error (loss) on input data, as well as
the gradients of each parameter with respect to the loss. These gradients can be used in gradient descent to
adjust the network’s parameters and minimize the loss on the training data. The Chain Rule
The chain rule is the fundamental rule from calculus which both motivates the usage of computation graphs
and allows for a computationally feasible backpropagation algorithm. Mathematically, it states that for a
variable z which is a function of n variables x1,..., xn and each xi is a function of m variables t1,...,tm, then
we can compute the derivative of z with respect to any ti as follows:
∂ f
∂ti
= ∂ f
x1
·
∂ x1
∂ti
+
∂ f
x2
·
∂ x2
∂ti
+...+
∂ f
xn
·
∂ xn
∂ti
CS 188, Fall 2017, Note 10 6


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 6}
Content: 2
x
3
y
+
g
4
z
⇤
f
In the context of computation graphs, this means that to compute the gradient of a given node ti with respect
to the output z, we take a sum of children(ti) terms. The Backpropagation Algorithm
2
x
3
y
+
g
4
z
⇤
f
2! 4
3! 4
5! 4
4! 5
20! 1
Figure 1: A computation graph for computing (x+y) ⇤ z with the values x = 2, y = 3, z = 4. Figure 1 shows an example computation graph for computing (x+y) ⇤ z with the values x = 2, y = 3, z = 4. We will write g = x+y and f = g ⇤ z. Values in green are the outputs of each node, which we compute in
the forward pass, where we apply each node’s operation to its input values coming from its parent nodes. Values in red after each node give gradients of the function computed by the graph, which are computed in
the backward pass: the value after each node is the partial derivative of the last node f value with respect
to the variable at that node. For example, the red value 4 after g is ∂ f
∂g , and the red value 4 after x is ∂ f∂ x . In
our simple example, f is just a multiplication node which outputs the product of its two input operands, but
in a real neural network the final node will usually compute the loss value that we are trying to minimize. The backward pass computes gradients by starting at the final node (which has a gradient of 1 since ∂ f
∂ f = 1)
and passing and updating gradients backward through the graph. Intuitively, each node’s gradient measures
CS 188, Fall 2017, Note 10 7


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 7}
Content: how much a change in that node’s value contributes to a change in the final node’s value. This will be the
product of how much the node contributes to a change in its child node, with how much the child node contributes to a change in the final node. Each node receives and combines gradients from its children, updates
this combined gradient based on the node’s inputs and the node’s operation, and then passes the updated
gradient backward to its parents. Computation graphs are a great way to visualize repeated application of
the chain rule from calculus, as this process is required for backpropagation in neural networks. Our goal during backpropagation is to determine the gradient of output with respect to each of the inputs. As you can see in Figure 1, in this case we want to compute the gradients ∂ f
∂ x , ∂ f∂ y , and ∂ f∂ z :
1. Since f is our final node, it has gradient ∂ f
∂ f = 1. Then we compute the gradients for its children, g
and z. We have ∂ f
∂g = ∂∂g (g ·z) = z = 4, and ∂ f∂ z = ∂∂ z(g ·z) = g = 5. 2. Now we can move on upstream to compute the gradients of x and y. For these, we’ll use the chain
rule and reuse the gradient we just computed for g, ∂ f
∂g . 3. For x, we have ∂ f
∂ x = ∂ f∂g
∂g
∂ x by the chain rule – the product of the gradient coming from g with the
partial derivative for x at this node. We have ∂g
∂ x = ∂∂ x (x+y) = ∂∂ x x+ ∂∂ x y = 1+0, so ∂ f∂ x = 4 · 1 = 4. Intuitively, the amount that a change in x contributes to a change in f is the product of the amount that
a change in g contributes to a change in f , with the amount that a change in x contributes to one in g. 4. The process for computing the gradient of the output with respect to y is almost identical. For y
we have ∂ f
∂ y = ∂ f∂g
∂g
∂ y by the chain rule – the product of the gradient coming from g with the partial
derivative for y at this node. We have ∂g
∂ y = ∂∂ y (x+y) = ∂∂ y
x+ ∂
∂ y y = 0+1, so ∂ f∂ y = 4 · 1 = 4. Since the backward pass step for a node in general depends on the node’s inputs (which are computed in the
forward pass), and gradients computed “downstream” of the current node by the node’s children (computed
earlier in the backward pass), we cache all of these values in the graph for efficiency. Taken together, the
forward and backward pass over the graph make up the backpropagation algorithm. 2
x
3
y
+
h
⇤
i
+
g
4
z
⇤
f
2!

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 7}
Content: 16 4
 12
3! 12 8
 4
 
5! 4
6! 4
11! 4
4! 11
44! 1
Figure 2: A computation graph for computing ((x+y)+(x · y))·z, with x = 2, y = 3, z = 4. For an example of applying the chain rule for a node with multiple children, consider the graph in Figure 2,
representing ((x+y)+(x · y))·z, with x = 2, y = 3, z = 4. x and y are each used in 2 operations, and so each
has two children. By the chain rule, their gradient values are the sum of the gradients computed for them
CS 188, Fall 2017, Note 10 8


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes D.pdf', 'page': 8}
Content: by their children (i.e. gradient values add at path junctions). For example, to compute the gradient for x, we
have
∂ f
∂ x = ∂ f∂h
∂h
∂ x
+
∂ f
∂ i
∂ i
∂ x = 4 · 1+4 · 3 = 4+12 = 16
Conclusion
In this note, we generalized the perceptron to neural networks, models which are powerful (and universal!)
function approximators but can be difficult to design and train. We talked about how the expressiveness of
neural networks comes from the activation functions they employ to introduce nonlinearities, as well as how
to optimize their parameters with backpropagation and gradient descent. Currently, a large portion of new
research in machine learning focuses on various aspects of neural network design such as:
1. Network Architectures - designing a network (choosing activation functions, number of layers, etc.)
that’s a good fit for a particular problem
2. Learning Algorithms - how to find parameters that achieve a low value of the loss function, a difficult
problem since gradient descent is a greedy algorithm and neural nets can have many local optima
3. Generalization and Transfer Learning - since neural nets have many parameters, it’s often easy to
overfit training data - how do you guarantee that they also have low loss on testing data you haven’t
seen before? CS 188, Fall 2017, Note 10 9


Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 0}
Content: CS272: Artificial Intelligence Intelligent Agents Page 1
Lecture Notes
Intelligent Agents
Instructor: Dr. Seemab Latif
 
 
National
University
vof
Sciences
and
Technology(
(NUST)
School of Electrical Engineering
and
IComputer
Science

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 1}
Content: CS272: Artificial Intelligence Intelligent Agents Page 2
Intelligent Agents 
Simple Reflex Agents
Simple reflex agents ignore the rest of the percept history and act only on the basis of the current 
percept. Percept history is the history of all that an agent has perceived to date. The agent 
function is based on the condition-action rule. A condition-action rule is a rule that maps a state 
i.e., a condition to an action. If the condition is true, then the action is taken, else not. This agent 
function only succeeds when the environment is fully observable. For simple reflex agents 
operating in partially observable environments, infinite loops are often unavoidable. It may be 
possible to escape from infinite loops if the agent can randomize its actions. Problems with Simple reflex agents are :
 Very limited intelligence.  No knowledge of non-perceptual parts of the state.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 1}
Content:  Usually too big to generate and store.  If there occurs any change in the environment, then the collection of rules needs to 
be updated. Model-Based Reflex Agents
It works by finding a rule whose condition matches the current situation. A model-based agent 
can handle partially observable environments using a model about the world. The agent must 
keep track of the internal state which is adjusted by each percept and that depends on the 
percept history. The current state is stored inside the agent which maintains some kind of 
structure describing the part of the world which cannot be seen. Updating the state requires information about:
 How the world evolves independently from the agent?  How do the agent’s actions affect the world? National
University
vof
Sciences
and
Technology(
(NUST)
School of Electrical Engineering
and
IComputer
SciencePercepts
Agent
Sensors
What the world
is right now
Environment
Condition-
What action I
action rules
should do now
Actuators
Actions

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 2}
Content: CS272: Artificial Intelligence Intelligent Agents Page 3
Goal-Based Agents
These kinds of agents take decisions based on how far they are currently from 
their goal(description of desirable situations). Their every action is intended to reduce their 
distance from the goal. This allows the agent a way to choose among multiple possibilities, 
selecting the one which reaches a goal state. The knowledge that supports its decisions is 
represented explicitly and can be modified, which makes these agents more flexible. They 
usually require search and planning. The goal-based agent’s behavior can easily be changed. National
University
vof
Sciences
and
Technology(
(NUST)
School of Electrical Engineering
and
IComputer
SciencePrecepts
Sensors
State
How the world evolves
What the world
is like now
What my actions do
Environment
Condition-action rules
What action I
should do now
Agent
Actuators
Actions

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 3}
Content: CS272: Artificial Intelligence Intelligent Agents Page 4
Utility-Based Agents
The agents which are developed having their end uses as building blocks are called utility-based 
agents. When there are multiple possible alternatives, then to decide which one is best, utilitybased agents are used. They choose actions based on a preference (utility)for each state. Sometimes achieving the desired goal is not enough.

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 3}
Content: We may look for a quicker, safer, cheaper 
trip to reach a destination. Agent happiness should be taken into consideration. Utility describes 
how “happy” the agent is. Because of the uncertainty in the world, a utility agent chooses the 
action that maximizes the expected utility. A utility function maps a state onto a real number 
which describes the associated degree of happiness. National
University
vof
Sciences
and
Technology(
(NUST)
School of Electrical Engineering
and
IComputer
SciencePrecepts
Sensors
State
How the world evolves
What the world
Environment
is like now
What my actions do
What it will be like
if I do action A
Goals
What action I
should do now
Actions
Agent
Actuators

Metadata: {'Instructor': 'Seemab Latif', 'Title': 'Artificial Intelligence', 'Course_category': 'Engineering', 'source': 'E:\\FYP\\code\\TALIM\\Data\\AI-Notes E.pdf', 'page': 4}
Content: CS272: Artificial Intelligence Intelligent Agents Page 5
National
University
vof
Sciences
and
Technology(
(NUST)
School of Electrical Engineering
and
IComputer
SciencePrecepts
Sensors
State
How the world evolves
What the world
Environment
is like now
What my actions do
What it will be like
if I do action A
Utility
How happyI will
be in such a state
Agent
What action I
should do now
Actions
Actuators

